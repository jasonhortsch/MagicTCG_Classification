{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hn/anaconda/lib/python2.7/site-packages/IPython/html.py:14: ShimWarning: The `IPython.html` package has been deprecated. You should import from `notebook` instead. `IPython.html.widgets` has moved to `ipywidgets`.\n",
      "  \"`IPython.html.widgets` has moved to `ipywidgets`.\", ShimWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cross_validation import train_test_split, KFold, cross_val_score\n",
    "import sklearn.metrics as sk\n",
    "\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({u'Black': 1576,\n",
       "         u'Blue': 1573,\n",
       "         u'Green': 1566,\n",
       "         u'Red': 1575,\n",
       "         u'White': 1584})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modern = pd.read_pickle('data/5color_modern_no_name_hardmode.pkl')\n",
    "Counter(modern.colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After all the data munging the classes are still amazingly balanced.\n",
    "\n",
    "## Lets single out blue and red for a binary classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>colors</th>\n",
       "      <th>cmc</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1709</th>\n",
       "      <td>Renegade Doppelganger</td>\n",
       "      <td>Blue</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Whenever another creature enters the battlefie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2782</th>\n",
       "      <td>Mercurial Pretender</td>\n",
       "      <td>Blue</td>\n",
       "      <td>5.0</td>\n",
       "      <td>You may have This enter the battlefield as a c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2907</th>\n",
       "      <td>Bloodfire Enforcers</td>\n",
       "      <td>Red</td>\n",
       "      <td>4.0</td>\n",
       "      <td>This has first strike and trample as long as a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3106</th>\n",
       "      <td>Coastal Discovery</td>\n",
       "      <td>Blue</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Draw two cards. Awaken 4—{5}{1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2635</th>\n",
       "      <td>Forgestoker Dragon</td>\n",
       "      <td>Red</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Flying {1}{1}: This deals 1 damage to target c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>Hearth Kami</td>\n",
       "      <td>Red</td>\n",
       "      <td>2.0</td>\n",
       "      <td>{1}, Sacrifice This: Destroy target artifact w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       name colors  cmc  \\\n",
       "1709  Renegade Doppelganger   Blue  2.0   \n",
       "2782    Mercurial Pretender   Blue  5.0   \n",
       "2907    Bloodfire Enforcers    Red  4.0   \n",
       "3106      Coastal Discovery   Blue  4.0   \n",
       "2635     Forgestoker Dragon    Red  6.0   \n",
       "279             Hearth Kami    Red  2.0   \n",
       "\n",
       "                                                   text  \n",
       "1709  Whenever another creature enters the battlefie...  \n",
       "2782  You may have This enter the battlefield as a c...  \n",
       "2907  This has first strike and trample as long as a...  \n",
       "3106                   Draw two cards. Awaken 4—{5}{1}   \n",
       "2635  Flying {1}{1}: This deals 1 damage to target c...  \n",
       "279   {1}, Sacrifice This: Destroy target artifact w...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UG = modern.loc[modern['colors'].isin(['Blue', 'Red'])]\n",
    "\n",
    "UG.reset_index(inplace=True)\n",
    "UG.pop('index')\n",
    "\n",
    "UG[['name', 'colors', 'cmc', 'text']].sample(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Blue</th>\n",
       "      <th>Red</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Blue  Red\n",
       "0   1.0  0.0\n",
       "1   0.0  1.0\n",
       "2   1.0  0.0\n",
       "3   1.0  0.0\n",
       "4   1.0  0.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummies = pd.get_dummies(UG.colors)\n",
    "dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2361L, 815L)\n",
      "(2361L, 2L)\n",
      "<type 'numpy.ndarray'>\n",
      "<type 'numpy.ndarray'>\n",
      "(2361L, 815L)\n",
      "(2361L, 2L)\n",
      "<type 'numpy.ndarray'>\n",
      "<type 'numpy.ndarray'>\n",
      "There are 815 words in the vocabulary.\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "\n",
    "vec_X = vectorizer.fit_transform(UG['text'])\n",
    "\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(vec_X, dummies,\n",
    "                                             random_state=42)\n",
    "\n",
    "xTrain = np.asarray(xTrain.todense())\n",
    "xTest  = np.asarray(xTest.todense())\n",
    "yTrain = np.asarray(yTrain)\n",
    "yTest  = np.asarray(yTest)\n",
    "\n",
    "print xTrain.shape\n",
    "print yTrain.shape\n",
    "print type(xTrain)\n",
    "print type(yTrain)\n",
    "\n",
    "# xTrain = xTrain.reshape(-1, 1, 1, 815)\n",
    "# xTest = xTest.reshape(-1, 1, 1, 815)\n",
    "\n",
    "print xTrain.shape\n",
    "print yTrain.shape\n",
    "print type(xTrain)\n",
    "print type(yTrain)\n",
    "\n",
    "print \"There are {:,} words in the vocabulary.\".format(len(vectorizer.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round: 0  Test: 0.651842439644  Train: 0.629394324439\n",
      "Round: 30  Test: 0.810673443456  Train: 0.831427361288\n",
      "Round: 60  Test: 0.833545108005  Train: 0.849216433715\n",
      "Round: 90  Test: 0.853875476493  Train: 0.861075815332\n",
      "Round: 120  Test: 0.852604828463  Train: 0.866581956798\n",
      "Round: 150  Test: 0.855146124524  Train: 0.873358746294\n",
      "Round: 180  Test: 0.861499364676  Train: 0.879288437103\n",
      "Round: 210  Test: 0.864040660737  Train: 0.885218127912\n",
      "Round: 240  Test: 0.870393900889  Train: 0.889030072003\n",
      "Round: 270  Test: 0.866581956798  Train: 0.892842016095\n",
      "Round: 300  Test: 0.866581956798  Train: 0.894536213469\n",
      "Round: 330  Test: 0.869123252859  Train: 0.894112664125\n",
      "Round: 360  Test: 0.87166454892  Train: 0.895383312156\n",
      "Round: 390  Test: 0.87166454892  Train: 0.895806861499\n"
     ]
    }
   ],
   "source": [
    "import theano\n",
    "from theano import tensor as T\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "\n",
    "\n",
    "def floatX(X):\n",
    "    return np.asarray(X, dtype=theano.config.floatX)\n",
    "\n",
    "def init_weights(shape):\n",
    "    (h, w) = shape\n",
    "    normalizer = 2.0 * sqrt(6) / sqrt(h + w) * .2  #factors: correct for uni[0,1], glo, glo, softmax deriv\n",
    "    return theano.shared(floatX((np.random.random_sample(shape) - 0.5) \\\n",
    "                                * normalizer))  #code for using Glorot init\n",
    "    \n",
    "def model(X, w):\n",
    "    return T.nnet.softmax(T.dot(X, w))\n",
    "\n",
    "def adaDelta(cost, params, eta=0.2, rho=0.9, epsilon=1e-6):\n",
    "    grads = T.grad(cost=cost, wrt=params)\n",
    "    updates = []\n",
    "    for p, g in zip(params, grads):\n",
    "        #calc g-squared\n",
    "        gSq = theano.shared(p.get_value() * 0.)\n",
    "        dwSq = theano.shared(p.get_value() * 0.)\n",
    "\n",
    "        #exp smoothed g squared\n",
    "        gSqNew = rho * gSq + (1 - rho) * g * g\n",
    "\n",
    "        #calc dx-squared\n",
    "        dw = eta * T.sqrt(dwSq + epsilon) * g / T.sqrt(gSq + epsilon)\n",
    "        dwSqNew = rho * dwSq + (1 - rho) * dw * dw\n",
    "\n",
    "        updates.append((dwSq, dwSqNew))\n",
    "        updates.append((gSq, gSqNew))\n",
    "        updates.append((p, p - dw))\n",
    "    return updates\n",
    "\n",
    "X = T.fmatrix()\n",
    "Y = T.fmatrix()\n",
    "# grad_list = theano.shared(np.array([0,0]), name='grad_list')\n",
    "\n",
    "w = init_weights((815 , 2))\n",
    "\n",
    "py_x = model(X, w)\n",
    "y_pred = T.argmax(py_x, axis=1)\n",
    "\n",
    "cost = T.mean(T.nnet.categorical_crossentropy(py_x, Y))\n",
    "gradient = T.grad(cost=cost, wrt=w)\n",
    "update = [[w, w - gradient * 0.1]]\n",
    "\n",
    "train = theano.function(inputs=[X, Y], \n",
    "                        outputs=[cost, gradient], \n",
    "                        updates=update, \n",
    "                        allow_input_downcast=True)\n",
    "\n",
    "predict = theano.function(inputs=[X], \n",
    "                          outputs=y_pred, \n",
    "                          allow_input_downcast=True)\n",
    "\n",
    "\n",
    "for i in range(401):\n",
    "# #     for start, end in zip(range(0, xTrain.shape[0], 128), \n",
    "# #                           range(128, xTrain.shape[0], 128)):\n",
    "# #         cost, gradient = train(xTrain[start:end], yTrain[start:end])\n",
    "    cost, gradient = train(xTrain, yTrain)\n",
    "    if i % 30 == 0: \n",
    "        tr = np.mean(np.argmax(yTest, axis=1) == predict(xTest))\n",
    "        trr =  np.mean(np.argmax(yTrain, axis=1) == predict(xTrain))\n",
    "        print 'Round:', i,\" Test:\", tr, ' Train:', trr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All Five Vs All Five\n",
    "\n",
    "And now the main event - simply comparing two colors was too easy. Five way classification of all the colors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5905, 1161)\n",
      "(5905, 5)\n",
      "<type 'numpy.ndarray'>\n",
      "<type 'numpy.ndarray'>\n",
      "(5905, 1161)\n",
      "(5905, 5)\n",
      "<type 'numpy.ndarray'>\n",
      "<type 'numpy.ndarray'>\n",
      "There are 1,161 words in the vocabulary.\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "\n",
    "y = pd.get_dummies(modern.colors)\n",
    "\n",
    "X = vectorizer.fit_transform(modern.text)\n",
    "\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(X, y, random_state=42)\n",
    "\n",
    "xTrain = np.asarray(xTrain.todense())\n",
    "xTest  = np.asarray(xTest.todense())\n",
    "yTrain = np.asarray(yTrain)\n",
    "yTest  = np.asarray(yTest)\n",
    "\n",
    "print xTrain.shape\n",
    "print yTrain.shape\n",
    "print type(xTrain)\n",
    "print type(yTrain)\n",
    "\n",
    "# xTrain = xTrain.reshape(-1, 1, 1, 815)\n",
    "# xTest = xTest.reshape(-1, 1, 1, 815)\n",
    "\n",
    "print xTrain.shape\n",
    "print yTrain.shape\n",
    "print type(xTrain)\n",
    "print type(yTrain)\n",
    "\n",
    "print \"There are {:,} words in the vocabulary.\".format(len(vectorizer.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round: 0  Test: 0.292026409345  Train: 0.27366638442\n",
      "Round: 500  Test: 0.601320467242  Train: 0.633192209992\n",
      "Round: 1000  Test: 0.627729812087  Train: 0.667739204064\n",
      "Round: 1500  Test: 0.641442356526  Train: 0.686875529213\n",
      "Round: 2000  Test: 0.655662772981  Train: 0.700423370025\n",
      "Round: 2500  Test: 0.66124936516  Train: 0.709060118544\n",
      "Round: 3000  Test: 0.664296597257  Train: 0.716680779001\n",
      "Round: 3500  Test: 0.676485525648  Train: 0.724132091448\n",
      "Round: 4000  Test: 0.682579989843  Train: 0.728704487722\n",
      "Round: 4500  Test: 0.687150837989  Train: 0.732938187976\n",
      "Round: 5000  Test: 0.688674454038  Train: 0.737849280271\n",
      "Round: 5500  Test: 0.688674454038  Train: 0.739881456393\n",
      "Round: 6000  Test: 0.686642965973  Train: 0.741913632515\n",
      "Round: 6500  Test: 0.691721686135  Train: 0.744623200677\n",
      "Round: 7000  Test: 0.694261046216  Train: 0.746655376799\n",
      "Round: 7500  Test: 0.695784662265  Train: 0.749364944962\n",
      "Round: 8000  Test: 0.695784662265  Train: 0.750381033023\n",
      "Round: 8500  Test: 0.694768918233  Train: 0.751905165114\n",
      "Round: 9000  Test: 0.695784662265  Train: 0.754445385267\n",
      "Round: 9500  Test: 0.696800406298  Train: 0.756816257409\n",
      "Round: 10000  Test: 0.698831894363  Train: 0.75766299746\n",
      "Wall time: 7min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import theano\n",
    "from theano import tensor as T\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "\n",
    "def floatX(X):\n",
    "    return np.asarray(X, dtype=theano.config.floatX)\n",
    "\n",
    "def init_weights(shape):\n",
    "    (h, w) = shape\n",
    "    normalizer = 2.0 * sqrt(6) / sqrt(h + w) * .2  #factors: correct for uni[0,1], glo, glo, softmax deriv\n",
    "    return theano.shared(floatX((np.random.random_sample(shape) - 0.5) \\\n",
    "                                * normalizer))  #code for using Glorot init\n",
    "    \n",
    "def model(X, w):\n",
    "    return T.nnet.softmax(T.dot(X, w))\n",
    "\n",
    "def adaDelta(cost, params, eta=0.2, rho=0.9, epsilon=1e-6):\n",
    "    grads = T.grad(cost=cost, wrt=params)\n",
    "    updates = []\n",
    "    for p, g in zip(params, grads):\n",
    "        #calc g-squared\n",
    "        gSq = theano.shared(p.get_value() * 0.)\n",
    "        dwSq = theano.shared(p.get_value() * 0.)\n",
    "\n",
    "        #exp smoothed g squared\n",
    "        gSqNew = rho * gSq + (1 - rho) * g * g\n",
    "\n",
    "        #calc dx-squared\n",
    "        dw = eta * T.sqrt(dwSq + epsilon) * g / T.sqrt(gSq + epsilon)\n",
    "        dwSqNew = rho * dwSq + (1 - rho) * dw * dw\n",
    "\n",
    "        updates.append((dwSq, dwSqNew))\n",
    "        updates.append((gSq, gSqNew))\n",
    "        updates.append((p, p - dw))\n",
    "    return updates\n",
    "\n",
    "X = T.fmatrix()\n",
    "Y = T.fmatrix()\n",
    "\n",
    "w = init_weights((len(vectorizer.vocabulary_) , yTest.shape[1]))\n",
    "\n",
    "py_x = model(X, w)\n",
    "y_pred = T.argmax(py_x, axis=1)\n",
    "\n",
    "cost = T.mean(T.nnet.categorical_crossentropy(py_x, Y))\n",
    "gradient = T.grad(cost=cost, wrt=w)\n",
    "update = [[w, w - gradient * 0.1]]\n",
    "\n",
    "train = theano.function(inputs=[X, Y], \n",
    "                        outputs=[cost, gradient], \n",
    "                        updates=update, \n",
    "                        allow_input_downcast=True)\n",
    "\n",
    "predict = theano.function(inputs=[X], \n",
    "                          outputs=y_pred, \n",
    "                          allow_input_downcast=True)\n",
    "\n",
    "\n",
    "for i in range(10001):\n",
    "# #     for start, end in zip(range(0, xTrain.shape[0], 128), \n",
    "# #                           range(128, xTrain.shape[0], 128)):\n",
    "# #         cost, gradient = train(xTrain[start:end], yTrain[start:end])\n",
    "    cost, gradient = train(xTrain, yTrain)\n",
    "    if i % 500 == 0: \n",
    "        tr = np.mean(np.argmax(yTest, axis=1) == predict(xTest))\n",
    "        trr =  np.mean(np.argmax(yTrain, axis=1) == predict(xTrain))\n",
    "        print 'Round:', i,\" Test:\", tr, ' Train:', trr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazing results, almost equal to logistic regression at 5min. \n",
    "\n",
    "### 4-Layer Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round: 0  Test: 0.27780599289  Train: 0.302116850127\n",
      "Round: 10  Test: 0.280853224987  Train: 0.287552921253\n",
      "Round: 20  Test: 0.46063991874  Train: 0.49483488569\n",
      "Round: 30  Test: 0.602336211275  Train: 0.63200677392\n",
      "Round: 40  Test: 0.653631284916  Train: 0.690770533446\n",
      "Round: 50  Test: 0.672930421534  Train: 0.734462320068\n",
      "Round: 60  Test: 0.691213814119  Train: 0.759017781541\n",
      "Round: 70  Test: 0.694261046216  Train: 0.774259102456\n",
      "Round: 80  Test: 0.699847638395  Train: 0.788653683319\n",
      "Round: 90  Test: 0.710512950736  Train: 0.805927180356\n",
      "Round: 100  Test: 0.723209751143  Train: 0.818797629128\n",
      "Round: 110  Test: 0.723717623159  Train: 0.832345469941\n",
      "Round: 120  Test: 0.729304215338  Train: 0.847417442845\n",
      "Round: 130  Test: 0.725241239208  Train: 0.853852667231\n",
      "Round: 140  Test: 0.727272727273  Train: 0.86316680779\n",
      "Round: 150  Test: 0.733875063484  Train: 0.872650296359\n",
      "Round: 160  Test: 0.723717623159  Train: 0.878577476715\n",
      "Round: 170  Test: 0.732351447435  Train: 0.885012701101\n",
      "Round: 180  Test: 0.731843575419  Train: 0.890939881456\n",
      "Round: 190  Test: 0.733367191468  Train: 0.897713801863\n",
      "Round: 200  Test: 0.724225495175  Train: 0.901270110076\n",
      "Round: 210  Test: 0.732351447435  Train: 0.90533446232\n",
      "Round: 220  Test: 0.728796343321  Train: 0.910922946655\n",
      "Round: 230  Test: 0.729304215338  Train: 0.91532599492\n",
      "Round: 240  Test: 0.73031995937  Train: 0.918035563082\n",
      "Round: 250  Test: 0.728288471305  Train: 0.919559695174\n",
      "Round: 260  Test: 0.731335703403  Train: 0.922946655377\n",
      "Round: 270  Test: 0.727780599289  Train: 0.92616426757\n",
      "Round: 280  Test: 0.730827831386  Train: 0.928196443692\n",
      "Round: 290  Test: 0.732351447435  Train: 0.930059271804\n",
      "Round: 300  Test: 0.727272727273  Train: 0.931752751905\n",
      "Round: 310  Test: 0.72625698324  Train: 0.933954276037\n",
      "Round: 320  Test: 0.725749111224  Train: 0.934292972058\n",
      "Round: 330  Test: 0.721686135094  Train: 0.93683319221\n",
      "Round: 340  Test: 0.727272727273  Train: 0.937849280271\n",
      "Round: 350  Test: 0.724733367191  Train: 0.93734123624\n",
      "Round: 360  Test: 0.724225495175  Train: 0.939542760373\n",
      "Round: 370  Test: 0.72625698324  Train: 0.939881456393\n",
      "Round: 380  Test: 0.73031995937  Train: 0.942252328535\n",
      "Round: 390  Test: 0.724225495175  Train: 0.942929720576\n",
      "Round: 400  Test: 0.726764855256  Train: 0.943607112616\n",
      "Wall time: 3min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import theano\n",
    "from theano import tensor as T\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "import numpy as np\n",
    "\n",
    "srng = RandomStreams()\n",
    "\n",
    "def floatX(X):\n",
    "    return np.asarray(X, dtype=theano.config.floatX)\n",
    "\n",
    "def init_weights(shape):\n",
    "    return theano.shared(floatX(np.random.randn(*shape) * 0.01))\n",
    "\n",
    "def rectify(X, alpha=1.0):\n",
    "#     return T.maximum(X, 0.)\n",
    "    return T.maximum(X, 0.05*X)  #leaky rectifier\n",
    "#     return T.switch(X > 0, X, alpha * (T.exp(X) - 1)) # ELU\n",
    "\n",
    "def softmax(X):\n",
    "    e_x = T.exp(X - X.max(axis=1).dimshuffle(0, 'x'))\n",
    "    return e_x / e_x.sum(axis=1).dimshuffle(0, 'x')\n",
    "\n",
    "def RMSprop(cost, params, lr=0.001, rho=0.9, epsilon=1e-6):\n",
    "    grads = T.grad(cost=cost, wrt=params)\n",
    "    updates = []\n",
    "    for p, g in zip(params, grads):\n",
    "        acc = theano.shared(p.get_value() * 0.)\n",
    "        acc_new = rho * acc + (1 - rho) * g ** 2\n",
    "        gradient_scaling = T.sqrt(acc_new + epsilon)\n",
    "        g = g / gradient_scaling\n",
    "        updates.append((acc, acc_new))\n",
    "        updates.append((p, p - lr * g))\n",
    "    return updates\n",
    "\n",
    "def dropout(X, p=0.):\n",
    "    if p > 0:\n",
    "        retain_prob = 1 - p\n",
    "        X *= srng.binomial(X.shape, p=retain_prob, dtype=theano.config.floatX)\n",
    "        X /= retain_prob\n",
    "    return X\n",
    "\n",
    "def model(X, w_h, w_h2, w_o, p_drop_input, p_drop_hidden):\n",
    "    X = dropout(X, p_drop_input)\n",
    "    h = rectify(T.dot(X, w_h))\n",
    "\n",
    "    h = dropout(h, p_drop_hidden)\n",
    "    h2 = rectify(T.dot(h, w_h2))\n",
    "\n",
    "    h2 = dropout(h2, p_drop_hidden)\n",
    "    py_x = softmax(T.dot(h2, w_o))\n",
    "    return h, h2, py_x\n",
    "\n",
    "X = T.fmatrix()\n",
    "Y = T.fmatrix()\n",
    "\n",
    "# w = init_weights((len(vectorizer.vocabulary_) , yTest.shape[1])) # old \n",
    "w_h = init_weights((len(vectorizer.vocabulary_), 600))\n",
    "w_h2 = init_weights((600, 600))\n",
    "w_o = init_weights((600, yTest.shape[1]))\n",
    "\n",
    "noise_h, noise_h2, noise_py_x = model(X, w_h, w_h2, w_o, 0.2, 0.5)\n",
    "h, h2, py_x = model(X, w_h, w_h2, w_o, 0., 0.)\n",
    "y_x = T.argmax(py_x, axis=1)\n",
    "\n",
    "cost = T.mean(T.nnet.categorical_crossentropy(noise_py_x, Y))\n",
    "params = [w_h, w_h2, w_o]\n",
    "updates = RMSprop(cost, params, lr=0.0001)\n",
    "\n",
    "train = theano.function(inputs=[X, Y], outputs=cost, updates=updates, allow_input_downcast=True)\n",
    "predict = theano.function(inputs=[X], outputs=y_x, allow_input_downcast=True)\n",
    "\n",
    "for i in range(401):\n",
    "    for start, end in zip(range(0, len(xTrain), 128), range(128, len(xTrain), 128)):\n",
    "        cost = train(xTrain[start:end], yTrain[start:end])\n",
    "    if i%10 == 0: \n",
    "        tr = np.mean(np.argmax(yTest, axis=1) == predict(xTest))\n",
    "        trr =  np.mean(np.argmax(yTrain, axis=1) == predict(xTrain))\n",
    "        print 'Round:', i,\" Test:\", tr, ' Train:', trr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Strong signs of overfitting\n",
    "\n",
    "Next steps:  \n",
    "\n",
    "Leaky RELU swapped out for ELU, alpha = 1.0   \n",
    "Train accuracy dropped by 14% (good)  \n",
    "Test accuracy dropped by 3% (bad)   \n",
    "\n",
    "Leaky RELU swapped out for ELU, alpha = .5  \n",
    "Train accuracy dropped by 7% (good)  \n",
    "Test accuracy dropped by 1% (bad)   \n",
    "\n",
    "Leaky RELU  alpha = .1\n",
    "RHO = .99\n",
    "Train accuracy dropped by 7% (good)  \n",
    "Test accuracy up by 1% (good)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round: 0  Test: 0.4281361097  Train: 0.435055038103\n",
      "Round: 10  Test: 0.594718131031  Train: 0.644199830652\n",
      "Round: 20  Test: 0.65718638903  Train: 0.71566469094\n",
      "Round: 30  Test: 0.680548501778  Train: 0.755122777307\n",
      "Round: 40  Test: 0.693245302184  Train: 0.781202370872\n",
      "Round: 50  Test: 0.703402742509  Train: 0.798814563929\n",
      "Round: 60  Test: 0.715083798883  Train: 0.815410668925\n",
      "Round: 70  Test: 0.720162519045  Train: 0.834546994073\n",
      "Round: 80  Test: 0.722701879126  Train: 0.846909398815\n",
      "Round: 90  Test: 0.72625698324  Train: 0.859610499577\n",
      "Round: 100  Test: 0.735906551549  Train: 0.867231160034\n",
      "Round: 110  Test: 0.728288471305  Train: 0.875867908552\n",
      "Round: 120  Test: 0.726764855256  Train: 0.88433530906\n",
      "Round: 130  Test: 0.731335703403  Train: 0.890431837426\n",
      "Round: 140  Test: 0.732859319451  Train: 0.898391193903\n",
      "Round: 150  Test: 0.733875063484  Train: 0.904487722269\n",
      "Round: 160  Test: 0.732859319451  Train: 0.908721422523\n",
      "Round: 170  Test: 0.730827831386  Train: 0.914140558848\n",
      "Round: 180  Test: 0.732859319451  Train: 0.91600338696\n",
      "Round: 190  Test: 0.732351447435  Train: 0.920575783235\n",
      "Round: 200  Test: 0.729812087354  Train: 0.922607959356\n",
      "Round: 210  Test: 0.726764855256  Train: 0.9266723116\n",
      "Round: 220  Test: 0.732351447435  Train: 0.929043183743\n",
      "Round: 230  Test: 0.725241239208  Train: 0.931075359865\n",
      "Round: 240  Test: 0.722701879126  Train: 0.934292972058\n",
      "Round: 250  Test: 0.725749111224  Train: 0.934801016088\n",
      "Round: 260  Test: 0.723209751143  Train: 0.937510584251\n",
      "Round: 270  Test: 0.723209751143  Train: 0.940389500423\n",
      "Round: 280  Test: 0.721686135094  Train: 0.941574936494\n",
      "Round: 290  Test: 0.719146775013  Train: 0.942082980525\n",
      "Round: 300  Test: 0.723209751143  Train: 0.944453852667\n",
      "Wall time: 2min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\"\"\" glorot 4-layer: dropout, dropout, dropout \"\"\"\n",
    "\n",
    "import theano\n",
    "from theano import tensor as T\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "import numpy as np\n",
    "\n",
    "srng = RandomStreams()\n",
    "\n",
    "def floatX(X):\n",
    "    return np.asarray(X, dtype=theano.config.floatX)\n",
    "\n",
    "def init_weights(shape):\n",
    "    (h, w) = shape\n",
    "    # Glorot normalization - last factor depends on non-linearity\n",
    "    # 0.25 for sigmoid and 0.1 for softmax, 1.0 for tanh or Relu\n",
    "    normalizer = 2.0 * np.sqrt(6) / np.sqrt(h + w) * 1.0\n",
    "    return theano.shared(floatX((np.random.random_sample(shape) - 0.5) * normalizer))\n",
    "\n",
    "\n",
    "def rectify(X, alpha=1.0):\n",
    "#     return T.maximum(X, 0.)\n",
    "    return T.maximum(X, 0.1*X)  #leaky rectifier\n",
    "#     return T.switch(X > 0, X, alpha * (T.exp(X) - 1)) # ELU\n",
    "\n",
    "def softmax(X):\n",
    "    e_x = T.exp(X - X.max(axis=1).dimshuffle(0, 'x'))\n",
    "    return e_x / e_x.sum(axis=1).dimshuffle(0, 'x')\n",
    "\n",
    "def RMSprop(cost, params, lr=0.001, rho=0.99, epsilon=1e-6):\n",
    "    grads = T.grad(cost=cost, wrt=params)\n",
    "    updates = []\n",
    "    for p, g in zip(params, grads):\n",
    "        acc = theano.shared(p.get_value() * 0.)\n",
    "        acc_new = rho * acc + (1 - rho) * g ** 2\n",
    "        gradient_scaling = T.sqrt(acc_new + epsilon)\n",
    "        g = g / gradient_scaling\n",
    "        updates.append((acc, acc_new))\n",
    "        updates.append((p, p - lr * g))\n",
    "    return updates\n",
    "\n",
    "def dropout(X, p=0.):\n",
    "    if p > 0:\n",
    "        retain_prob = 1 - p\n",
    "        X *= srng.binomial(X.shape, p=retain_prob, dtype=theano.config.floatX)\n",
    "        X /= retain_prob\n",
    "    return X\n",
    "\n",
    "def model(X, w_h, b_h, w_h2, b_h2, w_o, p_drop_input, p_drop_hidden):\n",
    "    X = dropout(X, p_drop_input)\n",
    "    h = rectify(T.dot(X, w_h) + b_h)\n",
    "\n",
    "    h = dropout(h, p_drop_hidden)\n",
    "    h2 = rectify(T.dot(h, w_h2) + b_h2)\n",
    "\n",
    "    h2 = dropout(h2, p_drop_hidden)\n",
    "    py_x = softmax(T.dot(h2, w_o))\n",
    "    return h, h2, py_x\n",
    "\n",
    "\n",
    "\n",
    "X = T.fmatrix()\n",
    "Y = T.fmatrix()\n",
    "\n",
    "w_h = init_weights((len(vectorizer.vocabulary_), 600))\n",
    "b_h = theano.shared(floatX(np.zeros(600,)))\n",
    "w_h2 = init_weights((600, 600))\n",
    "b_h2 = theano.shared(floatX(np.zeros(600,)))\n",
    "w_o = init_weights((600, yTest.shape[1]))\n",
    "# b_values = numpy.zeros((600,), dtype=theano.config.floatX)\n",
    "\n",
    "noise_h, noise_h2, noise_py_x = model(X, w_h, b_h, w_h2, b_h2, w_o, 0.2, 0.5)\n",
    "h, h2, py_x = model(X, w_h, b_h, w_h2, b_h2, w_o, 0., 0.)\n",
    "y_x = T.argmax(py_x, axis=1)\n",
    "\n",
    "\n",
    "cost = T.mean(T.nnet.categorical_crossentropy(noise_py_x, Y))\n",
    "params = [w_h, b_h, w_h2, b_h2, w_o]\n",
    "updates = RMSprop(cost, params, lr=0.0001)\n",
    "\n",
    "train = theano.function(inputs=[X, Y], outputs=cost, updates=updates, allow_input_downcast=True)\n",
    "predict = theano.function(inputs=[X], outputs=y_x, allow_input_downcast=True)\n",
    "\n",
    "for i in range(301):\n",
    "    for start, end in zip(range(0, len(xTrain), 128), range(128, len(xTrain), 128)):\n",
    "        cost = train(xTrain[start:end], yTrain[start:end])\n",
    "\n",
    "    if i%10 == 0: \n",
    "        tr = np.mean(np.argmax(yTest, axis=1) == predict(xTest))\n",
    "        trr =  np.mean(np.argmax(yTrain, axis=1) == predict(xTrain))\n",
    "        print 'Round:', i,\" Test:\", tr, ' Train:', trr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, batch normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG: nvcc STDOUT mod.cu\n",
      "   Creating library C:/Users/hollis_win/AppData/Local/Theano/compiledir_Windows-10-10.0.10586-Intel64_Family_6_Model_58_Stepping_9_GenuineIntel-2.7.11-64/tmptwqeg4/c7585c6abe0348b5e3c9444eb9e46735.lib and object C:/Users/hollis_win/AppData/Local/Theano/compiledir_Windows-10-10.0.10586-Intel64_Family_6_Model_58_Stepping_9_GenuineIntel-2.7.11-64/tmptwqeg4/c7585c6abe0348b5e3c9444eb9e46735.exp\n",
      "\n",
      "DEBUG: nvcc STDOUT mod.cu\n",
      "   Creating library C:/Users/hollis_win/AppData/Local/Theano/compiledir_Windows-10-10.0.10586-Intel64_Family_6_Model_58_Stepping_9_GenuineIntel-2.7.11-64/tmpexvlpd/0a62a215d8ac7d63cdc696727d41b45b.lib and object C:/Users/hollis_win/AppData/Local/Theano/compiledir_Windows-10-10.0.10586-Intel64_Family_6_Model_58_Stepping_9_GenuineIntel-2.7.11-64/tmpexvlpd/0a62a215d8ac7d63cdc696727d41b45b.exp\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round: 0  Test: 0.430167597765  Train: 0.462489415749\n",
      "Round: 10  Test: 0.601828339259  Train: 0.654022015241\n",
      "Round: 20  Test: 0.664804469274  Train: 0.740220152413\n",
      "Round: 30  Test: 0.695276790249  Train: 0.786960203218\n",
      "Round: 40  Test: 0.708481462671  Train: 0.811515664691\n",
      "Round: 50  Test: 0.726764855256  Train: 0.839796782388\n",
      "Round: 60  Test: 0.732351447435  Train: 0.860118543607\n",
      "Round: 70  Test: 0.739969527679  Train: 0.875867908552\n",
      "Round: 80  Test: 0.740477399695  Train: 0.888907705334\n",
      "Round: 90  Test: 0.740477399695  Train: 0.900254022015\n",
      "Round: 100  Test: 0.73844591163  Train: 0.907874682472\n"
     ]
    }
   ],
   "source": [
    "\"\"\" glorot 4-layer: batch, dropout, dropout \"\"\"\n",
    "\n",
    "# batch normalization code adapted from \n",
    "# https://groups.google.com/forum/#!topic/theano-users/dMV6aabL1Ds \n",
    "\n",
    "\n",
    "import theano\n",
    "from theano import tensor as T\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "from theano.tensor.nnet.bn import batch_normalization\n",
    "import numpy as np\n",
    "\n",
    "srng = RandomStreams()\n",
    "\n",
    "def floatX(X):\n",
    "    return np.asarray(X, dtype=theano.config.floatX)\n",
    "\n",
    "def init_weights(shape):\n",
    "    (h, w) = shape\n",
    "    # Glorot normalization - last factor depends on non-linearity\n",
    "    # 0.25 for sigmoid and 0.1 for softmax, 1.0 for tanh or Relu\n",
    "    normalizer = 2.0 * np.sqrt(6) / np.sqrt(h + w) * 1.0\n",
    "    return theano.shared(floatX((np.random.random_sample(shape) - 0.5) * normalizer))\n",
    "\n",
    "def rectify(X, alpha=1.0):\n",
    "#     return T.maximum(X, 0.)\n",
    "    return T.maximum(X, 0.1*X)  #leaky rectifier\n",
    "#     return T.switch(X > 0, X, alpha * (T.exp(X) - 1)) # ELU\n",
    "\n",
    "def softmax(X):\n",
    "    e_x = T.exp(X - X.max(axis=1).dimshuffle(0, 'x'))\n",
    "    return e_x / e_x.sum(axis=1).dimshuffle(0, 'x')\n",
    "\n",
    "def RMSprop(cost, params, lr=0.001, rho=0.99, epsilon=1e-6):\n",
    "    grads = T.grad(cost=cost, wrt=params)\n",
    "    updates = []\n",
    "    for p, g in zip(params, grads):\n",
    "        acc = theano.shared(p.get_value() * 0.)\n",
    "        acc_new = rho * acc + (1 - rho) * g ** 2\n",
    "        gradient_scaling = T.sqrt(acc_new + epsilon)\n",
    "        g = g / gradient_scaling\n",
    "        updates.append((acc, acc_new))\n",
    "        updates.append((p, p - lr * g))\n",
    "    return updates\n",
    "\n",
    "def dropout(X, p=0.):\n",
    "    if p > 0:\n",
    "        retain_prob = 1 - p\n",
    "        X *= srng.binomial(X.shape, p=retain_prob, dtype=theano.config.floatX)\n",
    "        X /= retain_prob\n",
    "    return X\n",
    "\n",
    "def model(X, w_h, b_h, gamma, beta, w_h2, b_h2, w_o, p_drop_input, p_drop_hidden):\n",
    "    X = T.dot(X, w_h) + b_h\n",
    "    mean = X.mean((0,), keepdims=True)\n",
    "    std = T.ones_like(X.var((0,), keepdims = True))\n",
    "    X = batch_normalization(X, gamma= gamma, beta= beta, \n",
    "                            mean= mean, #X.mean((0,), keepdims=True), \n",
    "                            std= std, mode='high_mem')    \n",
    "    h = rectify(X)\n",
    "\n",
    "    h = dropout(h, p_drop_hidden)\n",
    "    h2 = rectify(T.dot(h, w_h2) + b_h2)\n",
    "\n",
    "    h2 = dropout(h2, p_drop_hidden)\n",
    "    py_x = softmax(T.dot(h2, w_o))\n",
    "    return h, h2, py_x\n",
    "\n",
    "\n",
    "\n",
    "X = T.fmatrix()\n",
    "Y = T.fmatrix()\n",
    "\n",
    "w_h = init_weights((len(vectorizer.vocabulary_), 600))\n",
    "b_h = theano.shared(floatX(np.zeros(600)))\n",
    "gamma = theano.shared(floatX(np.ones(600)))\n",
    "beta = theano.shared(floatX(np.zeros(600)))\n",
    "\n",
    "w_h2 = init_weights((600, 600))\n",
    "b_h2 = theano.shared(floatX(np.zeros(600,)))\n",
    "w_o = init_weights((600, yTest.shape[1]))\n",
    "\n",
    "noise_h, noise_h2, noise_py_x = model(X, w_h, b_h, gamma, beta, w_h2, b_h2, w_o, 0.2, 0.5)\n",
    "h, h2, py_x = model(X, w_h, b_h, gamma, beta, w_h2, b_h2, w_o, 0., 0.)\n",
    "y_x = T.argmax(py_x, axis=1)\n",
    "\n",
    "\n",
    "cost = T.mean(T.nnet.categorical_crossentropy(noise_py_x, Y))\n",
    "params = [w_h, b_h, gamma, beta, w_h2, b_h2, w_o]\n",
    "updates = RMSprop(cost, params, lr=0.0001)\n",
    "\n",
    "train = theano.function(inputs=[X, Y], outputs=cost, updates=updates, allow_input_downcast=True)\n",
    "predict = theano.function(inputs=[X], outputs=y_x, allow_input_downcast=True)\n",
    "\n",
    "for i in range(101):\n",
    "    for start, end in zip(range(0, len(xTrain), 128), range(128, len(xTrain), 128)):\n",
    "        cost = train(xTrain[start:end], yTrain[start:end])\n",
    "\n",
    "    if i%10 == 0: \n",
    "        tr = np.mean(np.argmax(yTest, axis=1) == predict(xTest))\n",
    "        trr =  np.mean(np.argmax(yTrain, axis=1) == predict(xTrain))\n",
    "        print 'Round:', i,\" Test:\", tr, ' Train:', trr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 74% !!! \n",
    "\n",
    "Lets see if thats repeatable.... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round: 0  Test: 0.419502285424  Train: 0.459102455546\n",
      "Round: 5  Test: 0.539867953276  Train: 0.585266723116\n",
      "Round: 10  Test: 0.598781107161  Train: 0.661473327688\n",
      "Round: 15  Test: 0.647536820721  Train: 0.706689246401\n",
      "Round: 20  Test: 0.668359573388  Train: 0.743099068586\n",
      "Round: 25  Test: 0.687658710005  Train: 0.766299745978\n",
      "Round: 30  Test: 0.695784662265  Train: 0.788484335309\n",
      "Round: 35  Test: 0.704418486541  Train: 0.80033869602\n",
      "Round: 40  Test: 0.715083798883  Train: 0.812870448772\n",
      "Round: 45  Test: 0.717623158964  Train: 0.828281117697\n",
      "Round: 50  Test: 0.723717623159  Train: 0.838441998307\n",
      "Round: 55  Test: 0.725241239208  Train: 0.848602878916\n",
      "Round: 60  Test: 0.730827831386  Train: 0.856392887384\n",
      "Round: 65  Test: 0.734890807517  Train: 0.865876375953\n",
      "Round: 70  Test: 0.737430167598  Train: 0.876883996613\n",
      "Round: 75  Test: 0.737430167598  Train: 0.881964436918\n",
      "Round: 80  Test: 0.73844591163  Train: 0.890093141406\n",
      "Round: 85  Test: 0.740985271712  Train: 0.89466553768\n",
      "Round: 90  Test: 0.735398679533  Train: 0.900423370025\n",
      "Round: 95  Test: 0.734890807517  Train: 0.90482641829\n",
      "Round: 100  Test: 0.733367191468  Train: 0.909060118544\n"
     ]
    }
   ],
   "source": [
    "\"\"\" glorot 4-layer: batch, dropout, dropout \"\"\"\n",
    "\n",
    "# batch normalization code adapted from \n",
    "# https://groups.google.com/forum/#!topic/theano-users/dMV6aabL1Ds \n",
    "\n",
    "\n",
    "import theano\n",
    "from theano import tensor as T\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "from theano.tensor.nnet.bn import batch_normalization\n",
    "import numpy as np\n",
    "\n",
    "srng = RandomStreams()\n",
    "\n",
    "def floatX(X):\n",
    "    return np.asarray(X, dtype=theano.config.floatX)\n",
    "\n",
    "def init_weights(shape):\n",
    "    (h, w) = shape\n",
    "    # Glorot normalization - last factor depends on non-linearity\n",
    "    # 0.25 for sigmoid and 0.1 for softmax, 1.0 for tanh or Relu\n",
    "    normalizer = 2.0 * np.sqrt(6) / np.sqrt(h + w) * 1.0\n",
    "    return theano.shared(floatX((np.random.random_sample(shape) - 0.5) * normalizer))\n",
    "\n",
    "def rectify(X, alpha=1.0):\n",
    "#     return T.maximum(X, 0.)\n",
    "    return T.maximum(X, 0.1*X)  #leaky rectifier\n",
    "#     return T.switch(X > 0, X, alpha * (T.exp(X) - 1)) # ELU\n",
    "\n",
    "def softmax(X):\n",
    "    e_x = T.exp(X - X.max(axis=1).dimshuffle(0, 'x'))\n",
    "    return e_x / e_x.sum(axis=1).dimshuffle(0, 'x')\n",
    "\n",
    "def RMSprop(cost, params, lr=0.001, rho=0.99, epsilon=1e-6):\n",
    "    grads = T.grad(cost=cost, wrt=params)\n",
    "    updates = []\n",
    "    for p, g in zip(params, grads):\n",
    "        acc = theano.shared(p.get_value() * 0.)\n",
    "        acc_new = rho * acc + (1 - rho) * g ** 2\n",
    "        gradient_scaling = T.sqrt(acc_new + epsilon)\n",
    "        g = g / gradient_scaling\n",
    "        updates.append((acc, acc_new))\n",
    "        updates.append((p, p - lr * g))\n",
    "    return updates\n",
    "\n",
    "def dropout(X, p=0.):\n",
    "    if p > 0:\n",
    "        retain_prob = 1 - p\n",
    "        X *= srng.binomial(X.shape, p=retain_prob, dtype=theano.config.floatX)\n",
    "        X /= retain_prob\n",
    "    return X\n",
    "\n",
    "def model(X, w_h, b_h, gamma, beta, w_h2, b_h2, w_o, p_drop_input, p_drop_hidden):\n",
    "    X = T.dot(X, w_h) + b_h\n",
    "    mean = X.mean((0,), keepdims=True)\n",
    "    std = T.ones_like(X.var((0,), keepdims = True))\n",
    "    X = batch_normalization(X, gamma= gamma, beta= beta, \n",
    "                            mean= mean, #X.mean((0,), keepdims=True), \n",
    "                            std= std, mode='high_mem')    \n",
    "    h = rectify(X)\n",
    "\n",
    "    h = dropout(h, p_drop_hidden)\n",
    "    h2 = rectify(T.dot(h, w_h2) + b_h2)\n",
    "\n",
    "    h2 = dropout(h2, p_drop_hidden)\n",
    "    py_x = softmax(T.dot(h2, w_o))\n",
    "    return h, h2, py_x\n",
    "\n",
    "\n",
    "\n",
    "X = T.fmatrix()\n",
    "Y = T.fmatrix()\n",
    "\n",
    "w_h = init_weights((len(vectorizer.vocabulary_), 600))\n",
    "b_h = theano.shared(floatX(np.zeros(600)))\n",
    "gamma = theano.shared(floatX(np.ones(600)))\n",
    "beta = theano.shared(floatX(np.zeros(600)))\n",
    "\n",
    "w_h2 = init_weights((600, 600))\n",
    "b_h2 = theano.shared(floatX(np.zeros(600,)))\n",
    "w_o = init_weights((600, yTest.shape[1]))\n",
    "\n",
    "noise_h, noise_h2, noise_py_x = model(X, w_h, b_h, gamma, beta, w_h2, b_h2, w_o, 0.2, 0.5)\n",
    "h, h2, py_x = model(X, w_h, b_h, gamma, beta, w_h2, b_h2, w_o, 0., 0.)\n",
    "y_x = T.argmax(py_x, axis=1)\n",
    "\n",
    "\n",
    "cost = T.mean(T.nnet.categorical_crossentropy(noise_py_x, Y))\n",
    "params = [w_h, b_h, gamma, beta, w_h2, b_h2, w_o]\n",
    "updates = RMSprop(cost, params, lr=0.0001)\n",
    "\n",
    "train = theano.function(inputs=[X, Y], outputs=cost, updates=updates, allow_input_downcast=True)\n",
    "predict = theano.function(inputs=[X], outputs=y_x, allow_input_downcast=True)\n",
    "\n",
    "for i in range(101):\n",
    "    for start, end in zip(range(0, len(xTrain), 128), range(128, len(xTrain), 128)):\n",
    "        cost = train(xTrain[start:end], yTrain[start:end])\n",
    "\n",
    "    if i%5 == 0: \n",
    "        tr = np.mean(np.argmax(yTest, axis=1) == predict(xTest))\n",
    "        trr =  np.mean(np.argmax(yTrain, axis=1) == predict(xTrain))\n",
    "        print 'Round:', i,\" Test:\", tr, ' Train:', trr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 74% again!\n",
    "\n",
    "Next, 3-layer batch normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 970 (CNMeM is disabled, CuDNN not available)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round: 0  Test: 0.483494159472  Train: 0.528196443692\n",
      "Round: 1  Test: 0.527171152869  Train: 0.573581710415\n",
      "Round: 2  Test: 0.566277298121  Train: 0.613547840813\n",
      "Round: 3  Test: 0.596749619096  Train: 0.650973751058\n",
      "Round: 4  Test: 0.617572371762  Train: 0.681625740898\n",
      "Round: 5  Test: 0.641442356526  Train: 0.710584250635\n",
      "Round: 6  Test: 0.656678517014  Train: 0.731414055885\n",
      "Round: 7  Test: 0.668359573388  Train: 0.750211685013\n",
      "Round: 8  Test: 0.675977653631  Train: 0.764944961897\n",
      "Round: 9  Test: 0.688674454038  Train: 0.776121930567\n",
      "Round: 10  Test: 0.694768918233  Train: 0.791363251482\n",
      "Round: 11  Test: 0.70187912646  Train: 0.802201524132\n",
      "Round: 12  Test: 0.713052310818  Train: 0.813378492803\n",
      "Round: 13  Test: 0.717115286948  Train: 0.819983065199\n",
      "Round: 14  Test: 0.716099542915  Train: 0.828281117697\n",
      "Round: 15  Test: 0.72219400711  Train: 0.836409822185\n",
      "Round: 16  Test: 0.724225495175  Train: 0.845893310754\n",
      "Round: 17  Test: 0.723209751143  Train: 0.85249788315\n",
      "Round: 18  Test: 0.726764855256  Train: 0.858594411516\n",
      "Round: 19  Test: 0.729812087354  Train: 0.86316680779\n",
      "Round: 20  Test: 0.731335703403  Train: 0.868755292125\n",
      "Round: 21  Test: 0.731335703403  Train: 0.8733276884\n",
      "Round: 22  Test: 0.73031995937  Train: 0.878238780694\n",
      "Round: 23  Test: 0.730827831386  Train: 0.883488569009\n",
      "Round: 24  Test: 0.730827831386  Train: 0.889246401355\n",
      "Round: 25  Test: 0.73031995937  Train: 0.89449618967\n",
      "Round: 26  Test: 0.729304215338  Train: 0.896528365792\n",
      "Round: 27  Test: 0.728796343321  Train: 0.900254022015\n",
      "Round: 28  Test: 0.728288471305  Train: 0.903302286198\n",
      "Round: 29  Test: 0.728288471305  Train: 0.906181202371\n",
      "Round: 30  Test: 0.727780599289  Train: 0.909398814564\n",
      "Round: 31  Test: 0.728288471305  Train: 0.912447078747\n",
      "Round: 32  Test: 0.728796343321  Train: 0.91583403895\n",
      "Round: 33  Test: 0.727780599289  Train: 0.919220999153\n",
      "Round: 34  Test: 0.727780599289  Train: 0.921930567316\n",
      "Round: 35  Test: 0.727780599289  Train: 0.924132091448\n",
      "Round: 36  Test: 0.72625698324  Train: 0.92684165961\n",
      "Round: 37  Test: 0.72625698324  Train: 0.928196443692\n",
      "Round: 38  Test: 0.725749111224  Train: 0.929720575783\n",
      "Round: 39  Test: 0.726764855256  Train: 0.931075359865\n",
      "Round: 40  Test: 0.72625698324  Train: 0.932599491956\n",
      "Round: 41  Test: 0.724225495175  Train: 0.934123624047\n",
      "Round: 42  Test: 0.723717623159  Train: 0.935986452159\n",
      "Round: 43  Test: 0.723717623159  Train: 0.937679932261\n",
      "Round: 44  Test: 0.723209751143  Train: 0.938187976291\n",
      "Round: 45  Test: 0.721178263078  Train: 0.938865368332\n",
      "Round: 46  Test: 0.722701879126  Train: 0.940050804403\n",
      "Round: 47  Test: 0.721686135094  Train: 0.941066892464\n",
      "Round: 48  Test: 0.721686135094  Train: 0.941744284505\n",
      "Round: 49  Test: 0.72219400711  Train: 0.941913632515\n"
     ]
    }
   ],
   "source": [
    "\"\"\" glorot 4-layer: batch, batch, batch \"\"\"\n",
    "\n",
    "# batch normalization code adapted from \n",
    "# https://groups.google.com/forum/#!topic/theano-users/dMV6aabL1Ds \n",
    "\n",
    "\n",
    "import theano\n",
    "from theano import tensor as T\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "from theano.tensor.nnet.bn import batch_normalization\n",
    "import numpy as np\n",
    "\n",
    "srng = RandomStreams()\n",
    "\n",
    "def floatX(X):\n",
    "    return np.asarray(X, dtype=theano.config.floatX)\n",
    "\n",
    "def init_weights(shape):\n",
    "    (h, w) = shape\n",
    "    # Glorot normalization - last factor depends on non-linearity\n",
    "    # 0.25 for sigmoid and 0.1 for softmax, 1.0 for tanh or Relu\n",
    "    normalizer = 2.0 * np.sqrt(6) / np.sqrt(h + w) * 1.0\n",
    "    return theano.shared(floatX((np.random.random_sample(shape) - 0.5) * normalizer))\n",
    "\n",
    "def rectify(X, alpha=1.0):\n",
    "#     return T.maximum(X, 0.)\n",
    "    return T.maximum(X, 0.1*X)  #leaky rectifier\n",
    "#     return T.switch(X > 0, X, alpha * (T.exp(X) - 1)) # ELU\n",
    "\n",
    "def softmax(X):\n",
    "    e_x = T.exp(X - X.max(axis=1).dimshuffle(0, 'x'))\n",
    "    return e_x / e_x.sum(axis=1).dimshuffle(0, 'x')\n",
    "\n",
    "def RMSprop(cost, params, lr=0.001, rho=0.99, epsilon=1e-6):\n",
    "    grads = T.grad(cost=cost, wrt=params)\n",
    "    updates = []\n",
    "    for p, g in zip(params, grads):\n",
    "        acc = theano.shared(p.get_value() * 0.)\n",
    "        acc_new = rho * acc + (1 - rho) * g ** 2\n",
    "        gradient_scaling = T.sqrt(acc_new + epsilon)\n",
    "        g = g / gradient_scaling\n",
    "        updates.append((acc, acc_new))\n",
    "        updates.append((p, p - lr * g))\n",
    "    return updates\n",
    "\n",
    "\n",
    "def model(X, w_h, b_h, g_h, bb_h, w_h2, b_h2, g_h2, bb_h2, w_o, b_ho, g_ho, bb_ho):\n",
    "    X = T.dot(X, w_h) + b_h\n",
    "    X = batch_normalization(X, gamma= g_h, beta= bb_h, \n",
    "                            mean= X.mean((0,), keepdims=True),\n",
    "                            std= T.ones_like(X.var((0,), keepdims = True)), \n",
    "                            mode='high_mem')    \n",
    "    h = rectify(X)\n",
    "\n",
    "    h  = T.dot(h, w_h2) + b_h2\n",
    "    h = batch_normalization(h, gamma= g_h2, beta= bb_h2, \n",
    "                            mean= h.mean((0,), keepdims=True),\n",
    "                            std= T.ones_like(h.var((0,), keepdims = True)), \n",
    "                            mode='high_mem')       \n",
    "    h2 = rectify(h)\n",
    "\n",
    "    h2 = T.dot(h2, w_o) + b_ho\n",
    "    h2 = batch_normalization(h2, gamma= g_ho, beta= bb_ho, \n",
    "                            mean= h2.mean((0,), keepdims=True),\n",
    "                            std= T.ones_like(h2.var((0,), keepdims = True)), \n",
    "                            mode='high_mem')   \n",
    "    py_x = softmax(h2)\n",
    "    return h, h2, py_x\n",
    "\n",
    "\n",
    "X = T.fmatrix()\n",
    "Y = T.fmatrix()\n",
    "\n",
    "h1_size = 600\n",
    "h2_size = 550\n",
    "\n",
    "w_h = init_weights((len(vectorizer.vocabulary_), h1_size))\n",
    "b_h = theano.shared(floatX(np.zeros(h1_size)))\n",
    "g_h = theano.shared(floatX(np.ones(h1_size)))\n",
    "bb_h = theano.shared(floatX(np.zeros(h1_size)))\n",
    "\n",
    "w_h2 = init_weights((h1_size, h2_size))\n",
    "b_h2 = theano.shared(floatX(np.zeros(h2_size)))\n",
    "g_h2 = theano.shared(floatX(np.ones(h2_size)))\n",
    "bb_h2 = theano.shared(floatX(np.zeros(h2_size)))\n",
    "\n",
    "w_o = init_weights((h2_size, yTest.shape[1]))\n",
    "b_ho = theano.shared(floatX(np.zeros(yTest.shape[1])))\n",
    "g_ho = theano.shared(floatX(np.ones(yTest.shape[1])))\n",
    "bb_ho = theano.shared(floatX(np.zeros(yTest.shape[1])))\n",
    "\n",
    "noise_h, noise_h2, noise_py_x = model(X, w_h, b_h, g_h, bb_h, \n",
    "                                      w_h2, b_h2, g_h2, bb_h2, \n",
    "                                      w_o, b_ho, g_ho, bb_ho)\n",
    "\n",
    "h, h2, py_x = model(X, w_h, b_h, g_h, bb_h, \n",
    "                    w_h2, b_h2, g_h2, bb_h2, \n",
    "                    w_o, b_ho, g_ho, bb_ho)\n",
    "\n",
    "y_x = T.argmax(py_x, axis=1)\n",
    "\n",
    "\n",
    "cost = T.mean(T.nnet.categorical_crossentropy(noise_py_x, Y))\n",
    "params = [w_h, b_h, g_h, bb_h, w_h2, b_h2, g_h2, bb_h2, w_o, b_ho, g_ho, bb_ho]\n",
    "updates = RMSprop(cost, params, lr=0.0001)\n",
    "\n",
    "train = theano.function(inputs=[X, Y], outputs=cost, updates=updates, allow_input_downcast=True)\n",
    "predict = theano.function(inputs=[X], outputs=y_x, allow_input_downcast=True)\n",
    "\n",
    "batch_size = 60\n",
    "\n",
    "for i in range(50):\n",
    "    for start, end in zip(range(0, len(xTrain), batch_size), range(batch_size, len(xTrain), batch_size)):\n",
    "        cost = train(xTrain[start:end], yTrain[start:end])\n",
    "\n",
    "    tr = np.mean(np.argmax(yTest, axis=1) == predict(xTest))\n",
    "    trr =  np.mean(np.argmax(yTrain, axis=1) == predict(xTrain))\n",
    "    print 'Round:', i,\" Test:\", tr, ' Train:', trr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3558 5373 1030]\n",
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "[[ 0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  1.  0.]]\n"
     ]
    }
   ],
   "source": [
    "# random indexing for batches \n",
    "\n",
    "index = np.random.choice(xTrain.shape[0], 3)\n",
    "print index\n",
    "print xTrain[index]\n",
    "print yTrain[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round: 0  Test: 0.479939055358  Train: 0.51532599492\n",
      "Round: 1  Test: 0.515490096496  Train: 0.5583403895\n",
      "Round: 2  Test: 0.558151345861  Train: 0.613547840813\n",
      "Round: 3  Test: 0.583544946673  Train: 0.638272650296\n",
      "Round: 4  Test: 0.605383443372  Train: 0.667908552075\n",
      "Round: 5  Test: 0.633316404266  Train: 0.698221845893\n",
      "Round: 6  Test: 0.660741493144  Train: 0.725486875529\n",
      "Round: 7  Test: 0.671914677501  Train: 0.744453852667\n",
      "Round: 8  Test: 0.682579989843  Train: 0.762574089754\n",
      "Round: 9  Test: 0.684103605891  Train: 0.769348010161\n",
      "Round: 10  Test: 0.692229558151  Train: 0.785944115157\n",
      "Round: 11  Test: 0.701371254444  Train: 0.79966130398\n",
      "Round: 12  Test: 0.70594210259  Train: 0.806943268417\n",
      "Round: 13  Test: 0.71406805485  Train: 0.818120237087\n",
      "Round: 14  Test: 0.719146775013  Train: 0.824386113463\n",
      "Round: 15  Test: 0.717115286948  Train: 0.834716342083\n",
      "Round: 16  Test: 0.724733367191  Train: 0.844538526672\n",
      "Round: 17  Test: 0.724225495175  Train: 0.853852667231\n",
      "Round: 18  Test: 0.721178263078  Train: 0.85300592718\n",
      "Round: 19  Test: 0.726764855256  Train: 0.860287891617\n",
      "Round: 20  Test: 0.731843575419  Train: 0.864182895851\n",
      "Round: 21  Test: 0.724225495175  Train: 0.872311600339\n",
      "Round: 22  Test: 0.724225495175  Train: 0.878069432684\n",
      "Round: 23  Test: 0.729812087354  Train: 0.879762912786\n",
      "Round: 24  Test: 0.733875063484  Train: 0.887214225233\n"
     ]
    }
   ],
   "source": [
    "\"\"\" glorot 4-layer: batch, batch, batch \n",
    "    random indexing                       \"\"\"\n",
    "\n",
    "# batch normalization code adapted from \n",
    "# https://groups.google.com/forum/#!topic/theano-users/dMV6aabL1Ds \n",
    "\n",
    "\n",
    "import theano\n",
    "from theano import tensor as T\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "from theano.tensor.nnet.bn import batch_normalization\n",
    "import numpy as np\n",
    "\n",
    "srng = RandomStreams()\n",
    "\n",
    "def floatX(X):\n",
    "    return np.asarray(X, dtype=theano.config.floatX)\n",
    "\n",
    "def init_weights(shape):\n",
    "    (h, w) = shape\n",
    "    # Glorot normalization - last factor depends on non-linearity\n",
    "    # 0.25 for sigmoid and 0.1 for softmax, 1.0 for tanh or Relu\n",
    "    normalizer = 2.0 * np.sqrt(6) / np.sqrt(h + w) * 1.0\n",
    "    return theano.shared(floatX((np.random.random_sample(shape) - 0.5) * normalizer))\n",
    "\n",
    "def rectify(X, alpha=1.0):\n",
    "#     return T.maximum(X, 0.)\n",
    "    return T.maximum(X, 0.1*X)  #leaky rectifier\n",
    "#     return T.switch(X > 0, X, alpha * (T.exp(X) - 1)) # ELU\n",
    "\n",
    "def softmax(X):\n",
    "    e_x = T.exp(X - X.max(axis=1).dimshuffle(0, 'x'))\n",
    "    return e_x / e_x.sum(axis=1).dimshuffle(0, 'x')\n",
    "\n",
    "def RMSprop(cost, params, lr=0.001, rho=0.99, epsilon=1e-6):\n",
    "    grads = T.grad(cost=cost, wrt=params)\n",
    "    updates = []\n",
    "    for p, g in zip(params, grads):\n",
    "        acc = theano.shared(p.get_value() * 0.)\n",
    "        acc_new = rho * acc + (1 - rho) * g ** 2\n",
    "        gradient_scaling = T.sqrt(acc_new + epsilon)\n",
    "        g = g / gradient_scaling\n",
    "        updates.append((acc, acc_new))\n",
    "        updates.append((p, p - lr * g))\n",
    "    return updates\n",
    "\n",
    "\n",
    "def model(X, w_h, b_h, g_h, bb_h, w_h2, b_h2, g_h2, bb_h2, w_o, b_ho, g_ho, bb_ho):\n",
    "    X = T.dot(X, w_h) + b_h\n",
    "    X = batch_normalization(X, gamma= g_h, beta= bb_h, \n",
    "                            mean= X.mean((0,), keepdims=True),\n",
    "                            std= T.ones_like(X.var((0,), keepdims = True)), \n",
    "                            mode='high_mem')    \n",
    "    h = rectify(X)\n",
    "\n",
    "    h  = T.dot(h, w_h2) + b_h2\n",
    "    h = batch_normalization(h, gamma= g_h2, beta= bb_h2, \n",
    "                            mean= h.mean((0,), keepdims=True),\n",
    "                            std= T.ones_like(h.var((0,), keepdims = True)), \n",
    "                            mode='high_mem')       \n",
    "    h2 = rectify(h)\n",
    "\n",
    "    h2 = T.dot(h2, w_o) + b_ho\n",
    "    h2 = batch_normalization(h2, gamma= g_ho, beta= bb_ho, \n",
    "                            mean= h2.mean((0,), keepdims=True),\n",
    "                            std= T.ones_like(h2.var((0,), keepdims = True)), \n",
    "                            mode='high_mem')   \n",
    "    py_x = softmax(h2)\n",
    "    return h, h2, py_x\n",
    "\n",
    "\n",
    "X = T.fmatrix()\n",
    "Y = T.fmatrix()\n",
    "\n",
    "h1_size = 600\n",
    "h2_size = 550\n",
    "\n",
    "w_h = init_weights((len(vectorizer.vocabulary_), h1_size))\n",
    "b_h = theano.shared(floatX(np.zeros(h1_size)))\n",
    "g_h = theano.shared(floatX(np.ones(h1_size)))\n",
    "bb_h = theano.shared(floatX(np.zeros(h1_size)))\n",
    "\n",
    "w_h2 = init_weights((h1_size, h2_size))\n",
    "b_h2 = theano.shared(floatX(np.zeros(h2_size)))\n",
    "g_h2 = theano.shared(floatX(np.ones(h2_size)))\n",
    "bb_h2 = theano.shared(floatX(np.zeros(h2_size)))\n",
    "\n",
    "w_o = init_weights((h2_size, yTest.shape[1]))\n",
    "b_ho = theano.shared(floatX(np.zeros(yTest.shape[1])))\n",
    "g_ho = theano.shared(floatX(np.ones(yTest.shape[1])))\n",
    "bb_ho = theano.shared(floatX(np.zeros(yTest.shape[1])))\n",
    "\n",
    "noise_h, noise_h2, noise_py_x = model(X, w_h, b_h, g_h, bb_h, \n",
    "                                      w_h2, b_h2, g_h2, bb_h2, \n",
    "                                      w_o, b_ho, g_ho, bb_ho)\n",
    "\n",
    "h, h2, py_x = model(X, w_h, b_h, g_h, bb_h, \n",
    "                    w_h2, b_h2, g_h2, bb_h2, \n",
    "                    w_o, b_ho, g_ho, bb_ho)\n",
    "\n",
    "y_x = T.argmax(py_x, axis=1)\n",
    "\n",
    "\n",
    "cost = T.mean(T.nnet.categorical_crossentropy(noise_py_x, Y))\n",
    "params = [w_h, b_h, g_h, bb_h, w_h2, b_h2, g_h2, bb_h2, w_o, b_ho, g_ho, bb_ho]\n",
    "updates = RMSprop(cost, params, lr=0.0001)\n",
    "\n",
    "train = theano.function(inputs=[X, Y], outputs=cost, updates=updates, allow_input_downcast=True)\n",
    "predict = theano.function(inputs=[X], outputs=y_x, allow_input_downcast=True)\n",
    "\n",
    "batch_size = 60\n",
    "\n",
    "for i in range(26):\n",
    "    for start, end in zip(range(0, len(xTrain), batch_size), range(batch_size, len(xTrain), batch_size)):\n",
    "        index = np.random.choice(xTrain.shape[0], batch_size, replace=False)\n",
    "        cost = train(xTrain[index], yTrain[index])\n",
    "\n",
    "    tr = np.mean(np.argmax(yTest, axis=1) == predict(xTest))\n",
    "    trr =  np.mean(np.argmax(yTrain, axis=1) == predict(xTrain))\n",
    "    print 'Round:', i,\" Test:\", tr, ' Train:', trr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random idexing (with replacement) no effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round: 0  Test: 0.470797359066  Train: 0.517696867062\n",
      "Round: 1  Test: 0.511427120366  Train: 0.55766299746\n",
      "Round: 2  Test: 0.544438801422  Train: 0.598306519898\n",
      "Round: 3  Test: 0.585068562722  Train: 0.644877222693\n",
      "Round: 4  Test: 0.61655662773  Train: 0.677561388654\n",
      "Round: 5  Test: 0.626714068055  Train: 0.696697713802\n",
      "Round: 6  Test: 0.64499746064  Train: 0.725148179509\n",
      "Round: 7  Test: 0.672422549518  Train: 0.749872988992\n",
      "Round: 8  Test: 0.670391061453  Train: 0.764436917866\n",
      "Round: 9  Test: 0.687658710005  Train: 0.77866215072\n",
      "Round: 10  Test: 0.691721686135  Train: 0.795258255715\n",
      "Round: 11  Test: 0.69781615033  Train: 0.807620660457\n",
      "Round: 12  Test: 0.703910614525  Train: 0.816934801016\n",
      "Round: 13  Test: 0.709497206704  Train: 0.827265029636\n",
      "Round: 14  Test: 0.712036566785  Train: 0.836240474174\n",
      "Round: 15  Test: 0.716099542915  Train: 0.841320914479\n",
      "Round: 16  Test: 0.715591670899  Train: 0.851989839119\n",
      "Round: 17  Test: 0.71406805485  Train: 0.859779847587\n",
      "Round: 18  Test: 0.721686135094  Train: 0.868077900085\n",
      "Round: 19  Test: 0.72219400711  Train: 0.876545300593\n",
      "Round: 20  Test: 0.726764855256  Train: 0.883319220999\n",
      "Round: 21  Test: 0.720670391061  Train: 0.886028789162\n",
      "Round: 22  Test: 0.727780599289  Train: 0.889246401355\n",
      "Round: 23  Test: 0.725241239208  Train: 0.892633361558\n",
      "Round: 24  Test: 0.73031995937  Train: 0.900254022015\n",
      "Round: 25  Test: 0.731335703403  Train: 0.901608806097\n",
      "Round: 26  Test: 0.733367191468  Train: 0.904657070279\n",
      "Round: 27  Test: 0.73031995937  Train: 0.907197290432\n",
      "Round: 28  Test: 0.730827831386  Train: 0.913971210838\n",
      "Round: 29  Test: 0.728796343321  Train: 0.916680779001\n",
      "Round: 30  Test: 0.73031995937  Train: 0.918712955123\n",
      "Round: 31  Test: 0.731843575419  Train: 0.922607959356\n",
      "Round: 32  Test: 0.729812087354  Train: 0.925148179509\n",
      "Round: 33  Test: 0.730827831386  Train: 0.927519051651\n"
     ]
    }
   ],
   "source": [
    "\"\"\" glorot 4-layer: batch, batch, batch \n",
    "    random indexing, ELU                \"\"\"\n",
    "\n",
    "# batch normalization code adapted from \n",
    "# https://groups.google.com/forum/#!topic/theano-users/dMV6aabL1Ds \n",
    "\n",
    "\n",
    "import theano\n",
    "from theano import tensor as T\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "from theano.tensor.nnet.bn import batch_normalization\n",
    "import numpy as np\n",
    "\n",
    "srng = RandomStreams()\n",
    "\n",
    "def floatX(X):\n",
    "    return np.asarray(X, dtype=theano.config.floatX)\n",
    "\n",
    "def init_weights(shape):\n",
    "    (h, w) = shape\n",
    "    # Glorot normalization - last factor depends on non-linearity\n",
    "    # 0.25 for sigmoid and 0.1 for softmax, 1.0 for tanh or Relu\n",
    "    normalizer = 2.0 * np.sqrt(6) / np.sqrt(h + w) * 1.0\n",
    "    return theano.shared(floatX((np.random.random_sample(shape) - 0.5) * normalizer))\n",
    "\n",
    "def rectify(X, alpha=0.01):\n",
    "#     return T.maximum(X, 0.)\n",
    "#    return T.maximum(X, 0.1*X)  #leaky rectifier\n",
    "     return T.switch(X > 0, X, alpha * (T.exp(X) - 1)) # ELU\n",
    "\n",
    "def softmax(X):\n",
    "    e_x = T.exp(X - X.max(axis=1).dimshuffle(0, 'x'))\n",
    "    return e_x / e_x.sum(axis=1).dimshuffle(0, 'x')\n",
    "\n",
    "def RMSprop(cost, params, lr=0.001, rho=0.99, epsilon=1e-6):\n",
    "    grads = T.grad(cost=cost, wrt=params)\n",
    "    updates = []\n",
    "    for p, g in zip(params, grads):\n",
    "        acc = theano.shared(p.get_value() * 0.)\n",
    "        acc_new = rho * acc + (1 - rho) * g ** 2\n",
    "        gradient_scaling = T.sqrt(acc_new + epsilon)\n",
    "        g = g / gradient_scaling\n",
    "        updates.append((acc, acc_new))\n",
    "        updates.append((p, p - lr * g))\n",
    "    return updates\n",
    "\n",
    "\n",
    "def model(X, w_h, b_h, g_h, bb_h, w_h2, b_h2, g_h2, bb_h2, w_o, b_ho, g_ho, bb_ho):\n",
    "    X = T.dot(X, w_h) + b_h\n",
    "    X = batch_normalization(X, gamma= g_h, beta= bb_h, \n",
    "                            mean= X.mean((0,), keepdims=True),\n",
    "                            std= T.ones_like(X.var((0,), keepdims = True)), \n",
    "                            mode='high_mem')    \n",
    "    h = rectify(X)\n",
    "\n",
    "    h  = T.dot(h, w_h2) + b_h2\n",
    "    h = batch_normalization(h, gamma= g_h2, beta= bb_h2, \n",
    "                            mean= h.mean((0,), keepdims=True),\n",
    "                            std= T.ones_like(h.var((0,), keepdims = True)), \n",
    "                            mode='high_mem')       \n",
    "    h2 = rectify(h)\n",
    "\n",
    "    h2 = T.dot(h2, w_o) + b_ho\n",
    "    h2 = batch_normalization(h2, gamma= g_ho, beta= bb_ho, \n",
    "                            mean= h2.mean((0,), keepdims=True),\n",
    "                            std= T.ones_like(h2.var((0,), keepdims = True)), \n",
    "                            mode='high_mem')   \n",
    "    py_x = softmax(h2)\n",
    "    return h, h2, py_x\n",
    "\n",
    "\n",
    "X = T.fmatrix()\n",
    "Y = T.fmatrix()\n",
    "\n",
    "h1_size = 600\n",
    "h2_size = 550\n",
    "\n",
    "w_h = init_weights((len(vectorizer.vocabulary_), h1_size))\n",
    "b_h = theano.shared(floatX(np.zeros(h1_size)))\n",
    "g_h = theano.shared(floatX(np.ones(h1_size)))\n",
    "bb_h = theano.shared(floatX(np.zeros(h1_size)))\n",
    "\n",
    "w_h2 = init_weights((h1_size, h2_size))\n",
    "b_h2 = theano.shared(floatX(np.zeros(h2_size)))\n",
    "g_h2 = theano.shared(floatX(np.ones(h2_size)))\n",
    "bb_h2 = theano.shared(floatX(np.zeros(h2_size)))\n",
    "\n",
    "w_o = init_weights((h2_size, yTest.shape[1]))\n",
    "b_ho = theano.shared(floatX(np.zeros(yTest.shape[1])))\n",
    "g_ho = theano.shared(floatX(np.ones(yTest.shape[1])))\n",
    "bb_ho = theano.shared(floatX(np.zeros(yTest.shape[1])))\n",
    "\n",
    "noise_h, noise_h2, noise_py_x = model(X, w_h, b_h, g_h, bb_h, \n",
    "                                      w_h2, b_h2, g_h2, bb_h2, \n",
    "                                      w_o, b_ho, g_ho, bb_ho)\n",
    "\n",
    "h, h2, py_x = model(X, w_h, b_h, g_h, bb_h, \n",
    "                    w_h2, b_h2, g_h2, bb_h2, \n",
    "                    w_o, b_ho, g_ho, bb_ho)\n",
    "\n",
    "y_x = T.argmax(py_x, axis=1)\n",
    "\n",
    "\n",
    "cost = T.mean(T.nnet.categorical_crossentropy(noise_py_x, Y))\n",
    "params = [w_h, b_h, g_h, bb_h, w_h2, b_h2, g_h2, bb_h2, w_o, b_ho, g_ho, bb_ho]\n",
    "updates = RMSprop(cost, params, lr=0.0001)\n",
    "\n",
    "train = theano.function(inputs=[X, Y], outputs=cost, updates=updates, allow_input_downcast=True)\n",
    "predict = theano.function(inputs=[X], outputs=y_x, allow_input_downcast=True)\n",
    "\n",
    "batch_size = 60\n",
    "\n",
    "for i in range(34):\n",
    "    for start, end in zip(range(0, len(xTrain), batch_size), range(batch_size, len(xTrain), batch_size)):\n",
    "        index = np.random.choice(xTrain.shape[0], batch_size, replace=False)\n",
    "        cost = train(xTrain[index], yTrain[index])\n",
    "\n",
    "    tr = np.mean(np.argmax(yTest, axis=1) == predict(xTest))\n",
    "    trr =  np.mean(np.argmax(yTrain, axis=1) == predict(xTrain))\n",
    "    print 'Round:', i,\" Test:\", tr, ' Train:', trr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ELU, no effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round: 0     Test: 0.558333333333 Train: 0.597619047619\n",
      "Round: 1     Test: 0.599479166667 Train: 0.666836734694\n",
      "Round: 2     Test: 0.640625       Train: 0.711734693878\n",
      "Round: 3     Test: 0.6734375      Train: 0.740646258503\n",
      "Round: 4     Test: 0.688020833333 Train: 0.76343537415\n",
      "Round: 5     Test: 0.693229166667 Train: 0.786394557823\n",
      "Round: 6     Test: 0.704166666667 Train: 0.801700680272\n",
      "Round: 7     Test: 0.710416666667 Train: 0.820408163265\n",
      "Round: 8     Test: 0.717708333333 Train: 0.825680272109\n",
      "Round: 9     Test: 0.722916666667 Train: 0.837244897959\n",
      "Round: 10    Test: 0.711458333333 Train: 0.851530612245\n",
      "Round: 11    Test: 0.711979166667 Train: 0.866326530612\n",
      "Round: 12    Test: 0.722916666667 Train: 0.868367346939\n",
      "Round: 13    Test: 0.7265625      Train: 0.876870748299\n",
      "Round: 14    Test: 0.725          Train: 0.879591836735\n",
      "Round: 15    Test: 0.719791666667 Train: 0.892176870748\n",
      "Round: 16    Test: 0.727604166667 Train: 0.897789115646\n",
      "Round: 17    Test: 0.728125       Train: 0.899489795918\n",
      "Round: 18    Test: 0.73125        Train: 0.903401360544\n",
      "Round: 19    Test: 0.730729166667 Train: 0.910714285714\n",
      "Round: 20    Test: 0.725          Train: 0.909013605442\n",
      "Round: 21    Test: 0.721875       Train: 0.915476190476\n",
      "Round: 22    Test: 0.733854166667 Train: 0.914115646259\n",
      "Round: 23    Test: 0.7234375      Train: 0.919727891156\n",
      "Round: 24    Test: 0.71875        Train: 0.92619047619\n",
      "Round: 25    Test: 0.723958333333 Train: 0.924659863946\n",
      "Round: 26    Test: 0.721875       Train: 0.922619047619\n",
      "Round: 27    Test: 0.7296875      Train: 0.926530612245\n",
      "Round: 28    Test: 0.725          Train: 0.930612244898\n",
      "Round: 29    Test: 0.723958333333 Train: 0.930952380952\n",
      "Round: 30    Test: 0.719270833333 Train: 0.931802721088\n",
      "Round: 31    Test: 0.71875        Train: 0.933503401361\n",
      "Round: 32    Test: 0.7171875      Train: 0.937414965986\n",
      "Round: 33    Test: 0.711979166667 Train: 0.93843537415\n",
      "Round: 34    Test: 0.726041666667 Train: 0.938945578231\n",
      "Round: 35    Test: 0.719270833333 Train: 0.938265306122\n",
      "Round: 36    Test: 0.719791666667 Train: 0.939965986395\n",
      "Round: 37    Test: 0.720833333333 Train: 0.942857142857\n",
      "Round: 38    Test: 0.717708333333 Train: 0.939285714286\n",
      "Round: 39    Test: 0.7234375      Train: 0.940306122449\n",
      "Round: 40    Test: 0.70625        Train: 0.944557823129\n",
      "\n",
      "Wall time: 1min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\"\"\" glorot 4-layer: batch, batch, batch \n",
    "    random indexing without replacement, ELU                \n",
    "    epsilon=1e-9  \"\"\"\n",
    "\n",
    "# batch normalization code adapted from \n",
    "# https://groups.google.com/forum/#!topic/theano-users/dMV6aabL1Ds \n",
    "\n",
    "\n",
    "import theano\n",
    "from theano import tensor as T\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "from theano.tensor.nnet.bn import batch_normalization\n",
    "import numpy as np\n",
    "\n",
    "srng = RandomStreams()\n",
    "\n",
    "def shuffle(x, y):\n",
    "    # helper function to shuffle indicies each loop \n",
    "    index = np.random.choice(len(x), len(x), replace=False)\n",
    "    return x[index], y[index]\n",
    "\n",
    "def floatX(X):\n",
    "    return np.asarray(X, dtype=theano.config.floatX)\n",
    "\n",
    "def init_weights(shape):\n",
    "    (h, w) = shape\n",
    "    # Glorot normalization - last factor depends on non-linearity\n",
    "    # 0.25 for sigmoid and 0.1 for softmax, 1.0 for tanh or Relu\n",
    "    normalizer = 2.0 * np.sqrt(6) / np.sqrt(h + w) * 1.0\n",
    "    return theano.shared(floatX((np.random.random_sample(shape) - 0.5) * normalizer))\n",
    "\n",
    "def rectify(X, alpha=0.01):\n",
    "#     return T.maximum(X, 0.)\n",
    "#    return T.maximum(X, 0.1*X)  #leaky rectifier\n",
    "     return T.switch(X > 0, X, alpha * (T.exp(X) - 1)) # ELU\n",
    "\n",
    "def softmax(X):\n",
    "    e_x = T.exp(X - X.max(axis=1).dimshuffle(0, 'x'))\n",
    "    return e_x / e_x.sum(axis=1).dimshuffle(0, 'x')\n",
    "\n",
    "def RMSprop(cost, params, lr=0.001, rho=0.99, epsilon=1e-9):\n",
    "    grads = T.grad(cost=cost, wrt=params)\n",
    "    updates = []\n",
    "    for p, g in zip(params, grads):\n",
    "        acc = theano.shared(p.get_value() * 0.)\n",
    "        acc_new = rho * acc + (1 - rho) * g ** 2\n",
    "        gradient_scaling = T.sqrt(acc_new + epsilon)\n",
    "        g = g / gradient_scaling\n",
    "        updates.append((acc, acc_new))\n",
    "        updates.append((p, p - lr * g))\n",
    "    return updates\n",
    "\n",
    "\n",
    "def model(X, w_h, b_h, g_h, bb_h, w_h2, b_h2, g_h2, bb_h2, w_o, b_ho, g_ho, bb_ho):\n",
    "    X = T.dot(X, w_h) + b_h\n",
    "    X = batch_normalization(X, gamma= g_h, beta= bb_h, \n",
    "                            mean= X.mean((0,), keepdims=True),\n",
    "                            std= T.ones_like(X.var((0,), keepdims = True)), \n",
    "                            mode='high_mem')    \n",
    "    h = rectify(X)\n",
    "\n",
    "    h  = T.dot(h, w_h2) + b_h2\n",
    "    h = batch_normalization(h, gamma= g_h2, beta= bb_h2, \n",
    "                            mean= h.mean((0,), keepdims=True),\n",
    "                            std= T.ones_like(h.var((0,), keepdims = True)), \n",
    "                            mode='high_mem')       \n",
    "    h2 = rectify(h)\n",
    "\n",
    "    h2 = T.dot(h2, w_o) + b_ho\n",
    "    h2 = batch_normalization(h2, gamma= g_ho, beta= bb_ho, \n",
    "                            mean= h2.mean((0,), keepdims=True),\n",
    "                            std= T.ones_like(h2.var((0,), keepdims = True)), \n",
    "                            mode='high_mem')   \n",
    "    py_x = softmax(h2)\n",
    "    return h, h2, py_x\n",
    "\n",
    "\n",
    "X = T.fmatrix()\n",
    "Y = T.fmatrix()\n",
    "\n",
    "batch_size = 60\n",
    "\n",
    "h1_size = 600\n",
    "h2_size = 550\n",
    "\n",
    "w_h = init_weights((len(vectorizer.vocabulary_), h1_size))\n",
    "b_h = theano.shared(floatX(np.zeros(h1_size)))\n",
    "g_h = theano.shared(floatX(np.ones((h1_size))))\n",
    "bb_h = theano.shared(floatX(np.zeros((h1_size))))\n",
    "\n",
    "w_h2 = init_weights((h1_size, h2_size))\n",
    "b_h2 = theano.shared(floatX(np.zeros(h2_size)))\n",
    "g_h2 = theano.shared(floatX(np.ones((h2_size))))\n",
    "bb_h2 = theano.shared(floatX(np.zeros((h2_size))))\n",
    "\n",
    "w_o = init_weights((h2_size, yTest.shape[1]))\n",
    "b_ho = theano.shared(floatX(np.zeros(yTest.shape[1])))\n",
    "g_ho = theano.shared(floatX(np.ones((yTest.shape[1]))))\n",
    "bb_ho = theano.shared(floatX(np.zeros((yTest.shape[1]))))\n",
    "\n",
    "noise_h, noise_h2, noise_py_x = model(X, w_h, b_h, g_h, bb_h, \n",
    "                                      w_h2, b_h2, g_h2, bb_h2, \n",
    "                                      w_o, b_ho, g_ho, bb_ho)\n",
    "\n",
    "h, h2, py_x = model(X, w_h, b_h, g_h, bb_h, \n",
    "                    w_h2, b_h2, g_h2, bb_h2, \n",
    "                    w_o, b_ho, g_ho, bb_ho)\n",
    "\n",
    "y_x = T.argmax(py_x, axis=1)\n",
    "\n",
    "\n",
    "cost = T.mean(T.nnet.categorical_crossentropy(noise_py_x, Y))\n",
    "params = [w_h, b_h, g_h, bb_h, w_h2, b_h2, g_h2, bb_h2, w_o, b_ho, g_ho, bb_ho]\n",
    "updates = RMSprop(cost, params, lr=0.0001)\n",
    "\n",
    "train = theano.function(inputs=[X, Y], outputs=cost, updates=updates, allow_input_downcast=True)\n",
    "predict = theano.function(inputs=[X], outputs=y_x, allow_input_downcast=True)\n",
    "\n",
    "\n",
    "for i in range(41):\n",
    "\n",
    "    for start, end in zip(range(0, len(xTrain), batch_size), range(batch_size, len(xTrain), batch_size)):\n",
    "        cost = train(xTrain[start:end], yTrain[start:end])\n",
    "        \n",
    "    xTrain, yTrain = shuffle(xTrain, yTrain)\n",
    "    xTest, yTest   = shuffle(xTest, yTest)\n",
    "\n",
    "    trr, tr = [], []\n",
    "    for start, end in zip(range(0, len(xTrain), batch_size), range(batch_size, len(xTrain), batch_size)):        \n",
    "        trr += [np.argmax(yTrain[start:end], axis=1) == predict(xTrain[start:end])]\n",
    "\n",
    "    for start, end in zip(range(0, len(xTest), batch_size), range(batch_size, len(xTest), batch_size)):\n",
    "        tr += [np.argmax(yTest[start:end], axis=1) == predict(xTest[start:end])]\n",
    "\n",
    "    print \"Round: %-5s Test: %-14s Train: %-8s\" % (i, np.mean(tr), np.mean(trr))\n",
    "    \n",
    "print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "70% at round 6!\n",
    "\n",
    "\n",
    "73% at round 18. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round: 0     Test: 0.540625       Train: 0.60119047619\n",
      "Round: 1     Test: 0.601041666667 Train: 0.66768707483\n",
      "Round: 2     Test: 0.647916666667 Train: 0.712414965986\n",
      "Round: 3     Test: 0.665104166667 Train: 0.743027210884\n",
      "Round: 4     Test: 0.680208333333 Train: 0.770918367347\n",
      "Round: 5     Test: 0.696875       Train: 0.787585034014\n",
      "Round: 6     Test: 0.701041666667 Train: 0.803401360544\n",
      "Round: 7     Test: 0.706770833333 Train: 0.821768707483\n",
      "Round: 8     Test: 0.707291666667 Train: 0.833163265306\n",
      "Round: 9     Test: 0.7109375      Train: 0.840646258503\n",
      "Round: 10    Test: 0.718229166667 Train: 0.853231292517\n",
      "Round: 11    Test: 0.719791666667 Train: 0.859013605442\n",
      "Round: 12    Test: 0.727604166667 Train: 0.87380952381\n",
      "Round: 13    Test: 0.719270833333 Train: 0.875170068027\n",
      "Round: 14    Test: 0.7265625      Train: 0.883163265306\n",
      "Round: 15    Test: 0.721875       Train: 0.887755102041\n",
      "Round: 16    Test: 0.7078125      Train: 0.894727891156\n",
      "Round: 17    Test: 0.7234375      Train: 0.898469387755\n",
      "Round: 18    Test: 0.721354166667 Train: 0.901870748299\n",
      "Round: 19    Test: 0.725          Train: 0.911054421769\n",
      "Round: 20    Test: 0.720833333333 Train: 0.910714285714\n",
      "Round: 21    Test: 0.719791666667 Train: 0.916326530612\n",
      "Round: 22    Test: 0.7046875      Train: 0.912585034014\n",
      "Round: 23    Test: 0.716666666667 Train: 0.919047619048\n",
      "Round: 24    Test: 0.708854166667 Train: 0.926020408163\n",
      "Round: 25    Test: 0.730729166667 Train: 0.926360544218\n",
      "Round: 26    Test: 0.723958333333 Train: 0.928401360544\n",
      "Round: 27    Test: 0.714583333333 Train: 0.927891156463\n",
      "Round: 28    Test: 0.715104166667 Train: 0.933333333333\n",
      "Round: 29    Test: 0.725520833333 Train: 0.933843537415\n",
      "Round: 30    Test: 0.722916666667 Train: 0.935544217687\n",
      "Round: 31    Test: 0.706770833333 Train: 0.936054421769\n",
      "Round: 32    Test: 0.7171875      Train: 0.938605442177\n",
      "Round: 33    Test: 0.722395833333 Train: 0.938265306122\n",
      "Round: 34    Test: 0.7125         Train: 0.939455782313\n",
      "Round: 35    Test: 0.7171875      Train: 0.938775510204\n",
      "Round: 36    Test: 0.713541666667 Train: 0.940816326531\n",
      "Round: 37    Test: 0.713541666667 Train: 0.939965986395\n",
      "Round: 38    Test: 0.70625        Train: 0.940646258503\n",
      "Round: 39    Test: 0.708854166667 Train: 0.941836734694\n",
      "Round: 40    Test: 0.713541666667 Train: 0.941496598639\n",
      "\n",
      "Wall time: 1min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\"\"\" glorot 4-layer: batch, batch, batch \n",
    "    random indexing without replacement, RELU                \n",
    "    epsilon=1e-9  \"\"\"\n",
    "\n",
    "# batch normalization code adapted from \n",
    "# https://groups.google.com/forum/#!topic/theano-users/dMV6aabL1Ds \n",
    "\n",
    "\n",
    "import theano\n",
    "from theano import tensor as T\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "from theano.tensor.nnet.bn import batch_normalization\n",
    "import numpy as np\n",
    "\n",
    "srng = RandomStreams()\n",
    "\n",
    "def shuffle(x, y):\n",
    "    # helper function to shuffle indicies each loop \n",
    "    index = np.random.choice(len(x), len(x), replace=False)\n",
    "    return x[index], y[index]\n",
    "\n",
    "def floatX(X):\n",
    "    return np.asarray(X, dtype=theano.config.floatX)\n",
    "\n",
    "def init_weights(shape):\n",
    "    (h, w) = shape\n",
    "    # Glorot normalization - last factor depends on non-linearity\n",
    "    # 0.25 for sigmoid and 0.1 for softmax, 1.0 for tanh or Relu\n",
    "    normalizer = 2.0 * np.sqrt(6) / np.sqrt(h + w) * 1.0\n",
    "    return theano.shared(floatX((np.random.random_sample(shape) - 0.5) * normalizer))\n",
    "\n",
    "def rectify(X, alpha=0.01):\n",
    "#     return T.maximum(X, 0.)\n",
    "#    return T.maximum(X, 0.1*X)  #leaky rectifier\n",
    "     return T.switch(X > 0, X, alpha * (T.exp(X) - 1)) # ELU\n",
    "\n",
    "def softmax(X):\n",
    "    e_x = T.exp(X - X.max(axis=1).dimshuffle(0, 'x'))\n",
    "    return e_x / e_x.sum(axis=1).dimshuffle(0, 'x')\n",
    "\n",
    "def RMSprop(cost, params, lr=0.001, rho=0.99, epsilon=1e-9):\n",
    "    grads = T.grad(cost=cost, wrt=params)\n",
    "    updates = []\n",
    "    for p, g in zip(params, grads):\n",
    "        acc = theano.shared(p.get_value() * 0.)\n",
    "        acc_new = rho * acc + (1 - rho) * g ** 2\n",
    "        gradient_scaling = T.sqrt(acc_new + epsilon)\n",
    "        g = g / gradient_scaling\n",
    "        updates.append((acc, acc_new))\n",
    "        updates.append((p, p - lr * g))\n",
    "    return updates\n",
    "\n",
    "\n",
    "def model(X, w_h, b_h, g_h, bb_h, w_h2, b_h2, g_h2, bb_h2, w_o, b_ho, g_ho, bb_ho):\n",
    "    X = T.dot(X, w_h) + b_h\n",
    "    X = batch_normalization(X, gamma= g_h, beta= bb_h, \n",
    "                            mean= X.mean((0,), keepdims=True),\n",
    "                            std= T.ones_like(X.var((0,), keepdims = True)), \n",
    "                            mode='high_mem')    \n",
    "    h = rectify(X)\n",
    "\n",
    "    h  = T.dot(h, w_h2) + b_h2\n",
    "    h = batch_normalization(h, gamma= g_h2, beta= bb_h2, \n",
    "                            mean= h.mean((0,), keepdims=True),\n",
    "                            std= T.ones_like(h.var((0,), keepdims = True)), \n",
    "                            mode='high_mem')       \n",
    "    h2 = rectify(h)\n",
    "\n",
    "    h2 = T.dot(h2, w_o) + b_ho\n",
    "    h2 = batch_normalization(h2, gamma= g_ho, beta= bb_ho, \n",
    "                            mean= h2.mean((0,), keepdims=True),\n",
    "                            std= T.ones_like(h2.var((0,), keepdims = True)), \n",
    "                            mode='high_mem')   \n",
    "    py_x = softmax(h2)\n",
    "    return h, h2, py_x\n",
    "\n",
    "\n",
    "X = T.fmatrix()\n",
    "Y = T.fmatrix()\n",
    "\n",
    "batch_size = 60\n",
    "\n",
    "h1_size = 600\n",
    "h2_size = 550\n",
    "\n",
    "w_h = init_weights((len(vectorizer.vocabulary_), h1_size))\n",
    "b_h = theano.shared(floatX(np.zeros(h1_size)))\n",
    "g_h = theano.shared(floatX(np.ones((h1_size))))\n",
    "bb_h = theano.shared(floatX(np.zeros((h1_size))))\n",
    "\n",
    "w_h2 = init_weights((h1_size, h2_size))\n",
    "b_h2 = theano.shared(floatX(np.zeros(h2_size)))\n",
    "g_h2 = theano.shared(floatX(np.ones((h2_size))))\n",
    "bb_h2 = theano.shared(floatX(np.zeros((h2_size))))\n",
    "\n",
    "w_o = init_weights((h2_size, yTest.shape[1]))\n",
    "b_ho = theano.shared(floatX(np.zeros(yTest.shape[1])))\n",
    "g_ho = theano.shared(floatX(np.ones((yTest.shape[1]))))\n",
    "bb_ho = theano.shared(floatX(np.zeros((yTest.shape[1]))))\n",
    "\n",
    "noise_h, noise_h2, noise_py_x = model(X, w_h, b_h, g_h, bb_h, \n",
    "                                      w_h2, b_h2, g_h2, bb_h2, \n",
    "                                      w_o, b_ho, g_ho, bb_ho)\n",
    "\n",
    "h, h2, py_x = model(X, w_h, b_h, g_h, bb_h, \n",
    "                    w_h2, b_h2, g_h2, bb_h2, \n",
    "                    w_o, b_ho, g_ho, bb_ho)\n",
    "\n",
    "y_x = T.argmax(py_x, axis=1)\n",
    "\n",
    "\n",
    "cost = T.mean(T.nnet.categorical_crossentropy(noise_py_x, Y))\n",
    "params = [w_h, b_h, g_h, bb_h, w_h2, b_h2, g_h2, bb_h2, w_o, b_ho, g_ho, bb_ho]\n",
    "updates = RMSprop(cost, params, lr=0.0001)\n",
    "\n",
    "train = theano.function(inputs=[X, Y], outputs=cost, updates=updates, allow_input_downcast=True)\n",
    "predict = theano.function(inputs=[X], outputs=y_x, allow_input_downcast=True)\n",
    "\n",
    "\n",
    "for i in range(41):\n",
    "\n",
    "    for start, end in zip(range(0, len(xTrain), batch_size), range(batch_size, len(xTrain), batch_size)):\n",
    "        cost = train(xTrain[start:end], yTrain[start:end])\n",
    "        \n",
    "    xTrain, yTrain = shuffle(xTrain, yTrain)\n",
    "    xTest, yTest   = shuffle(xTest, yTest)\n",
    "\n",
    "    trr, tr = [], []\n",
    "    for start, end in zip(range(0, len(xTrain), batch_size), range(batch_size, len(xTrain), batch_size)):        \n",
    "        trr += [np.argmax(yTrain[start:end], axis=1) == predict(xTrain[start:end])]\n",
    "\n",
    "    for start, end in zip(range(0, len(xTest), batch_size), range(batch_size, len(xTest), batch_size)):\n",
    "        tr += [np.argmax(yTest[start:end], axis=1) == predict(xTest[start:end])]\n",
    "\n",
    "    print \"Round: %-5s Test: %-14s Train: %-8s\" % (i, np.mean(tr), np.mean(trr))\n",
    "    \n",
    "print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Will adding back dropout get us back to 74%?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round: 0     Test: 0.515625       Train: 0.56462585034\n",
      "Round: 1     Test: 0.569791666667 Train: 0.614965986395\n",
      "Round: 2     Test: 0.609375       Train: 0.666156462585\n",
      "Round: 3     Test: 0.628125       Train: 0.702380952381\n",
      "Round: 4     Test: 0.646875       Train: 0.725170068027\n",
      "Round: 5     Test: 0.669270833333 Train: 0.74574829932\n",
      "Round: 6     Test: 0.678645833333 Train: 0.764115646259\n",
      "Round: 7     Test: 0.686458333333 Train: 0.775680272109\n",
      "Round: 8     Test: 0.693229166667 Train: 0.791836734694\n",
      "Round: 9     Test: 0.7015625      Train: 0.803401360544\n",
      "Round: 10    Test: 0.714583333333 Train: 0.809013605442\n",
      "Round: 11    Test: 0.7125         Train: 0.822619047619\n",
      "Round: 12    Test: 0.715104166667 Train: 0.829421768707\n",
      "Round: 13    Test: 0.716145833333 Train: 0.83537414966\n",
      "Round: 14    Test: 0.721354166667 Train: 0.838775510204\n",
      "Round: 15    Test: 0.713020833333 Train: 0.852551020408\n",
      "Round: 16    Test: 0.71875        Train: 0.853231292517\n",
      "Round: 17    Test: 0.721354166667 Train: 0.860204081633\n",
      "Round: 18    Test: 0.721354166667 Train: 0.865986394558\n",
      "Round: 19    Test: 0.725520833333 Train: 0.867176870748\n",
      "Round: 20    Test: 0.736979166667 Train: 0.873299319728\n",
      "Round: 21    Test: 0.734375       Train: 0.875170068027\n",
      "Round: 22    Test: 0.725520833333 Train: 0.880272108844\n",
      "Round: 23    Test: 0.721875       Train: 0.883163265306\n",
      "Round: 24    Test: 0.717708333333 Train: 0.886224489796\n",
      "Round: 25    Test: 0.733333333333 Train: 0.893027210884\n",
      "Round: 26    Test: 0.73125        Train: 0.89693877551\n",
      "Round: 27    Test: 0.727604166667 Train: 0.894727891156\n",
      "Round: 28    Test: 0.738020833333 Train: 0.901020408163\n",
      "Round: 29    Test: 0.730208333333 Train: 0.90425170068\n",
      "Round: 30    Test: 0.738541666667 Train: 0.901530612245\n",
      "Round: 31    Test: 0.7328125      Train: 0.90612244898\n",
      "Round: 32    Test: 0.729166666667 Train: 0.906972789116\n",
      "Round: 33    Test: 0.730208333333 Train: 0.908503401361\n",
      "Round: 34    Test: 0.725520833333 Train: 0.917346938776\n",
      "Round: 35    Test: 0.7359375      Train: 0.916496598639\n",
      "Round: 36    Test: 0.736979166667 Train: 0.915136054422\n",
      "Round: 37    Test: 0.7265625      Train: 0.917857142857\n",
      "Round: 38    Test: 0.7328125      Train: 0.919727891156\n",
      "Round: 39    Test: 0.727604166667 Train: 0.919047619048\n",
      "Round: 40    Test: 0.727604166667 Train: 0.921768707483\n",
      "Round: 41    Test: 0.729166666667 Train: 0.92074829932\n",
      "Round: 42    Test: 0.732291666667 Train: 0.921428571429\n",
      "Round: 43    Test: 0.732291666667 Train: 0.924319727891\n",
      "Round: 44    Test: 0.7375         Train: 0.92619047619\n",
      "Round: 45    Test: 0.739583333333 Train: 0.926020408163\n",
      "Round: 46    Test: 0.734895833333 Train: 0.926700680272\n",
      "Round: 47    Test: 0.731770833333 Train: 0.925680272109\n",
      "Round: 48    Test: 0.7296875      Train: 0.93112244898\n",
      "Round: 49    Test: 0.727604166667 Train: 0.931972789116\n",
      "Round: 50    Test: 0.7328125      Train: 0.932142857143\n",
      "\n",
      "Wall time: 1min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\"\"\" glorot 4-layer: batch, drop, batch, drop, batch drop \n",
    "    random indexing without replacement, ELU, epsilon=1e-9  \"\"\"\n",
    "\n",
    "# batch normalization code adapted from \n",
    "# https://groups.google.com/forum/#!topic/theano-users/dMV6aabL1Ds \n",
    "\n",
    "\n",
    "import theano\n",
    "from theano import tensor as T\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "from theano.tensor.nnet.bn import batch_normalization\n",
    "import numpy as np\n",
    "\n",
    "srng = RandomStreams()\n",
    "\n",
    "def shuffle(x, y):\n",
    "    # helper function to shuffle indicies each loop \n",
    "    index = np.random.choice(len(x), len(x), replace=False)\n",
    "    return x[index], y[index]\n",
    "\n",
    "def floatX(X):\n",
    "    return np.asarray(X, dtype=theano.config.floatX)\n",
    "\n",
    "def init_weights(shape):\n",
    "    (h, w) = shape\n",
    "    # Glorot normalization - last factor depends on non-linearity\n",
    "    # 0.25 for sigmoid and 0.1 for softmax, 1.0 for tanh or Relu\n",
    "    normalizer = 2.0 * np.sqrt(6) / np.sqrt(h + w) * 1.0\n",
    "    return theano.shared(floatX((np.random.random_sample(shape) - 0.5) * normalizer))\n",
    "\n",
    "def rectify(X, alpha=0.01):\n",
    "#     return T.maximum(X, 0.)\n",
    "#    return T.maximum(X, 0.1*X)  #leaky rectifier\n",
    "     return T.switch(X > 0, X, alpha * (T.exp(X) - 1)) # ELU\n",
    "\n",
    "def softmax(X):\n",
    "    e_x = T.exp(X - X.max(axis=1).dimshuffle(0, 'x'))\n",
    "    return e_x / e_x.sum(axis=1).dimshuffle(0, 'x')\n",
    "\n",
    "def RMSprop(cost, params, lr=0.001, rho=0.99, epsilon=1e-9):\n",
    "    grads = T.grad(cost=cost, wrt=params)\n",
    "    updates = []\n",
    "    for p, g in zip(params, grads):\n",
    "        acc = theano.shared(p.get_value() * 0.)\n",
    "        acc_new = rho * acc + (1 - rho) * g ** 2\n",
    "        gradient_scaling = T.sqrt(acc_new + epsilon)\n",
    "        g = g / gradient_scaling\n",
    "        updates.append((acc, acc_new))\n",
    "        updates.append((p, p - lr * g))\n",
    "    return updates\n",
    "\n",
    "def dropout(X, p=0.):\n",
    "    if p > 0:\n",
    "        retain_prob = 1 - p\n",
    "        X *= srng.binomial(X.shape, p=retain_prob, dtype=theano.config.floatX)\n",
    "        X /= retain_prob\n",
    "    return X\n",
    "\n",
    "\n",
    "def model(X, w_h, b_h, g_h, bb_h, w_h2, b_h2, g_h2, bb_h2, \n",
    "          w_o, b_ho, g_ho, bb_ho, p_drop_input, p_drop_hidden):\n",
    "    X = T.dot(X, w_h) + b_h\n",
    "    X = batch_normalization(X, gamma= g_h, beta= bb_h, \n",
    "                            mean= X.mean((0,), keepdims=True),\n",
    "                            std= T.ones_like(X.var((0,), keepdims = True)), \n",
    "                            mode='high_mem') \n",
    "    X = dropout(X, p_drop_hidden)\n",
    "    h = rectify(X)\n",
    "\n",
    "    h  = T.dot(h, w_h2) + b_h2\n",
    "    h = batch_normalization(h, gamma= g_h2, beta= bb_h2, \n",
    "                            mean= h.mean((0,), keepdims=True),\n",
    "                            std= T.ones_like(h.var((0,), keepdims = True)), \n",
    "                            mode='high_mem') \n",
    "    h = dropout(h, p_drop_hidden)\n",
    "    h2 = rectify(h)\n",
    "\n",
    "    h2 = T.dot(h2, w_o) + b_ho\n",
    "    h2 = batch_normalization(h2, gamma= g_ho, beta= bb_ho, \n",
    "                            mean= h2.mean((0,), keepdims=True),\n",
    "                            std= T.ones_like(h2.var((0,), keepdims = True)), \n",
    "                            mode='high_mem') \n",
    "    h2 = dropout(h2, p_drop_hidden)\n",
    "    py_x = softmax(h2)\n",
    "    return h, h2, py_x\n",
    "\n",
    "\n",
    "X = T.fmatrix()\n",
    "Y = T.fmatrix()\n",
    "\n",
    "batch_size = 60\n",
    "\n",
    "h1_size = 600\n",
    "h2_size = 550\n",
    "\n",
    "w_h = init_weights((len(vectorizer.vocabulary_), h1_size))\n",
    "b_h = theano.shared(floatX(np.zeros(h1_size)))\n",
    "g_h = theano.shared(floatX(np.ones((h1_size))))\n",
    "bb_h = theano.shared(floatX(np.zeros((h1_size))))\n",
    "\n",
    "w_h2 = init_weights((h1_size, h2_size))\n",
    "b_h2 = theano.shared(floatX(np.zeros(h2_size)))\n",
    "g_h2 = theano.shared(floatX(np.ones((h2_size))))\n",
    "bb_h2 = theano.shared(floatX(np.zeros((h2_size))))\n",
    "\n",
    "w_o = init_weights((h2_size, yTest.shape[1]))\n",
    "b_ho = theano.shared(floatX(np.zeros(yTest.shape[1])))\n",
    "g_ho = theano.shared(floatX(np.ones((yTest.shape[1]))))\n",
    "bb_ho = theano.shared(floatX(np.zeros((yTest.shape[1]))))\n",
    "\n",
    "noise_h, noise_h2, noise_py_x = model(X, w_h, b_h, g_h, bb_h, \n",
    "                                      w_h2, b_h2, g_h2, bb_h2, \n",
    "                                      w_o, b_ho, g_ho, bb_ho, .0, .2)\n",
    "\n",
    "h, h2, py_x = model(X, w_h, b_h, g_h, bb_h, \n",
    "                    w_h2, b_h2, g_h2, bb_h2, \n",
    "                    w_o, b_ho, g_ho, bb_ho, .0, .0)\n",
    "\n",
    "y_x = T.argmax(py_x, axis=1)\n",
    "\n",
    "\n",
    "cost = T.mean(T.nnet.categorical_crossentropy(noise_py_x, Y))\n",
    "params = [w_h, b_h, g_h, bb_h, w_h2, b_h2, g_h2, bb_h2, \n",
    "          w_o, b_ho, g_ho, bb_ho]\n",
    "updates = RMSprop(cost, params, lr=0.0001)\n",
    "\n",
    "train = theano.function(inputs=[X, Y], outputs=cost, updates=updates, allow_input_downcast=True)\n",
    "predict = theano.function(inputs=[X], outputs=y_x, allow_input_downcast=True)\n",
    "\n",
    "\n",
    "for i in range(51):\n",
    "\n",
    "    for start, end in zip(range(0, len(xTrain), batch_size), range(batch_size, len(xTrain), batch_size)):\n",
    "        cost = train(xTrain[start:end], yTrain[start:end])\n",
    "        \n",
    "    xTrain, yTrain = shuffle(xTrain, yTrain)\n",
    "    xTest, yTest   = shuffle(xTest, yTest)\n",
    "\n",
    "    trr, tr = [], []\n",
    "    for start, end in zip(range(0, len(xTrain), batch_size), range(batch_size, len(xTrain), batch_size)):        \n",
    "        trr += [np.argmax(yTrain[start:end], axis=1) == predict(xTrain[start:end])]\n",
    "\n",
    "    for start, end in zip(range(0, len(xTest), batch_size), range(batch_size, len(xTest), batch_size)):\n",
    "        tr += [np.argmax(yTest[start:end], axis=1) == predict(xTest[start:end])]\n",
    "\n",
    "    print \"Round: %-5s Test: %-14s Train: %-8s\" % (i, np.mean(tr), np.mean(trr))\n",
    "    \n",
    "print\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High of 73.9% with just a little dropout (0.0, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round: 0     Test: 0.533333333333 Train: 0.571088435374\n",
      "Round: 1     Test: 0.576041666667 Train: 0.620238095238\n",
      "Round: 2     Test: 0.610416666667 Train: 0.666496598639\n",
      "Round: 3     Test: 0.631770833333 Train: 0.695578231293\n",
      "Round: 4     Test: 0.663020833333 Train: 0.724319727891\n",
      "Round: 5     Test: 0.670833333333 Train: 0.741836734694\n",
      "Round: 6     Test: 0.684895833333 Train: 0.76156462585\n",
      "Round: 7     Test: 0.685416666667 Train: 0.779081632653\n",
      "Round: 8     Test: 0.6859375      Train: 0.791326530612\n",
      "Round: 9     Test: 0.697395833333 Train: 0.799659863946\n",
      "Round: 10    Test: 0.703125       Train: 0.809183673469\n",
      "Round: 11    Test: 0.7125         Train: 0.819387755102\n",
      "Round: 12    Test: 0.70625        Train: 0.82925170068\n",
      "Round: 13    Test: 0.7078125      Train: 0.834353741497\n",
      "Round: 14    Test: 0.721875       Train: 0.842857142857\n",
      "Round: 15    Test: 0.719791666667 Train: 0.852380952381\n",
      "Round: 16    Test: 0.7234375      Train: 0.85425170068\n",
      "Round: 17    Test: 0.7171875      Train: 0.857142857143\n",
      "Round: 18    Test: 0.720833333333 Train: 0.863605442177\n",
      "Round: 19    Test: 0.721875       Train: 0.873469387755\n",
      "Round: 20    Test: 0.727604166667 Train: 0.868027210884\n",
      "Round: 21    Test: 0.728645833333 Train: 0.879081632653\n",
      "Round: 22    Test: 0.723958333333 Train: 0.881632653061\n",
      "Round: 23    Test: 0.726041666667 Train: 0.886054421769\n",
      "Round: 24    Test: 0.7265625      Train: 0.888775510204\n",
      "Round: 25    Test: 0.730208333333 Train: 0.889965986395\n",
      "Round: 26    Test: 0.721354166667 Train: 0.895408163265\n",
      "Round: 27    Test: 0.729166666667 Train: 0.896088435374\n",
      "Round: 28    Test: 0.729166666667 Train: 0.899489795918\n",
      "Round: 29    Test: 0.720833333333 Train: 0.903401360544\n",
      "Round: 30    Test: 0.7234375      Train: 0.905102040816\n",
      "Round: 31    Test: 0.727604166667 Train: 0.90612244898\n",
      "Round: 32    Test: 0.736458333333 Train: 0.911734693878\n",
      "Round: 33    Test: 0.725          Train: 0.906972789116\n",
      "Round: 34    Test: 0.731770833333 Train: 0.912925170068\n",
      "Round: 35    Test: 0.715625       Train: 0.91462585034\n",
      "Round: 36    Test: 0.727604166667 Train: 0.914285714286\n",
      "Round: 37    Test: 0.729166666667 Train: 0.922278911565\n",
      "Round: 38    Test: 0.7234375      Train: 0.919387755102\n",
      "Round: 39    Test: 0.732291666667 Train: 0.923469387755\n",
      "Round: 40    Test: 0.732291666667 Train: 0.924489795918\n",
      "Round: 41    Test: 0.727083333333 Train: 0.920918367347\n",
      "Round: 42    Test: 0.722395833333 Train: 0.925850340136\n",
      "Round: 43    Test: 0.725          Train: 0.926870748299\n",
      "Round: 44    Test: 0.722916666667 Train: 0.928231292517\n",
      "Round: 45    Test: 0.720833333333 Train: 0.928911564626\n",
      "Round: 46    Test: 0.7234375      Train: 0.928571428571\n",
      "Round: 47    Test: 0.729166666667 Train: 0.930442176871\n",
      "Round: 48    Test: 0.727604166667 Train: 0.927891156463\n",
      "Round: 49    Test: 0.727083333333 Train: 0.929931972789\n",
      "Round: 50    Test: 0.727083333333 Train: 0.927380952381\n",
      "\n",
      "Wall time: 1min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\"\"\" glorot 4-layer: batch, drop, batch, drop, batch drop \n",
    "    random indexing without replacement, ELU, epsilon=1e-9  \"\"\"\n",
    "\n",
    "# batch normalization code adapted from \n",
    "# https://groups.google.com/forum/#!topic/theano-users/dMV6aabL1Ds \n",
    "\n",
    "\n",
    "import theano\n",
    "from theano import tensor as T\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "from theano.tensor.nnet.bn import batch_normalization\n",
    "import numpy as np\n",
    "\n",
    "srng = RandomStreams()\n",
    "\n",
    "def shuffle(x, y):\n",
    "    # helper function to shuffle indicies each loop \n",
    "    index = np.random.choice(len(x), len(x), replace=False)\n",
    "    return x[index], y[index]\n",
    "\n",
    "def floatX(X):\n",
    "    return np.asarray(X, dtype=theano.config.floatX)\n",
    "\n",
    "def init_weights(shape):\n",
    "    (h, w) = shape\n",
    "    # Glorot normalization - last factor depends on non-linearity\n",
    "    # 0.25 for sigmoid and 0.1 for softmax, 1.0 for tanh or Relu\n",
    "    normalizer = 2.0 * np.sqrt(6) / np.sqrt(h + w) * 1.0\n",
    "    return theano.shared(floatX((np.random.random_sample(shape) - 0.5) * normalizer))\n",
    "\n",
    "def rectify(X, alpha=0.01):\n",
    "#     return T.maximum(X, 0.)\n",
    "#    return T.maximum(X, 0.1*X)  #leaky rectifier\n",
    "     return T.switch(X > 0, X, alpha * (T.exp(X) - 1)) # ELU\n",
    "\n",
    "def softmax(X):\n",
    "    e_x = T.exp(X - X.max(axis=1).dimshuffle(0, 'x'))\n",
    "    return e_x / e_x.sum(axis=1).dimshuffle(0, 'x')\n",
    "\n",
    "def RMSprop(cost, params, lr=0.001, rho=0.99, epsilon=1e-9):\n",
    "    grads = T.grad(cost=cost, wrt=params)\n",
    "    updates = []\n",
    "    for p, g in zip(params, grads):\n",
    "        acc = theano.shared(p.get_value() * 0.)\n",
    "        acc_new = rho * acc + (1 - rho) * g ** 2\n",
    "        gradient_scaling = T.sqrt(acc_new + epsilon)\n",
    "        g = g / gradient_scaling\n",
    "        updates.append((acc, acc_new))\n",
    "        updates.append((p, p - lr * g))\n",
    "    return updates\n",
    "\n",
    "def dropout(X, p=0.):\n",
    "    if p > 0:\n",
    "        retain_prob = 1 - p\n",
    "        X *= srng.binomial(X.shape, p=retain_prob, dtype=theano.config.floatX)\n",
    "        X /= retain_prob\n",
    "    return X\n",
    "\n",
    "\n",
    "def model(X, w_h, b_h, g_h, bb_h, w_h2, b_h2, g_h2, bb_h2, \n",
    "          w_o, b_ho, g_ho, bb_ho, p_drop_input, p_drop_hidden):\n",
    "    X = T.dot(X, w_h) + b_h\n",
    "    X = batch_normalization(X, gamma= g_h, beta= bb_h, \n",
    "                            mean= X.mean((0,), keepdims=True),\n",
    "                            std= T.ones_like(X.var((0,), keepdims = True)), \n",
    "                            mode='high_mem') \n",
    "    X = dropout(X, p_drop_hidden)\n",
    "    h = rectify(X)\n",
    "\n",
    "    h  = T.dot(h, w_h2) + b_h2\n",
    "    h = batch_normalization(h, gamma= g_h2, beta= bb_h2, \n",
    "                            mean= h.mean((0,), keepdims=True),\n",
    "                            std= T.ones_like(h.var((0,), keepdims = True)), \n",
    "                            mode='high_mem') \n",
    "    h = dropout(h, p_drop_hidden)\n",
    "    h2 = rectify(h)\n",
    "\n",
    "    h2 = T.dot(h2, w_o) + b_ho\n",
    "    h2 = batch_normalization(h2, gamma= g_ho, beta= bb_ho, \n",
    "                            mean= h2.mean((0,), keepdims=True),\n",
    "                            std= T.ones_like(h2.var((0,), keepdims = True)), \n",
    "                            mode='high_mem') \n",
    "    h2 = dropout(h2, p_drop_hidden)\n",
    "    py_x = softmax(h2)\n",
    "    return h, h2, py_x\n",
    "\n",
    "\n",
    "X = T.fmatrix()\n",
    "Y = T.fmatrix()\n",
    "\n",
    "batch_size = 60\n",
    "\n",
    "h1_size = 600\n",
    "h2_size = 550\n",
    "\n",
    "w_h = init_weights((len(vectorizer.vocabulary_), h1_size))\n",
    "b_h = theano.shared(floatX(np.zeros(h1_size)))\n",
    "g_h = theano.shared(floatX(np.ones((h1_size))))\n",
    "bb_h = theano.shared(floatX(np.zeros((h1_size))))\n",
    "\n",
    "w_h2 = init_weights((h1_size, h2_size))\n",
    "b_h2 = theano.shared(floatX(np.zeros(h2_size)))\n",
    "g_h2 = theano.shared(floatX(np.ones((h2_size))))\n",
    "bb_h2 = theano.shared(floatX(np.zeros((h2_size))))\n",
    "\n",
    "w_o = init_weights((h2_size, yTest.shape[1]))\n",
    "b_ho = theano.shared(floatX(np.zeros(yTest.shape[1])))\n",
    "g_ho = theano.shared(floatX(np.ones((yTest.shape[1]))))\n",
    "bb_ho = theano.shared(floatX(np.zeros((yTest.shape[1]))))\n",
    "\n",
    "noise_h, noise_h2, noise_py_x = model(X, w_h, b_h, g_h, bb_h, \n",
    "                                      w_h2, b_h2, g_h2, bb_h2, \n",
    "                                      w_o, b_ho, g_ho, bb_ho, .0, .2)\n",
    "\n",
    "h, h2, py_x = model(X, w_h, b_h, g_h, bb_h, \n",
    "                    w_h2, b_h2, g_h2, bb_h2, \n",
    "                    w_o, b_ho, g_ho, bb_ho, .0, .0)\n",
    "\n",
    "y_x = T.argmax(py_x, axis=1)\n",
    "\n",
    "\n",
    "cost = T.mean(T.nnet.categorical_crossentropy(noise_py_x, Y))\n",
    "params = [w_h, b_h, g_h, bb_h, w_h2, b_h2, g_h2, bb_h2, \n",
    "          w_o, b_ho, g_ho, bb_ho]\n",
    "updates = RMSprop(cost, params, lr=0.0001)\n",
    "\n",
    "train = theano.function(inputs=[X, Y], outputs=cost, updates=updates, allow_input_downcast=True)\n",
    "predict = theano.function(inputs=[X], outputs=y_x, allow_input_downcast=True)\n",
    "\n",
    "\n",
    "for i in range(51):\n",
    "\n",
    "    for start, end in zip(range(0, len(xTrain), batch_size), range(batch_size, len(xTrain), batch_size)):\n",
    "        cost = train(xTrain[start:end], yTrain[start:end])\n",
    "        \n",
    "    xTrain, yTrain = shuffle(xTrain, yTrain)\n",
    "    xTest, yTest   = shuffle(xTest, yTest)\n",
    "\n",
    "    trr, tr = [], []\n",
    "    for start, end in zip(range(0, len(xTrain), batch_size), range(batch_size, len(xTrain), batch_size)):        \n",
    "        trr += [np.argmax(yTrain[start:end], axis=1) == predict(xTrain[start:end])]\n",
    "\n",
    "    for start, end in zip(range(0, len(xTest), batch_size), range(batch_size, len(xTest), batch_size)):\n",
    "        tr += [np.argmax(yTest[start:end], axis=1) == predict(xTest[start:end])]\n",
    "\n",
    "    print \"Round: %-5s Test: %-14s Train: %-8s\" % (i, np.mean(tr), np.mean(trr))\n",
    "    \n",
    "print\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round: 0     Test: 0.5296875      Train: 0.56768707483\n",
      "Round: 1     Test: 0.5734375      Train: 0.618537414966\n",
      "Round: 2     Test: 0.6078125      Train: 0.659183673469\n",
      "Round: 3     Test: 0.628125       Train: 0.693367346939\n",
      "Round: 4     Test: 0.652604166667 Train: 0.726360544218\n",
      "Round: 5     Test: 0.665625       Train: 0.744727891156\n",
      "Round: 6     Test: 0.675520833333 Train: 0.763605442177\n",
      "Round: 7     Test: 0.677604166667 Train: 0.773639455782\n",
      "Round: 8     Test: 0.6890625      Train: 0.78843537415\n",
      "Round: 9     Test: 0.693229166667 Train: 0.794557823129\n",
      "Round: 10    Test: 0.715104166667 Train: 0.808333333333\n",
      "Round: 11    Test: 0.7109375      Train: 0.818367346939\n",
      "Round: 12    Test: 0.711458333333 Train: 0.827551020408\n",
      "Round: 13    Test: 0.713020833333 Train: 0.837414965986\n",
      "Round: 14    Test: 0.716145833333 Train: 0.839285714286\n",
      "Round: 15    Test: 0.7203125      Train: 0.848639455782\n",
      "Round: 16    Test: 0.720833333333 Train: 0.855272108844\n",
      "Round: 17    Test: 0.736979166667 Train: 0.855782312925\n",
      "Round: 18    Test: 0.728125       Train: 0.864795918367\n",
      "Round: 19    Test: 0.730208333333 Train: 0.871428571429\n",
      "Round: 20    Test: 0.7296875      Train: 0.874829931973\n",
      "Round: 21    Test: 0.740625       Train: 0.879421768707\n",
      "Round: 22    Test: 0.726041666667 Train: 0.877721088435\n",
      "Round: 23    Test: 0.7296875      Train: 0.882482993197\n",
      "Round: 24    Test: 0.719270833333 Train: 0.886734693878\n",
      "Round: 25    Test: 0.73125        Train: 0.889795918367\n",
      "Round: 26    Test: 0.721875       Train: 0.896088435374\n",
      "Round: 27    Test: 0.715104166667 Train: 0.897448979592\n",
      "Round: 28    Test: 0.7265625      Train: 0.903571428571\n",
      "Round: 29    Test: 0.719791666667 Train: 0.904761904762\n",
      "Round: 30    Test: 0.73125        Train: 0.901020408163\n",
      "Round: 31    Test: 0.727604166667 Train: 0.903911564626\n",
      "Round: 32    Test: 0.727604166667 Train: 0.909353741497\n",
      "Round: 33    Test: 0.723958333333 Train: 0.909353741497\n",
      "Round: 34    Test: 0.724479166667 Train: 0.911394557823\n",
      "Round: 35    Test: 0.719270833333 Train: 0.914795918367\n",
      "Round: 36    Test: 0.7296875      Train: 0.917006802721\n",
      "Round: 37    Test: 0.725          Train: 0.92074829932\n",
      "Round: 38    Test: 0.727083333333 Train: 0.92380952381\n",
      "Round: 39    Test: 0.719270833333 Train: 0.923129251701\n",
      "Round: 40    Test: 0.721875       Train: 0.924489795918\n",
      "Round: 41    Test: 0.73125        Train: 0.922448979592\n",
      "Round: 42    Test: 0.736979166667 Train: 0.921428571429\n",
      "Round: 43    Test: 0.727604166667 Train: 0.927891156463\n",
      "Round: 44    Test: 0.73125        Train: 0.925510204082\n",
      "Round: 45    Test: 0.733333333333 Train: 0.931632653061\n",
      "Round: 46    Test: 0.725520833333 Train: 0.929931972789\n",
      "Round: 47    Test: 0.727083333333 Train: 0.933503401361\n",
      "Round: 48    Test: 0.727604166667 Train: 0.930272108844\n",
      "Round: 49    Test: 0.721354166667 Train: 0.932823129252\n",
      "Round: 50    Test: 0.733333333333 Train: 0.93231292517\n",
      "\n",
      "CPU times: user 4min 49s, sys: 15.3 s, total: 5min 4s\n",
      "Wall time: 3min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\"\"\" glorot 4-layer: batch, drop, batch, drop, batch drop \n",
    "    random indexing without replacement, ELU, epsilon=1e-9  \"\"\"\n",
    "\n",
    "# batch normalization code adapted from \n",
    "# https://groups.google.com/forum/#!topic/theano-users/dMV6aabL1Ds \n",
    "\n",
    "\n",
    "import theano\n",
    "from theano import tensor as T\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "from theano.tensor.nnet.bn import batch_normalization\n",
    "import numpy as np\n",
    "\n",
    "srng = RandomStreams()\n",
    "\n",
    "def shuffle(x, y):\n",
    "    # helper function to shuffle indicies each loop \n",
    "    index = np.random.choice(len(x), len(x), replace=False)\n",
    "    return x[index], y[index]\n",
    "\n",
    "def floatX(X):\n",
    "    return np.asarray(X, dtype=theano.config.floatX)\n",
    "\n",
    "def init_weights(shape):\n",
    "    (h, w) = shape\n",
    "    # Glorot normalization - last factor depends on non-linearity\n",
    "    # 0.25 for sigmoid and 0.1 for softmax, 1.0 for tanh or Relu\n",
    "    normalizer = 2.0 * np.sqrt(6) / np.sqrt(h + w) * 1.0\n",
    "    return theano.shared(floatX((np.random.random_sample(shape) - 0.5) * normalizer))\n",
    "\n",
    "def rectify(X, alpha=0.01):\n",
    "#     return T.maximum(X, 0.)\n",
    "#    return T.maximum(X, 0.1*X)  #leaky rectifier\n",
    "     return T.switch(X > 0, X, alpha * (T.exp(X) - 1)) # ELU\n",
    "\n",
    "def softmax(X):\n",
    "    e_x = T.exp(X - X.max(axis=1).dimshuffle(0, 'x'))\n",
    "    return e_x / e_x.sum(axis=1).dimshuffle(0, 'x')\n",
    "\n",
    "def RMSprop(cost, params, lr=0.001, rho=0.99, epsilon=1e-9):\n",
    "    grads = T.grad(cost=cost, wrt=params)\n",
    "    updates = []\n",
    "    for p, g in zip(params, grads):\n",
    "        acc = theano.shared(p.get_value() * 0.)\n",
    "        acc_new = rho * acc + (1 - rho) * g ** 2\n",
    "        gradient_scaling = T.sqrt(acc_new + epsilon)\n",
    "        g = g / gradient_scaling\n",
    "        updates.append((acc, acc_new))\n",
    "        updates.append((p, p - lr * g))\n",
    "    return updates\n",
    "\n",
    "def dropout(X, p=0.):\n",
    "    if p > 0:\n",
    "        retain_prob = 1 - p\n",
    "        X *= srng.binomial(X.shape, p=retain_prob, dtype=theano.config.floatX)\n",
    "        X /= retain_prob\n",
    "    return X\n",
    "\n",
    "\n",
    "def model(X, w_h, b_h, g_h, bb_h, w_h2, b_h2, g_h2, bb_h2, \n",
    "          w_o, b_ho, g_ho, bb_ho, p_drop_input, p_drop_hidden):\n",
    "    X = T.dot(X, w_h) + b_h\n",
    "    X = batch_normalization(X, gamma= g_h, beta= bb_h, \n",
    "                            mean= X.mean((0,), keepdims=True),\n",
    "                            std= T.ones_like(X.var((0,), keepdims = True)), \n",
    "                            mode='high_mem') \n",
    "    X = dropout(X, p_drop_hidden)\n",
    "    h = rectify(X)\n",
    "\n",
    "    h  = T.dot(h, w_h2) + b_h2\n",
    "    h = batch_normalization(h, gamma= g_h2, beta= bb_h2, \n",
    "                            mean= h.mean((0,), keepdims=True),\n",
    "                            std= T.ones_like(h.var((0,), keepdims = True)), \n",
    "                            mode='high_mem') \n",
    "    h = dropout(h, p_drop_hidden)\n",
    "    h2 = rectify(h)\n",
    "\n",
    "    h2 = T.dot(h2, w_o) + b_ho\n",
    "    h2 = batch_normalization(h2, gamma= g_ho, beta= bb_ho, \n",
    "                            mean= h2.mean((0,), keepdims=True),\n",
    "                            std= T.ones_like(h2.var((0,), keepdims = True)), \n",
    "                            mode='high_mem') \n",
    "    h2 = dropout(h2, p_drop_hidden)\n",
    "    py_x = softmax(h2)\n",
    "    return h, h2, py_x\n",
    "\n",
    "\n",
    "X = T.fmatrix()\n",
    "Y = T.fmatrix()\n",
    "\n",
    "batch_size = 60\n",
    "\n",
    "h1_size = 600\n",
    "h2_size = 550\n",
    "\n",
    "w_h = init_weights((len(vectorizer.vocabulary_), h1_size))\n",
    "b_h = theano.shared(floatX(np.zeros(h1_size)))\n",
    "g_h = theano.shared(floatX(np.ones((h1_size))))\n",
    "bb_h = theano.shared(floatX(np.zeros((h1_size))))\n",
    "\n",
    "w_h2 = init_weights((h1_size, h2_size))\n",
    "b_h2 = theano.shared(floatX(np.zeros(h2_size)))\n",
    "g_h2 = theano.shared(floatX(np.ones((h2_size))))\n",
    "bb_h2 = theano.shared(floatX(np.zeros((h2_size))))\n",
    "\n",
    "w_o = init_weights((h2_size, yTest.shape[1]))\n",
    "b_ho = theano.shared(floatX(np.zeros(yTest.shape[1])))\n",
    "g_ho = theano.shared(floatX(np.ones((yTest.shape[1]))))\n",
    "bb_ho = theano.shared(floatX(np.zeros((yTest.shape[1]))))\n",
    "\n",
    "noise_h, noise_h2, noise_py_x = model(X, w_h, b_h, g_h, bb_h, \n",
    "                                      w_h2, b_h2, g_h2, bb_h2, \n",
    "                                      w_o, b_ho, g_ho, bb_ho, .0, .2)\n",
    "\n",
    "h, h2, py_x = model(X, w_h, b_h, g_h, bb_h, \n",
    "                    w_h2, b_h2, g_h2, bb_h2, \n",
    "                    w_o, b_ho, g_ho, bb_ho, .0, .0)\n",
    "\n",
    "y_x = T.argmax(py_x, axis=1)\n",
    "\n",
    "\n",
    "cost = T.mean(T.nnet.categorical_crossentropy(noise_py_x, Y))\n",
    "params = [w_h, b_h, g_h, bb_h, w_h2, b_h2, g_h2, bb_h2, \n",
    "          w_o, b_ho, g_ho, bb_ho]\n",
    "updates = RMSprop(cost, params, lr=0.0001)\n",
    "\n",
    "train = theano.function(inputs=[X, Y], outputs=cost, updates=updates, allow_input_downcast=True)\n",
    "predict = theano.function(inputs=[X], outputs=y_x, allow_input_downcast=True)\n",
    "\n",
    "\n",
    "for i in range(51):\n",
    "\n",
    "    for start, end in zip(range(0, len(xTrain), batch_size), range(batch_size, len(xTrain), batch_size)):\n",
    "        cost = train(xTrain[start:end], yTrain[start:end])\n",
    "        \n",
    "    xTrain, yTrain = shuffle(xTrain, yTrain)\n",
    "    xTest, yTest   = shuffle(xTest, yTest)\n",
    "\n",
    "    trr, tr = [], []\n",
    "    for start, end in zip(range(0, len(xTrain), batch_size), range(batch_size, len(xTrain), batch_size)):        \n",
    "        trr += [np.argmax(yTrain[start:end], axis=1) == predict(xTrain[start:end])]\n",
    "\n",
    "    for start, end in zip(range(0, len(xTest), batch_size), range(batch_size, len(xTest), batch_size)):\n",
    "        tr += [np.argmax(yTest[start:end], axis=1) == predict(xTest[start:end])]\n",
    "\n",
    "    print \"Round: %-5s Test: %-14s Train: %-8s\" % (i, np.mean(tr), np.mean(trr))\n",
    "    \n",
    "print\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round: 0     Test: 0.544791666667 Train: 0.605782312925\n",
      "Round: 1     Test: 0.6046875      Train: 0.66768707483\n",
      "Round: 2     Test: 0.635416666667 Train: 0.71037414966\n",
      "Round: 3     Test: 0.665625       Train: 0.741666666667\n",
      "Round: 4     Test: 0.6734375      Train: 0.767517006803\n",
      "Round: 5     Test: 0.690104166667 Train: 0.790476190476\n",
      "Round: 6     Test: 0.698958333333 Train: 0.804081632653\n",
      "Round: 7     Test: 0.706770833333 Train: 0.822108843537\n",
      "Round: 8     Test: 0.697395833333 Train: 0.828571428571\n",
      "Round: 9     Test: 0.715104166667 Train: 0.844387755102\n",
      "Round: 10    Test: 0.716666666667 Train: 0.848979591837\n",
      "Round: 11    Test: 0.715625       Train: 0.857993197279\n",
      "Round: 12    Test: 0.723958333333 Train: 0.862755102041\n",
      "Round: 13    Test: 0.725520833333 Train: 0.875170068027\n",
      "Round: 14    Test: 0.725          Train: 0.881972789116\n",
      "Round: 15    Test: 0.721875       Train: 0.889965986395\n",
      "Round: 16    Test: 0.729166666667 Train: 0.895238095238\n",
      "Round: 17    Test: 0.717708333333 Train: 0.898299319728\n",
      "Round: 18    Test: 0.730729166667 Train: 0.905442176871\n",
      "Round: 19    Test: 0.73125        Train: 0.908673469388\n",
      "Round: 20    Test: 0.7171875      Train: 0.911734693878\n",
      "Round: 21    Test: 0.722916666667 Train: 0.913095238095\n",
      "Round: 22    Test: 0.728125       Train: 0.921768707483\n",
      "Round: 23    Test: 0.727083333333 Train: 0.921428571429\n",
      "Round: 24    Test: 0.7328125      Train: 0.920918367347\n",
      "Round: 25    Test: 0.728645833333 Train: 0.919557823129\n",
      "Round: 26    Test: 0.730208333333 Train: 0.925340136054\n",
      "Round: 27    Test: 0.717708333333 Train: 0.931802721088\n",
      "Round: 28    Test: 0.71875        Train: 0.925340136054\n",
      "Round: 29    Test: 0.726041666667 Train: 0.933163265306\n",
      "\n",
      "CPU times: user 2min 44s, sys: 6.94 s, total: 2min 51s\n",
      "Wall time: 1min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\"\"\" glorot 4-layer: batch, drop, batch, drop, batch drop \n",
    "    random indexing without replacement, ELU, epsilon=1e-9  \"\"\"\n",
    "\n",
    "# batch normalization code adapted from \n",
    "# https://groups.google.com/forum/#!topic/theano-users/dMV6aabL1Ds \n",
    "\n",
    "\n",
    "import theano\n",
    "from theano import tensor as T\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "from theano.tensor.nnet.bn import batch_normalization\n",
    "import numpy as np\n",
    "\n",
    "srng = RandomStreams()\n",
    "\n",
    "def floatX(X):\n",
    "    return np.asarray(X, dtype=theano.config.floatX)\n",
    "\n",
    "def init_weights(shape):\n",
    "    (h, w) = shape\n",
    "    # Glorot normalization - last factor depends on non-linearity\n",
    "    # 0.25 for sigmoid and 0.1 for softmax, 1.0 for tanh or Relu\n",
    "    normalizer = 2.0 * np.sqrt(6) / np.sqrt(h + w) * 1.0\n",
    "    return theano.shared(floatX((np.random.random_sample(shape) - 0.5) * normalizer))\n",
    "\n",
    "def rectify(X, alpha=0.01):\n",
    "#     return T.maximum(X, 0.)\n",
    "#    return T.maximum(X, 0.1*X)  #leaky rectifier\n",
    "     return T.switch(X > 0, X, alpha * (T.exp(X) - 1)) # ELU\n",
    "\n",
    "def softmax(X):\n",
    "    e_x = T.exp(X - X.max(axis=1).dimshuffle(0, 'x'))\n",
    "    return e_x / e_x.sum(axis=1).dimshuffle(0, 'x')\n",
    "\n",
    "def RMSprop(cost, params, lr=0.001, rho=0.99, epsilon=1e-9):\n",
    "    grads = T.grad(cost=cost, wrt=params)\n",
    "    updates = []\n",
    "    for p, g in zip(params, grads):\n",
    "        acc = theano.shared(p.get_value() * 0.)\n",
    "        acc_new = rho * acc + (1 - rho) * g ** 2\n",
    "        gradient_scaling = T.sqrt(acc_new + epsilon)\n",
    "        g = g / gradient_scaling\n",
    "        updates.append((acc, acc_new))\n",
    "        updates.append((p, p - lr * g))\n",
    "    return updates\n",
    "\n",
    "def model(X, w_h, g_h, bb_h, w_h2, g_h2, bb_h2, w_o, g_ho, bb_ho):\n",
    "    \n",
    "    X = T.dot(X, w_h) \n",
    "    X = batch_normalization(X, gamma= g_h, beta= bb_h, \n",
    "                            mean= X.mean((0,), keepdims=True),\n",
    "                            std= T.ones_like(X.var((0,), keepdims = True)), \n",
    "                            mode='high_mem') \n",
    "    h = rectify(X)\n",
    "\n",
    "    h  = T.dot(h, w_h2)\n",
    "    h = batch_normalization(h, gamma= g_h2, beta= bb_h2, \n",
    "                            mean= h.mean((0,), keepdims=True),\n",
    "                            std= T.ones_like(h.var((0,), keepdims = True)), \n",
    "                            mode='high_mem') \n",
    "    h2 = rectify(h)\n",
    "\n",
    "    h2 = T.dot(h2, w_o)\n",
    "    h2 = batch_normalization(h2, gamma= g_ho, beta= bb_ho, \n",
    "                            mean= h2.mean((0,), keepdims=True),\n",
    "                            std= T.ones_like(h2.var((0,), keepdims = True)), \n",
    "                            mode='high_mem') \n",
    "    py_x = softmax(h2)\n",
    "    return h, h2, py_x\n",
    "\n",
    "\n",
    "X = T.fmatrix()\n",
    "Y = T.fmatrix()\n",
    "\n",
    "batch_size = 60\n",
    "\n",
    "h1_size = 600\n",
    "h2_size = 550\n",
    "h3_size = 550\n",
    "\n",
    "w_h = init_weights((len(vectorizer.vocabulary_), h1_size))\n",
    "g_h = theano.shared(floatX(np.ones((h1_size))))\n",
    "bb_h = theano.shared(floatX(np.zeros((h1_size))))\n",
    "\n",
    "w_h2 = init_weights((h1_size, h2_size))\n",
    "g_h2 = theano.shared(floatX(np.ones((h2_size))))\n",
    "bb_h2 = theano.shared(floatX(np.zeros((h2_size))))\n",
    "\n",
    "w_h3 = init_weights((h2_size, h3_size))\n",
    "g_h3 = theano.shared(floatX(np.ones((h3_size))))\n",
    "bb_h3 = theano.shared(floatX(np.zeros((h3_size))))\n",
    "\n",
    "w_o = init_weights((h3_size, yTest.shape[1]))\n",
    "g_ho = theano.shared(floatX(np.ones((yTest.shape[1]))))\n",
    "bb_ho = theano.shared(floatX(np.zeros((yTest.shape[1]))))\n",
    "\n",
    "noise_h, noise_h2, noise_py_x = model(X, w_h, g_h, bb_h, \n",
    "                                      w_h2, g_h2, bb_h2, \n",
    "                                      w_o, g_ho, bb_ho)\n",
    "\n",
    "h, h2, py_x = model(X, w_h, g_h, bb_h, \n",
    "                    w_h2, g_h2, bb_h2, \n",
    "                    w_o, g_ho, bb_ho)\n",
    "\n",
    "y_x = T.argmax(py_x, axis=1)\n",
    "\n",
    "\n",
    "cost = T.mean(T.nnet.categorical_crossentropy(noise_py_x, Y))\n",
    "params = [w_h, g_h, bb_h, w_h2, g_h2, bb_h2, \n",
    "          w_o, g_ho, bb_ho]\n",
    "updates = RMSprop(cost, params, lr=0.0001)\n",
    "\n",
    "train = theano.function(inputs=[X, Y], outputs=cost, updates=updates, allow_input_downcast=True)\n",
    "predict = theano.function(inputs=[X], outputs=y_x, allow_input_downcast=True)\n",
    "\n",
    "\n",
    "for i in range(30):\n",
    "\n",
    "    for start, end in zip(range(0, len(xTrain), batch_size), range(batch_size, len(xTrain), batch_size)):\n",
    "        cost = train(xTrain[start:end], yTrain[start:end])\n",
    "        \n",
    "    xTrain, yTrain = shuffle(xTrain, yTrain)\n",
    "    xTest, yTest   = shuffle(xTest, yTest)\n",
    "\n",
    "    trr, tr = [], []\n",
    "    for start, end in zip(range(0, len(xTrain), batch_size), range(batch_size, len(xTrain), batch_size)):        \n",
    "        trr += [np.argmax(yTrain[start:end], axis=1) == predict(xTrain[start:end])]\n",
    "\n",
    "    for start, end in zip(range(0, len(xTest), batch_size), range(batch_size, len(xTest), batch_size)):\n",
    "        tr += [np.argmax(yTest[start:end], axis=1) == predict(xTest[start:end])]\n",
    "\n",
    "    print \"Round: %-5s Test: %-14s Train: %-8s\" % (i, np.mean(tr), np.mean(trr))\n",
    "    \n",
    "print\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round: 0     Test: 0.628645833333 Train: 0.698469387755\n",
      "Round: 1     Test: 0.676041666667 Train: 0.766666666667\n",
      "Round: 2     Test: 0.703125       Train: 0.804421768707\n",
      "Round: 3     Test: 0.7125         Train: 0.831972789116\n",
      "Round: 4     Test: 0.7203125      Train: 0.863605442177\n",
      "Round: 5     Test: 0.705208333333 Train: 0.854761904762\n",
      "Round: 6     Test: 0.715625       Train: 0.890986394558\n",
      "Round: 7     Test: 0.714583333333 Train: 0.907482993197\n",
      "Round: 8     Test: 0.7234375      Train: 0.919387755102\n",
      "Round: 9     Test: 0.7234375      Train: 0.927040816327\n",
      "Round: 10    Test: 0.714583333333 Train: 0.926530612245\n",
      "\n",
      "CPU times: user 3min 15s, sys: 7.69 s, total: 3min 23s\n",
      "Wall time: 2min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\"\"\" 5-layer \"\"\"\n",
    "\n",
    "# batch normalization code adapted from \n",
    "# https://groups.google.com/forum/#!topic/theano-users/dMV6aabL1Ds \n",
    "\n",
    "\n",
    "import theano\n",
    "from theano import tensor as T\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "from theano.tensor.nnet.bn import batch_normalization\n",
    "import numpy as np\n",
    "\n",
    "srng = RandomStreams()\n",
    "\n",
    "def floatX(X):\n",
    "    return np.asarray(X, dtype=theano.config.floatX)\n",
    "\n",
    "def init_weights(shape):\n",
    "    (h, w) = shape\n",
    "    # Glorot normalization - last factor depends on non-linearity\n",
    "    # 0.25 for sigmoid and 0.1 for softmax, 1.0 for tanh or Relu\n",
    "    normalizer = 2.0 * np.sqrt(6) / np.sqrt(h + w) * 1.0\n",
    "    return theano.shared(floatX((np.random.random_sample(shape) - 0.5) * normalizer))\n",
    "\n",
    "def rectify(X, alpha=0.01):\n",
    "#     return T.maximum(X, 0.)\n",
    "#    return T.maximum(X, 0.1*X)  #leaky rectifier\n",
    "     return T.switch(X > 0, X, alpha * (T.exp(X) - 1)) # ELU\n",
    "\n",
    "def softmax(X):\n",
    "    e_x = T.exp(X - X.max(axis=1).dimshuffle(0, 'x'))\n",
    "    return e_x / e_x.sum(axis=1).dimshuffle(0, 'x')\n",
    "\n",
    "def RMSprop(cost, params, lr=0.001, rho=0.99, epsilon=1e-9):\n",
    "    grads = T.grad(cost=cost, wrt=params)\n",
    "    updates = []\n",
    "    for p, g in zip(params, grads):\n",
    "        acc = theano.shared(p.get_value() * 0.)\n",
    "        acc_new = rho * acc + (1 - rho) * g ** 2\n",
    "        gradient_scaling = T.sqrt(acc_new + epsilon)\n",
    "        g = g / gradient_scaling\n",
    "        updates.append((acc, acc_new))\n",
    "        updates.append((p, p - lr * g))\n",
    "    return updates\n",
    "\n",
    "def model(X, w_h, g_h, bb_h, w_h2, g_h2, bb_h2,\n",
    "          w_h3, g_h3, bb_h3, w_o, g_ho, bb_ho):\n",
    "    \n",
    "    X = T.dot(X, w_h) \n",
    "    X = batch_normalization(X, gamma= g_h, beta= bb_h, \n",
    "                            mean= X.mean((1,), keepdims=True),\n",
    "                            std= T.ones_like(X.std((1,), keepdims = True)), \n",
    "                            mode='high_mem') \n",
    "    h = rectify(X)\n",
    "\n",
    "    h  = T.dot(h, w_h2)\n",
    "    h = batch_normalization(h, gamma= g_h2, beta= bb_h2, \n",
    "                            mean= h.mean((1,), keepdims=True),\n",
    "                            std= T.ones_like(h.std((1,), keepdims = True)), \n",
    "                            mode='high_mem') \n",
    "    h2 = rectify(h)\n",
    "\n",
    "    h2 = T.dot(h2, w_h3)\n",
    "    h2 = batch_normalization(h2, gamma= g_h3, beta= bb_h3, \n",
    "                            mean= h2.mean((1,), keepdims=True),\n",
    "                            std= T.ones_like(h2.std((1,), keepdims = True)), \n",
    "                            mode='high_mem') \n",
    "    h3 = rectify(h2)\n",
    "    \n",
    "    h3 = T.dot(h3, w_o)\n",
    "    h3 = batch_normalization(h3, gamma= g_ho, beta= bb_ho, \n",
    "                            mean= h3.mean((1,), keepdims=True),\n",
    "                            std= T.ones_like(h3.std((1,), keepdims = True)), \n",
    "                            mode='high_mem') \n",
    "    py_x = softmax(h3)\n",
    "    return h, h2, h3, py_x\n",
    "\n",
    "\n",
    "X = T.fmatrix()\n",
    "Y = T.fmatrix()\n",
    "\n",
    "batch_size = 60\n",
    "\n",
    "h1_size = 1000\n",
    "h2_size = 1000\n",
    "h3_size = 1000\n",
    "\n",
    "w_h = init_weights((len(vectorizer.vocabulary_), h1_size))\n",
    "g_h = theano.shared(floatX(np.ones((h1_size))))\n",
    "bb_h = theano.shared(floatX(np.zeros((h1_size))))\n",
    "\n",
    "w_h2 = init_weights((h1_size, h2_size))\n",
    "g_h2 = theano.shared(floatX(np.ones((h2_size))))\n",
    "bb_h2 = theano.shared(floatX(np.zeros((h2_size))))\n",
    "\n",
    "w_h3 = init_weights((h2_size, h3_size))\n",
    "g_h3 = theano.shared(floatX(np.ones((h3_size))))\n",
    "bb_h3 = theano.shared(floatX(np.zeros((h3_size))))\n",
    "\n",
    "w_o = init_weights((h3_size, yTest.shape[1]))\n",
    "g_ho = theano.shared(floatX(np.ones((yTest.shape[1]))))\n",
    "bb_ho = theano.shared(floatX(np.zeros((yTest.shape[1]))))\n",
    "\n",
    "noise_h, noise_h2, noise_h3, noise_py_x = model(X, w_h, g_h, bb_h, \n",
    "                                      w_h2, g_h2, bb_h2, \n",
    "                                       w_h3, g_h3, bb_h3, \n",
    "                                      w_o, g_ho, bb_ho)\n",
    "\n",
    "h, h2, h3, py_x = model(X, w_h, g_h, bb_h, \n",
    "                    w_h2, g_h2, bb_h2, \n",
    "                     w_h3, g_h3, bb_h3, \n",
    "                    w_o, g_ho, bb_ho)\n",
    "\n",
    "y_x = T.argmax(py_x, axis=1)\n",
    "\n",
    "\n",
    "cost = T.mean(T.nnet.categorical_crossentropy(noise_py_x, Y))\n",
    "params = [w_h, g_h, bb_h, w_h2, g_h2, bb_h2, \n",
    "           w_h3, g_h3, bb_h3, w_o, g_ho, bb_ho]\n",
    "updates = RMSprop(cost, params, lr=0.0001)\n",
    "\n",
    "train = theano.function(inputs=[X, Y], outputs=cost, updates=updates, allow_input_downcast=True)\n",
    "predict = theano.function(inputs=[X], outputs=y_x, allow_input_downcast=True)\n",
    "\n",
    "\n",
    "for i in range(11):\n",
    "\n",
    "    for start, end in zip(range(0, len(xTrain), batch_size), range(batch_size, len(xTrain), batch_size)):\n",
    "        cost = train(xTrain[start:end], yTrain[start:end])\n",
    "        \n",
    "    xTrain, yTrain = shuffle(xTrain, yTrain)\n",
    "    xTest, yTest   = shuffle(xTest, yTest)\n",
    "\n",
    "    trr, tr = [], []\n",
    "    for start, end in zip(range(0, len(xTrain), batch_size), range(batch_size, len(xTrain), batch_size)):        \n",
    "        trr += [np.argmax(yTrain[start:end], axis=1) == predict(xTrain[start:end])]\n",
    "\n",
    "    for start, end in zip(range(0, len(xTest), batch_size), range(batch_size, len(xTest), batch_size)):\n",
    "        tr += [np.argmax(yTest[start:end], axis=1) == predict(xTest[start:end])]\n",
    "\n",
    "    print \"Round: %-5s Test: %-14s Train: %-8s\" % (i, np.mean(tr), np.mean(trr))\n",
    "    \n",
    "print\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Results from removing T.ones_like \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Round: 0     Test: 0.203125       Train: 0.198469387755\n",
    "Round: 1     Test: 0.2046875      Train: 0.198979591837\n",
    "Round: 2     Test: 0.205208333333 Train: 0.198979591837\n",
    "Round: 3     Test: 0.203125       Train: 0.198299319728\n",
    "Round: 4     Test: 0.203125       Train: 0.198979591837\n",
    "Round: 5     Test: 0.2046875      Train: 0.198979591837\n",
    "Round: 6     Test: 0.2046875      Train: 0.197959183673\n",
    "Round: 7     Test: 0.203645833333 Train: 0.19880952381\n",
    "Round: 8     Test: 0.205208333333 Train: 0.198979591837\n",
    "Round: 9     Test: 0.203125       Train: 0.198639455782\n",
    "Round: 10    Test: 0.205208333333 Train: 0.198979591837\n",
    "\n",
    "CPU times: user 3min 8s, sys: 6.83 s, total: 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
