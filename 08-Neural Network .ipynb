{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cross_validation import train_test_split, KFold, cross_val_score\n",
    "import sklearn.metrics as sk\n",
    "\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({u'Black': 1576,\n",
       "         u'Blue': 1573,\n",
       "         u'Green': 1566,\n",
       "         u'Red': 1575,\n",
       "         u'White': 1584})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modern = pd.read_pickle('data/5color_modern_no_name_hardmode.pkl')\n",
    "Counter(modern.colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After all the data munging the classes are still amazingly balanced.\n",
    "\n",
    "## Lets single out blue and red for a binary classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>colors</th>\n",
       "      <th>cmc</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1668</th>\n",
       "      <td>Dormant Gomazoa</td>\n",
       "      <td>Blue</td>\n",
       "      <td>3</td>\n",
       "      <td>Flying This enters the battlefield tapped. Thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>913</th>\n",
       "      <td>Prodigal Pyromancer</td>\n",
       "      <td>Red</td>\n",
       "      <td>3</td>\n",
       "      <td>Tap : This deals 1 damage to target creature o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2655</th>\n",
       "      <td>Scouring Sands</td>\n",
       "      <td>Red</td>\n",
       "      <td>2</td>\n",
       "      <td>This deals 1 damage to each creature your oppo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Evacuation</td>\n",
       "      <td>Blue</td>\n",
       "      <td>5</td>\n",
       "      <td>Return all creatures to their owners hands.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>Disarm</td>\n",
       "      <td>Blue</td>\n",
       "      <td>1</td>\n",
       "      <td>Unattach all Equipment from target creature.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2358</th>\n",
       "      <td>Paralyzing Grasp</td>\n",
       "      <td>Blue</td>\n",
       "      <td>3</td>\n",
       "      <td>Enchant creature Enchanted creature doesnt unt...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     name colors  cmc  \\\n",
       "1668      Dormant Gomazoa   Blue    3   \n",
       "913   Prodigal Pyromancer    Red    3   \n",
       "2655       Scouring Sands    Red    2   \n",
       "26             Evacuation   Blue    5   \n",
       "119                Disarm   Blue    1   \n",
       "2358     Paralyzing Grasp   Blue    3   \n",
       "\n",
       "                                                   text  \n",
       "1668  Flying This enters the battlefield tapped. Thi...  \n",
       "913   Tap : This deals 1 damage to target creature o...  \n",
       "2655  This deals 1 damage to each creature your oppo...  \n",
       "26          Return all creatures to their owners hands.  \n",
       "119        Unattach all Equipment from target creature.  \n",
       "2358  Enchant creature Enchanted creature doesnt unt...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UG = modern.loc[modern['colors'].isin(['Blue', 'Red'])]\n",
    "\n",
    "UG.reset_index(inplace=True)\n",
    "UG.pop('index')\n",
    "\n",
    "UG[['name', 'colors', 'cmc', 'text']].sample(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Blue</th>\n",
       "      <th>Red</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Blue  Red\n",
       "0     1    0\n",
       "1     0    1\n",
       "2     1    0\n",
       "3     1    0\n",
       "4     1    0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummies = pd.get_dummies(UG.colors)\n",
    "dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2361, 815)\n",
      "(2361, 2)\n",
      "<type 'numpy.ndarray'>\n",
      "<type 'numpy.ndarray'>\n",
      "(2361, 815)\n",
      "(2361, 2)\n",
      "<type 'numpy.ndarray'>\n",
      "<type 'numpy.ndarray'>\n",
      "There are 815 words in the vocabulary.\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "\n",
    "vec_X = vectorizer.fit_transform(UG['text'])\n",
    "\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(vec_X, dummies,\n",
    "                                             random_state=42)\n",
    "\n",
    "xTrain = np.asarray(xTrain.todense())\n",
    "xTest  = np.asarray(xTest.todense())\n",
    "yTrain = np.asarray(yTrain)\n",
    "yTest  = np.asarray(yTest)\n",
    "\n",
    "print xTrain.shape\n",
    "print yTrain.shape\n",
    "print type(xTrain)\n",
    "print type(yTrain)\n",
    "\n",
    "# xTrain = xTrain.reshape(-1, 1, 1, 815)\n",
    "# xTest = xTest.reshape(-1, 1, 1, 815)\n",
    "\n",
    "print xTrain.shape\n",
    "print yTrain.shape\n",
    "print type(xTrain)\n",
    "print type(yTrain)\n",
    "\n",
    "print \"There are {:,} words in the vocabulary.\".format(len(vectorizer.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round: 0  Test: 0.651842439644  Train: 0.629394324439\n",
      "Round: 30  Test: 0.810673443456  Train: 0.831427361288\n",
      "Round: 60  Test: 0.833545108005  Train: 0.849216433715\n",
      "Round: 90  Test: 0.853875476493  Train: 0.861075815332\n",
      "Round: 120  Test: 0.852604828463  Train: 0.866581956798\n",
      "Round: 150  Test: 0.855146124524  Train: 0.873358746294\n",
      "Round: 180  Test: 0.861499364676  Train: 0.879288437103\n",
      "Round: 210  Test: 0.864040660737  Train: 0.885218127912\n",
      "Round: 240  Test: 0.870393900889  Train: 0.889030072003\n",
      "Round: 270  Test: 0.866581956798  Train: 0.892842016095\n",
      "Round: 300  Test: 0.866581956798  Train: 0.894536213469\n",
      "Round: 330  Test: 0.869123252859  Train: 0.894112664125\n",
      "Round: 360  Test: 0.87166454892  Train: 0.895383312156\n",
      "Round: 390  Test: 0.87166454892  Train: 0.895806861499\n"
     ]
    }
   ],
   "source": [
    "import theano\n",
    "from theano import tensor as T\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "\n",
    "\n",
    "def floatX(X):\n",
    "    return np.asarray(X, dtype=theano.config.floatX)\n",
    "\n",
    "def init_weights(shape):\n",
    "    (h, w) = shape\n",
    "    normalizer = 2.0 * sqrt(6) / sqrt(h + w) * .2  #factors: correct for uni[0,1], glo, glo, softmax deriv\n",
    "    return theano.shared(floatX((np.random.random_sample(shape) - 0.5) \\\n",
    "                                * normalizer))  #code for using Glorot init\n",
    "    \n",
    "def model(X, w):\n",
    "    return T.nnet.softmax(T.dot(X, w))\n",
    "\n",
    "def adaDelta(cost, params, eta=0.2, rho=0.9, epsilon=1e-6):\n",
    "    grads = T.grad(cost=cost, wrt=params)\n",
    "    updates = []\n",
    "    for p, g in zip(params, grads):\n",
    "        #calc g-squared\n",
    "        gSq = theano.shared(p.get_value() * 0.)\n",
    "        dwSq = theano.shared(p.get_value() * 0.)\n",
    "\n",
    "        #exp smoothed g squared\n",
    "        gSqNew = rho * gSq + (1 - rho) * g * g\n",
    "\n",
    "        #calc dx-squared\n",
    "        dw = eta * T.sqrt(dwSq + epsilon) * g / T.sqrt(gSq + epsilon)\n",
    "        dwSqNew = rho * dwSq + (1 - rho) * dw * dw\n",
    "\n",
    "        updates.append((dwSq, dwSqNew))\n",
    "        updates.append((gSq, gSqNew))\n",
    "        updates.append((p, p - dw))\n",
    "    return updates\n",
    "\n",
    "X = T.fmatrix()\n",
    "Y = T.fmatrix()\n",
    "# grad_list = theano.shared(np.array([0,0]), name='grad_list')\n",
    "\n",
    "w = init_weights((815 , 2))\n",
    "\n",
    "py_x = model(X, w)\n",
    "y_pred = T.argmax(py_x, axis=1)\n",
    "\n",
    "cost = T.mean(T.nnet.categorical_crossentropy(py_x, Y))\n",
    "gradient = T.grad(cost=cost, wrt=w)\n",
    "update = [[w, w - gradient * 0.1]]\n",
    "\n",
    "train = theano.function(inputs=[X, Y], \n",
    "                        outputs=[cost, gradient], \n",
    "                        updates=update, \n",
    "                        allow_input_downcast=True)\n",
    "\n",
    "predict = theano.function(inputs=[X], \n",
    "                          outputs=y_pred, \n",
    "                          allow_input_downcast=True)\n",
    "\n",
    "\n",
    "for i in range(401):\n",
    "# #     for start, end in zip(range(0, xTrain.shape[0], 128), \n",
    "# #                           range(128, xTrain.shape[0], 128)):\n",
    "# #         cost, gradient = train(xTrain[start:end], yTrain[start:end])\n",
    "    cost, gradient = train(xTrain, yTrain)\n",
    "    if i % 30 == 0: \n",
    "        tr = np.mean(np.argmax(yTest, axis=1) == predict(xTest))\n",
    "        trr =  np.mean(np.argmax(yTrain, axis=1) == predict(xTrain))\n",
    "        print 'Round:', i,\" Test:\", tr, ' Train:', trr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All Five Vs All Five\n",
    "\n",
    "And now the main event - simply comparing two colors was too easy. Five way classification of all the colors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5905, 1161)\n",
      "(5905, 5)\n",
      "<type 'numpy.ndarray'>\n",
      "<type 'numpy.ndarray'>\n",
      "(5905, 1161)\n",
      "(5905, 5)\n",
      "<type 'numpy.ndarray'>\n",
      "<type 'numpy.ndarray'>\n",
      "There are 1,161 words in the vocabulary.\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "\n",
    "y = pd.get_dummies(modern.colors)\n",
    "\n",
    "X = vectorizer.fit_transform(modern.text)\n",
    "\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(X, y, random_state=42)\n",
    "\n",
    "xTrain = np.asarray(xTrain.todense())\n",
    "xTest  = np.asarray(xTest.todense())\n",
    "yTrain = np.asarray(yTrain)\n",
    "yTest  = np.asarray(yTest)\n",
    "\n",
    "print xTrain.shape\n",
    "print yTrain.shape\n",
    "print type(xTrain)\n",
    "print type(yTrain)\n",
    "\n",
    "# xTrain = xTrain.reshape(-1, 1, 1, 815)\n",
    "# xTest = xTest.reshape(-1, 1, 1, 815)\n",
    "\n",
    "print xTrain.shape\n",
    "print yTrain.shape\n",
    "print type(xTrain)\n",
    "print type(yTrain)\n",
    "\n",
    "print \"There are {:,} words in the vocabulary.\".format(len(vectorizer.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round: 0  Test: 0.292026409345  Train: 0.27366638442\n",
      "Round: 500  Test: 0.601320467242  Train: 0.633192209992\n",
      "Round: 1000  Test: 0.627729812087  Train: 0.667739204064\n",
      "Round: 1500  Test: 0.641442356526  Train: 0.686875529213\n",
      "Round: 2000  Test: 0.655662772981  Train: 0.700423370025\n",
      "Round: 2500  Test: 0.66124936516  Train: 0.709060118544\n",
      "Round: 3000  Test: 0.664296597257  Train: 0.716680779001\n",
      "Round: 3500  Test: 0.676485525648  Train: 0.724132091448\n",
      "Round: 4000  Test: 0.682579989843  Train: 0.728704487722\n",
      "Round: 4500  Test: 0.687150837989  Train: 0.732938187976\n",
      "Round: 5000  Test: 0.688674454038  Train: 0.737849280271\n",
      "Round: 5500  Test: 0.688674454038  Train: 0.739881456393\n",
      "Round: 6000  Test: 0.686642965973  Train: 0.741913632515\n",
      "Round: 6500  Test: 0.691721686135  Train: 0.744623200677\n",
      "Round: 7000  Test: 0.694261046216  Train: 0.746655376799\n",
      "Round: 7500  Test: 0.695784662265  Train: 0.749364944962\n",
      "Round: 8000  Test: 0.695784662265  Train: 0.750381033023\n",
      "Round: 8500  Test: 0.694768918233  Train: 0.751905165114\n",
      "Round: 9000  Test: 0.695784662265  Train: 0.754445385267\n",
      "Round: 9500  Test: 0.696800406298  Train: 0.756816257409\n",
      "Round: 10000  Test: 0.698831894363  Train: 0.75766299746\n",
      "Wall time: 7min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import theano\n",
    "from theano import tensor as T\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn\n",
    "# %matplotlib inline\n",
    "\n",
    "def floatX(X):\n",
    "    return np.asarray(X, dtype=theano.config.floatX)\n",
    "\n",
    "def init_weights(shape):\n",
    "    (h, w) = shape\n",
    "    normalizer = 2.0 * sqrt(6) / sqrt(h + w) * .2  #factors: correct for uni[0,1], glo, glo, softmax deriv\n",
    "    return theano.shared(floatX((np.random.random_sample(shape) - 0.5) \\\n",
    "                                * normalizer))  #code for using Glorot init\n",
    "    \n",
    "def model(X, w):\n",
    "    return T.nnet.softmax(T.dot(X, w))\n",
    "\n",
    "def adaDelta(cost, params, eta=0.2, rho=0.9, epsilon=1e-6):\n",
    "    grads = T.grad(cost=cost, wrt=params)\n",
    "    updates = []\n",
    "    for p, g in zip(params, grads):\n",
    "        #calc g-squared\n",
    "        gSq = theano.shared(p.get_value() * 0.)\n",
    "        dwSq = theano.shared(p.get_value() * 0.)\n",
    "\n",
    "        #exp smoothed g squared\n",
    "        gSqNew = rho * gSq + (1 - rho) * g * g\n",
    "\n",
    "        #calc dx-squared\n",
    "        dw = eta * T.sqrt(dwSq + epsilon) * g / T.sqrt(gSq + epsilon)\n",
    "        dwSqNew = rho * dwSq + (1 - rho) * dw * dw\n",
    "\n",
    "        updates.append((dwSq, dwSqNew))\n",
    "        updates.append((gSq, gSqNew))\n",
    "        updates.append((p, p - dw))\n",
    "    return updates\n",
    "\n",
    "X = T.fmatrix()\n",
    "Y = T.fmatrix()\n",
    "\n",
    "w = init_weights((len(vectorizer.vocabulary_) , yTest.shape[1]))\n",
    "\n",
    "py_x = model(X, w)\n",
    "y_pred = T.argmax(py_x, axis=1)\n",
    "\n",
    "cost = T.mean(T.nnet.categorical_crossentropy(py_x, Y))\n",
    "gradient = T.grad(cost=cost, wrt=w)\n",
    "update = [[w, w - gradient * 0.1]]\n",
    "\n",
    "train = theano.function(inputs=[X, Y], \n",
    "                        outputs=[cost, gradient], \n",
    "                        updates=update, \n",
    "                        allow_input_downcast=True)\n",
    "\n",
    "predict = theano.function(inputs=[X], \n",
    "                          outputs=y_pred, \n",
    "                          allow_input_downcast=True)\n",
    "\n",
    "\n",
    "for i in range(10001):\n",
    "# #     for start, end in zip(range(0, xTrain.shape[0], 128), \n",
    "# #                           range(128, xTrain.shape[0], 128)):\n",
    "# #         cost, gradient = train(xTrain[start:end], yTrain[start:end])\n",
    "    cost, gradient = train(xTrain, yTrain)\n",
    "    if i % 500 == 0: \n",
    "        tr = np.mean(np.argmax(yTest, axis=1) == predict(xTest))\n",
    "        trr =  np.mean(np.argmax(yTrain, axis=1) == predict(xTrain))\n",
    "        print 'Round:', i,\" Test:\", tr, ' Train:', trr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazing results, almost equal to logistic regression at 5min. \n",
    "\n",
    "### 4-Layer Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round: 0  Test: 0.27780599289  Train: 0.302116850127\n",
      "Round: 10  Test: 0.280853224987  Train: 0.287552921253\n",
      "Round: 20  Test: 0.46063991874  Train: 0.49483488569\n",
      "Round: 30  Test: 0.602336211275  Train: 0.63200677392\n",
      "Round: 40  Test: 0.653631284916  Train: 0.690770533446\n",
      "Round: 50  Test: 0.672930421534  Train: 0.734462320068\n",
      "Round: 60  Test: 0.691213814119  Train: 0.759017781541\n",
      "Round: 70  Test: 0.694261046216  Train: 0.774259102456\n",
      "Round: 80  Test: 0.699847638395  Train: 0.788653683319\n",
      "Round: 90  Test: 0.710512950736  Train: 0.805927180356\n",
      "Round: 100  Test: 0.723209751143  Train: 0.818797629128\n",
      "Round: 110  Test: 0.723717623159  Train: 0.832345469941\n",
      "Round: 120  Test: 0.729304215338  Train: 0.847417442845\n",
      "Round: 130  Test: 0.725241239208  Train: 0.853852667231\n",
      "Round: 140  Test: 0.727272727273  Train: 0.86316680779\n",
      "Round: 150  Test: 0.733875063484  Train: 0.872650296359\n",
      "Round: 160  Test: 0.723717623159  Train: 0.878577476715\n",
      "Round: 170  Test: 0.732351447435  Train: 0.885012701101\n",
      "Round: 180  Test: 0.731843575419  Train: 0.890939881456\n",
      "Round: 190  Test: 0.733367191468  Train: 0.897713801863\n",
      "Round: 200  Test: 0.724225495175  Train: 0.901270110076\n",
      "Round: 210  Test: 0.732351447435  Train: 0.90533446232\n",
      "Round: 220  Test: 0.728796343321  Train: 0.910922946655\n",
      "Round: 230  Test: 0.729304215338  Train: 0.91532599492\n",
      "Round: 240  Test: 0.73031995937  Train: 0.918035563082\n",
      "Round: 250  Test: 0.728288471305  Train: 0.919559695174\n",
      "Round: 260  Test: 0.731335703403  Train: 0.922946655377\n",
      "Round: 270  Test: 0.727780599289  Train: 0.92616426757\n",
      "Round: 280  Test: 0.730827831386  Train: 0.928196443692\n",
      "Round: 290  Test: 0.732351447435  Train: 0.930059271804\n",
      "Round: 300  Test: 0.727272727273  Train: 0.931752751905\n",
      "Round: 310  Test: 0.72625698324  Train: 0.933954276037\n",
      "Round: 320  Test: 0.725749111224  Train: 0.934292972058\n",
      "Round: 330  Test: 0.721686135094  Train: 0.93683319221\n",
      "Round: 340  Test: 0.727272727273  Train: 0.937849280271\n",
      "Round: 350  Test: 0.724733367191  Train: 0.93734123624\n",
      "Round: 360  Test: 0.724225495175  Train: 0.939542760373\n",
      "Round: 370  Test: 0.72625698324  Train: 0.939881456393\n",
      "Round: 380  Test: 0.73031995937  Train: 0.942252328535\n",
      "Round: 390  Test: 0.724225495175  Train: 0.942929720576\n",
      "Round: 400  Test: 0.726764855256  Train: 0.943607112616\n",
      "Wall time: 3min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import theano\n",
    "from theano import tensor as T\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "import numpy as np\n",
    "\n",
    "srng = RandomStreams()\n",
    "\n",
    "def floatX(X):\n",
    "    return np.asarray(X, dtype=theano.config.floatX)\n",
    "\n",
    "def init_weights(shape):\n",
    "    return theano.shared(floatX(np.random.randn(*shape) * 0.01))\n",
    "\n",
    "def rectify(X, alpha=1.0):\n",
    "#     return T.maximum(X, 0.)\n",
    "    return T.maximum(X, 0.05*X)  #leaky rectifier\n",
    "#     return T.switch(X > 0, X, alpha * (T.exp(X) - 1)) # ELU\n",
    "\n",
    "def softmax(X):\n",
    "    e_x = T.exp(X - X.max(axis=1).dimshuffle(0, 'x'))\n",
    "    return e_x / e_x.sum(axis=1).dimshuffle(0, 'x')\n",
    "\n",
    "def RMSprop(cost, params, lr=0.001, rho=0.9, epsilon=1e-6):\n",
    "    grads = T.grad(cost=cost, wrt=params)\n",
    "    updates = []\n",
    "    for p, g in zip(params, grads):\n",
    "        acc = theano.shared(p.get_value() * 0.)\n",
    "        acc_new = rho * acc + (1 - rho) * g ** 2\n",
    "        gradient_scaling = T.sqrt(acc_new + epsilon)\n",
    "        g = g / gradient_scaling\n",
    "        updates.append((acc, acc_new))\n",
    "        updates.append((p, p - lr * g))\n",
    "    return updates\n",
    "\n",
    "def dropout(X, p=0.):\n",
    "    if p > 0:\n",
    "        retain_prob = 1 - p\n",
    "        X *= srng.binomial(X.shape, p=retain_prob, dtype=theano.config.floatX)\n",
    "        X /= retain_prob\n",
    "    return X\n",
    "\n",
    "def model(X, w_h, w_h2, w_o, p_drop_input, p_drop_hidden):\n",
    "    X = dropout(X, p_drop_input)\n",
    "    h = rectify(T.dot(X, w_h))\n",
    "\n",
    "    h = dropout(h, p_drop_hidden)\n",
    "    h2 = rectify(T.dot(h, w_h2))\n",
    "\n",
    "    h2 = dropout(h2, p_drop_hidden)\n",
    "    py_x = softmax(T.dot(h2, w_o))\n",
    "    return h, h2, py_x\n",
    "\n",
    "X = T.fmatrix()\n",
    "Y = T.fmatrix()\n",
    "\n",
    "# w = init_weights((len(vectorizer.vocabulary_) , yTest.shape[1])) # old \n",
    "w_h = init_weights((len(vectorizer.vocabulary_), 600))\n",
    "w_h2 = init_weights((600, 600))\n",
    "w_o = init_weights((600, yTest.shape[1]))\n",
    "\n",
    "noise_h, noise_h2, noise_py_x = model(X, w_h, w_h2, w_o, 0.2, 0.5)\n",
    "h, h2, py_x = model(X, w_h, w_h2, w_o, 0., 0.)\n",
    "y_x = T.argmax(py_x, axis=1)\n",
    "\n",
    "cost = T.mean(T.nnet.categorical_crossentropy(noise_py_x, Y))\n",
    "params = [w_h, w_h2, w_o]\n",
    "updates = RMSprop(cost, params, lr=0.0001)\n",
    "\n",
    "train = theano.function(inputs=[X, Y], outputs=cost, updates=updates, allow_input_downcast=True)\n",
    "predict = theano.function(inputs=[X], outputs=y_x, allow_input_downcast=True)\n",
    "\n",
    "for i in range(401):\n",
    "    for start, end in zip(range(0, len(xTrain), 128), range(128, len(xTrain), 128)):\n",
    "        cost = train(xTrain[start:end], yTrain[start:end])\n",
    "    if i%10 == 0: \n",
    "        tr = np.mean(np.argmax(yTest, axis=1) == predict(xTest))\n",
    "        trr =  np.mean(np.argmax(yTrain, axis=1) == predict(xTrain))\n",
    "        print 'Round:', i,\" Test:\", tr, ' Train:', trr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Strong signs of overfitting\n",
    "\n",
    "Next steps:  \n",
    "\n",
    "Leaky RELU swapped out for ELU, alpha = 1.0   \n",
    "Train accuracy dropped by 14% (good)  \n",
    "Test accuracy dropped by 3% (bad)   \n",
    "\n",
    "Leaky RELU swapped out for ELU, alpha = .5  \n",
    "Train accuracy dropped by 7% (good)  \n",
    "Test accuracy dropped by 1% (bad)   \n",
    "\n",
    "Leaky RELU  alpha = .1\n",
    "RHO = .99\n",
    "Train accuracy dropped by 7% (good)  \n",
    "Test accuracy up by 1% (good)  \n",
    "\n",
    "Next, batch normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round: 0  Test: 0.4281361097  Train: 0.435055038103\n",
      "Round: 10  Test: 0.594718131031  Train: 0.644199830652\n",
      "Round: 20  Test: 0.65718638903  Train: 0.71566469094\n",
      "Round: 30  Test: 0.680548501778  Train: 0.755122777307\n",
      "Round: 40  Test: 0.693245302184  Train: 0.781202370872\n",
      "Round: 50  Test: 0.703402742509  Train: 0.798814563929\n",
      "Round: 60  Test: 0.715083798883  Train: 0.815410668925\n",
      "Round: 70  Test: 0.720162519045  Train: 0.834546994073\n",
      "Round: 80  Test: 0.722701879126  Train: 0.846909398815\n",
      "Round: 90  Test: 0.72625698324  Train: 0.859610499577\n",
      "Round: 100  Test: 0.735906551549  Train: 0.867231160034\n",
      "Round: 110  Test: 0.728288471305  Train: 0.875867908552\n",
      "Round: 120  Test: 0.726764855256  Train: 0.88433530906\n",
      "Round: 130  Test: 0.731335703403  Train: 0.890431837426\n",
      "Round: 140  Test: 0.732859319451  Train: 0.898391193903\n",
      "Round: 150  Test: 0.733875063484  Train: 0.904487722269\n",
      "Round: 160  Test: 0.732859319451  Train: 0.908721422523\n",
      "Round: 170  Test: 0.730827831386  Train: 0.914140558848\n",
      "Round: 180  Test: 0.732859319451  Train: 0.91600338696\n",
      "Round: 190  Test: 0.732351447435  Train: 0.920575783235\n",
      "Round: 200  Test: 0.729812087354  Train: 0.922607959356\n",
      "Round: 210  Test: 0.726764855256  Train: 0.9266723116\n",
      "Round: 220  Test: 0.732351447435  Train: 0.929043183743\n",
      "Round: 230  Test: 0.725241239208  Train: 0.931075359865\n",
      "Round: 240  Test: 0.722701879126  Train: 0.934292972058\n",
      "Round: 250  Test: 0.725749111224  Train: 0.934801016088\n",
      "Round: 260  Test: 0.723209751143  Train: 0.937510584251\n",
      "Round: 270  Test: 0.723209751143  Train: 0.940389500423\n",
      "Round: 280  Test: 0.721686135094  Train: 0.941574936494\n",
      "Round: 290  Test: 0.719146775013  Train: 0.942082980525\n",
      "Round: 300  Test: 0.723209751143  Train: 0.944453852667\n",
      "Wall time: 2min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\"\"\" glorot 4-layer: dropout, dropout, dropout \"\"\"\n",
    "\n",
    "import theano\n",
    "from theano import tensor as T\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "import numpy as np\n",
    "\n",
    "srng = RandomStreams()\n",
    "\n",
    "def floatX(X):\n",
    "    return np.asarray(X, dtype=theano.config.floatX)\n",
    "\n",
    "def init_weights(shape):\n",
    "    (h, w) = shape\n",
    "    # Glorot normalization - last factor depends on non-linearity\n",
    "    # 0.25 for sigmoid and 0.1 for softmax, 1.0 for tanh or Relu\n",
    "    normalizer = 2.0 * np.sqrt(6) / np.sqrt(h + w) * 1.0\n",
    "    return theano.shared(floatX((np.random.random_sample(shape) - 0.5) * normalizer))\n",
    "\n",
    "\n",
    "def rectify(X, alpha=1.0):\n",
    "#     return T.maximum(X, 0.)\n",
    "    return T.maximum(X, 0.1*X)  #leaky rectifier\n",
    "#     return T.switch(X > 0, X, alpha * (T.exp(X) - 1)) # ELU\n",
    "\n",
    "def softmax(X):\n",
    "    e_x = T.exp(X - X.max(axis=1).dimshuffle(0, 'x'))\n",
    "    return e_x / e_x.sum(axis=1).dimshuffle(0, 'x')\n",
    "\n",
    "def RMSprop(cost, params, lr=0.001, rho=0.99, epsilon=1e-6):\n",
    "    grads = T.grad(cost=cost, wrt=params)\n",
    "    updates = []\n",
    "    for p, g in zip(params, grads):\n",
    "        acc = theano.shared(p.get_value() * 0.)\n",
    "        acc_new = rho * acc + (1 - rho) * g ** 2\n",
    "        gradient_scaling = T.sqrt(acc_new + epsilon)\n",
    "        g = g / gradient_scaling\n",
    "        updates.append((acc, acc_new))\n",
    "        updates.append((p, p - lr * g))\n",
    "    return updates\n",
    "\n",
    "def dropout(X, p=0.):\n",
    "    if p > 0:\n",
    "        retain_prob = 1 - p\n",
    "        X *= srng.binomial(X.shape, p=retain_prob, dtype=theano.config.floatX)\n",
    "        X /= retain_prob\n",
    "    return X\n",
    "\n",
    "def model(X, w_h, b_h, w_h2, b_h2, w_o, p_drop_input, p_drop_hidden):\n",
    "    X = dropout(X, p_drop_input)\n",
    "    h = rectify(T.dot(X, w_h) + b_h)\n",
    "\n",
    "    h = dropout(h, p_drop_hidden)\n",
    "    h2 = rectify(T.dot(h, w_h2) + b_h2)\n",
    "\n",
    "    h2 = dropout(h2, p_drop_hidden)\n",
    "    py_x = softmax(T.dot(h2, w_o))\n",
    "    return h, h2, py_x\n",
    "\n",
    "\n",
    "\n",
    "X = T.fmatrix()\n",
    "Y = T.fmatrix()\n",
    "\n",
    "w_h = init_weights((len(vectorizer.vocabulary_), 600))\n",
    "b_h = theano.shared(floatX(np.zeros(600,)))\n",
    "w_h2 = init_weights((600, 600))\n",
    "b_h2 = theano.shared(floatX(np.zeros(600,)))\n",
    "w_o = init_weights((600, yTest.shape[1]))\n",
    "# b_values = numpy.zeros((600,), dtype=theano.config.floatX)\n",
    "\n",
    "noise_h, noise_h2, noise_py_x = model(X, w_h, b_h, w_h2, b_h2, w_o, 0.2, 0.5)\n",
    "h, h2, py_x = model(X, w_h, b_h, w_h2, b_h2, w_o, 0., 0.)\n",
    "y_x = T.argmax(py_x, axis=1)\n",
    "\n",
    "\n",
    "cost = T.mean(T.nnet.categorical_crossentropy(noise_py_x, Y))\n",
    "params = [w_h, b_h, w_h2, b_h2, w_o]\n",
    "updates = RMSprop(cost, params, lr=0.0001)\n",
    "\n",
    "train = theano.function(inputs=[X, Y], outputs=cost, updates=updates, allow_input_downcast=True)\n",
    "predict = theano.function(inputs=[X], outputs=y_x, allow_input_downcast=True)\n",
    "\n",
    "for i in range(301):\n",
    "    for start, end in zip(range(0, len(xTrain), 128), range(128, len(xTrain), 128)):\n",
    "        cost = train(xTrain[start:end], yTrain[start:end])\n",
    "\n",
    "    if i%10 == 0: \n",
    "        tr = np.mean(np.argmax(yTest, axis=1) == predict(xTest))\n",
    "        trr =  np.mean(np.argmax(yTrain, axis=1) == predict(xTrain))\n",
    "        print 'Round:', i,\" Test:\", tr, ' Train:', trr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG: nvcc STDOUT mod.cu\n",
      "   Creating library C:/Users/hollis_win/AppData/Local/Theano/compiledir_Windows-10-10.0.10586-Intel64_Family_6_Model_58_Stepping_9_GenuineIntel-2.7.11-64/tmptwqeg4/c7585c6abe0348b5e3c9444eb9e46735.lib and object C:/Users/hollis_win/AppData/Local/Theano/compiledir_Windows-10-10.0.10586-Intel64_Family_6_Model_58_Stepping_9_GenuineIntel-2.7.11-64/tmptwqeg4/c7585c6abe0348b5e3c9444eb9e46735.exp\n",
      "\n",
      "DEBUG: nvcc STDOUT mod.cu\n",
      "   Creating library C:/Users/hollis_win/AppData/Local/Theano/compiledir_Windows-10-10.0.10586-Intel64_Family_6_Model_58_Stepping_9_GenuineIntel-2.7.11-64/tmpexvlpd/0a62a215d8ac7d63cdc696727d41b45b.lib and object C:/Users/hollis_win/AppData/Local/Theano/compiledir_Windows-10-10.0.10586-Intel64_Family_6_Model_58_Stepping_9_GenuineIntel-2.7.11-64/tmpexvlpd/0a62a215d8ac7d63cdc696727d41b45b.exp\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round: 0  Test: 0.430167597765  Train: 0.462489415749\n",
      "Round: 10  Test: 0.601828339259  Train: 0.654022015241\n",
      "Round: 20  Test: 0.664804469274  Train: 0.740220152413\n",
      "Round: 30  Test: 0.695276790249  Train: 0.786960203218\n",
      "Round: 40  Test: 0.708481462671  Train: 0.811515664691\n",
      "Round: 50  Test: 0.726764855256  Train: 0.839796782388\n",
      "Round: 60  Test: 0.732351447435  Train: 0.860118543607\n",
      "Round: 70  Test: 0.739969527679  Train: 0.875867908552\n",
      "Round: 80  Test: 0.740477399695  Train: 0.888907705334\n",
      "Round: 90  Test: 0.740477399695  Train: 0.900254022015\n",
      "Round: 100  Test: 0.73844591163  Train: 0.907874682472\n"
     ]
    }
   ],
   "source": [
    "\"\"\" glorot 4-layer: batch, dropout, dropout \"\"\"\n",
    "\n",
    "# batch normalization code adapted from \n",
    "# https://groups.google.com/forum/#!topic/theano-users/dMV6aabL1Ds \n",
    "\n",
    "\n",
    "import theano\n",
    "from theano import tensor as T\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "from theano.tensor.nnet.bn import batch_normalization\n",
    "import numpy as np\n",
    "\n",
    "srng = RandomStreams()\n",
    "\n",
    "def floatX(X):\n",
    "    return np.asarray(X, dtype=theano.config.floatX)\n",
    "\n",
    "def init_weights(shape):\n",
    "    (h, w) = shape\n",
    "    # Glorot normalization - last factor depends on non-linearity\n",
    "    # 0.25 for sigmoid and 0.1 for softmax, 1.0 for tanh or Relu\n",
    "    normalizer = 2.0 * np.sqrt(6) / np.sqrt(h + w) * 1.0\n",
    "    return theano.shared(floatX((np.random.random_sample(shape) - 0.5) * normalizer))\n",
    "\n",
    "def rectify(X, alpha=1.0):\n",
    "#     return T.maximum(X, 0.)\n",
    "    return T.maximum(X, 0.1*X)  #leaky rectifier\n",
    "#     return T.switch(X > 0, X, alpha * (T.exp(X) - 1)) # ELU\n",
    "\n",
    "def softmax(X):\n",
    "    e_x = T.exp(X - X.max(axis=1).dimshuffle(0, 'x'))\n",
    "    return e_x / e_x.sum(axis=1).dimshuffle(0, 'x')\n",
    "\n",
    "def RMSprop(cost, params, lr=0.001, rho=0.99, epsilon=1e-6):\n",
    "    grads = T.grad(cost=cost, wrt=params)\n",
    "    updates = []\n",
    "    for p, g in zip(params, grads):\n",
    "        acc = theano.shared(p.get_value() * 0.)\n",
    "        acc_new = rho * acc + (1 - rho) * g ** 2\n",
    "        gradient_scaling = T.sqrt(acc_new + epsilon)\n",
    "        g = g / gradient_scaling\n",
    "        updates.append((acc, acc_new))\n",
    "        updates.append((p, p - lr * g))\n",
    "    return updates\n",
    "\n",
    "def dropout(X, p=0.):\n",
    "    if p > 0:\n",
    "        retain_prob = 1 - p\n",
    "        X *= srng.binomial(X.shape, p=retain_prob, dtype=theano.config.floatX)\n",
    "        X /= retain_prob\n",
    "    return X\n",
    "\n",
    "def model(X, w_h, b_h, gamma, beta, w_h2, b_h2, w_o, p_drop_input, p_drop_hidden):\n",
    "    X = T.dot(X, w_h) + b_h\n",
    "    mean = X.mean((0,), keepdims=True)\n",
    "    std = T.ones_like(X.var((0,), keepdims = True))\n",
    "    X = batch_normalization(X, gamma= gamma, beta= beta, \n",
    "                            mean= mean, #X.mean((0,), keepdims=True), \n",
    "                            std= std, mode='high_mem')    \n",
    "    h = rectify(X)\n",
    "\n",
    "    h = dropout(h, p_drop_hidden)\n",
    "    h2 = rectify(T.dot(h, w_h2) + b_h2)\n",
    "\n",
    "    h2 = dropout(h2, p_drop_hidden)\n",
    "    py_x = softmax(T.dot(h2, w_o))\n",
    "    return h, h2, py_x\n",
    "\n",
    "\n",
    "\n",
    "X = T.fmatrix()\n",
    "Y = T.fmatrix()\n",
    "\n",
    "w_h = init_weights((len(vectorizer.vocabulary_), 600))\n",
    "b_h = theano.shared(floatX(np.zeros(600)))\n",
    "gamma = theano.shared(floatX(np.ones(600)))\n",
    "beta = theano.shared(floatX(np.zeros(600)))\n",
    "\n",
    "w_h2 = init_weights((600, 600))\n",
    "b_h2 = theano.shared(floatX(np.zeros(600,)))\n",
    "w_o = init_weights((600, yTest.shape[1]))\n",
    "\n",
    "noise_h, noise_h2, noise_py_x = model(X, w_h, b_h, gamma, beta, w_h2, b_h2, w_o, 0.2, 0.5)\n",
    "h, h2, py_x = model(X, w_h, b_h, gamma, beta, w_h2, b_h2, w_o, 0., 0.)\n",
    "y_x = T.argmax(py_x, axis=1)\n",
    "\n",
    "\n",
    "cost = T.mean(T.nnet.categorical_crossentropy(noise_py_x, Y))\n",
    "params = [w_h, b_h, gamma, beta, w_h2, b_h2, w_o]\n",
    "updates = RMSprop(cost, params, lr=0.0001)\n",
    "\n",
    "train = theano.function(inputs=[X, Y], outputs=cost, updates=updates, allow_input_downcast=True)\n",
    "predict = theano.function(inputs=[X], outputs=y_x, allow_input_downcast=True)\n",
    "\n",
    "for i in range(101):\n",
    "    for start, end in zip(range(0, len(xTrain), 128), range(128, len(xTrain), 128)):\n",
    "        cost = train(xTrain[start:end], yTrain[start:end])\n",
    "\n",
    "    if i%10 == 0: \n",
    "        tr = np.mean(np.argmax(yTest, axis=1) == predict(xTest))\n",
    "        trr =  np.mean(np.argmax(yTrain, axis=1) == predict(xTrain))\n",
    "        print 'Round:', i,\" Test:\", tr, ' Train:', trr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round: 0  Test: 0.461655662773  Train: 0.498221845893\n",
      "Round: 1  Test: 0.518537328593  Train: 0.563420829805\n",
      "Round: 2  Test: 0.556627729812  Train: 0.605927180356\n",
      "Round: 3  Test: 0.589639410868  Train: 0.646570702794\n",
      "Round: 4  Test: 0.617064499746  Train: 0.681287044877\n",
      "Round: 5  Test: 0.645505332656  Train: 0.709060118544\n",
      "Round: 6  Test: 0.657694261046  Train: 0.731075359865\n",
      "Round: 7  Test: 0.668359573388  Train: 0.748856900931\n",
      "Round: 8  Test: 0.675469781615  Train: 0.765114309907\n",
      "Round: 9  Test: 0.687150837989  Train: 0.777476714649\n",
      "Round: 10  Test: 0.694768918233  Train: 0.787806943268\n",
      "Round: 11  Test: 0.701371254444  Train: 0.79949195597\n",
      "Round: 12  Test: 0.704926358558  Train: 0.808975444539\n",
      "Round: 13  Test: 0.709497206704  Train: 0.818458933108\n",
      "Round: 14  Test: 0.717623158964  Train: 0.828111769687\n",
      "Round: 15  Test: 0.72219400711  Train: 0.837087214225\n",
      "Round: 16  Test: 0.724225495175  Train: 0.844538526672\n",
      "Round: 17  Test: 0.727272727273  Train: 0.850296359018\n",
      "Round: 18  Test: 0.727272727273  Train: 0.857747671465\n",
      "Round: 19  Test: 0.726764855256  Train: 0.861812023709\n",
      "Round: 20  Test: 0.729304215338  Train: 0.867739204064\n",
      "Round: 21  Test: 0.732351447435  Train: 0.87349703641\n",
      "Round: 22  Test: 0.732859319451  Train: 0.880270956816\n",
      "Round: 23  Test: 0.731843575419  Train: 0.88399661304\n",
      "Round: 24  Test: 0.731335703403  Train: 0.887383573243\n"
     ]
    }
   ],
   "source": [
    "\"\"\" glorot 4-layer: batch, batch, batch \"\"\"\n",
    "\n",
    "# batch normalization code adapted from \n",
    "# https://groups.google.com/forum/#!topic/theano-users/dMV6aabL1Ds \n",
    "\n",
    "\n",
    "import theano\n",
    "from theano import tensor as T\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "from theano.tensor.nnet.bn import batch_normalization\n",
    "import numpy as np\n",
    "\n",
    "srng = RandomStreams()\n",
    "\n",
    "def floatX(X):\n",
    "    return np.asarray(X, dtype=theano.config.floatX)\n",
    "\n",
    "def init_weights(shape):\n",
    "    (h, w) = shape\n",
    "    # Glorot normalization - last factor depends on non-linearity\n",
    "    # 0.25 for sigmoid and 0.1 for softmax, 1.0 for tanh or Relu\n",
    "    normalizer = 2.0 * np.sqrt(6) / np.sqrt(h + w) * 1.0\n",
    "    return theano.shared(floatX((np.random.random_sample(shape) - 0.5) * normalizer))\n",
    "\n",
    "def rectify(X, alpha=1.0):\n",
    "#     return T.maximum(X, 0.)\n",
    "    return T.maximum(X, 0.1*X)  #leaky rectifier\n",
    "#     return T.switch(X > 0, X, alpha * (T.exp(X) - 1)) # ELU\n",
    "\n",
    "def softmax(X):\n",
    "    e_x = T.exp(X - X.max(axis=1).dimshuffle(0, 'x'))\n",
    "    return e_x / e_x.sum(axis=1).dimshuffle(0, 'x')\n",
    "\n",
    "def RMSprop(cost, params, lr=0.001, rho=0.99, epsilon=1e-6):\n",
    "    grads = T.grad(cost=cost, wrt=params)\n",
    "    updates = []\n",
    "    for p, g in zip(params, grads):\n",
    "        acc = theano.shared(p.get_value() * 0.)\n",
    "        acc_new = rho * acc + (1 - rho) * g ** 2\n",
    "        gradient_scaling = T.sqrt(acc_new + epsilon)\n",
    "        g = g / gradient_scaling\n",
    "        updates.append((acc, acc_new))\n",
    "        updates.append((p, p - lr * g))\n",
    "    return updates\n",
    "\n",
    "\n",
    "def model(X, w_h, b_h, g_h, bb_h, w_h2, b_h2, g_h2, bb_h2, w_o, b_ho, g_ho, bb_ho):\n",
    "    X = T.dot(X, w_h) + b_h\n",
    "    X = batch_normalization(X, gamma= g_h, beta= bb_h, \n",
    "                            mean= X.mean((0,), keepdims=True),\n",
    "                            std= T.ones_like(X.var((0,), keepdims = True)), \n",
    "                            mode='high_mem')    \n",
    "    h = rectify(X)\n",
    "\n",
    "    h  = T.dot(h, w_h2) + b_h2\n",
    "    h = batch_normalization(h, gamma= g_h2, beta= bb_h2, \n",
    "                            mean= h.mean((0,), keepdims=True),\n",
    "                            std= T.ones_like(h.var((0,), keepdims = True)), \n",
    "                            mode='high_mem')       \n",
    "    h2 = rectify(h)\n",
    "\n",
    "    h2 = T.dot(h2, w_o) + b_ho\n",
    "    h2 = batch_normalization(h2, gamma= g_ho, beta= bb_ho, \n",
    "                            mean= h2.mean((0,), keepdims=True),\n",
    "                            std= T.ones_like(h2.var((0,), keepdims = True)), \n",
    "                            mode='high_mem')   \n",
    "    py_x = softmax(h2)\n",
    "    return h, h2, py_x\n",
    "\n",
    "\n",
    "X = T.fmatrix()\n",
    "Y = T.fmatrix()\n",
    "\n",
    "h1_size = 600\n",
    "h2_size = 550\n",
    "\n",
    "w_h = init_weights((len(vectorizer.vocabulary_), h1_size))\n",
    "b_h = theano.shared(floatX(np.zeros(h1_size)))\n",
    "g_h = theano.shared(floatX(np.ones(h1_size)))\n",
    "bb_h = theano.shared(floatX(np.zeros(h1_size)))\n",
    "\n",
    "w_h2 = init_weights((h1_size, h2_size))\n",
    "b_h2 = theano.shared(floatX(np.zeros(h2_size)))\n",
    "g_h2 = theano.shared(floatX(np.ones(h2_size)))\n",
    "bb_h2 = theano.shared(floatX(np.zeros(h2_size)))\n",
    "\n",
    "w_o = init_weights((h2_size, yTest.shape[1]))\n",
    "b_ho = theano.shared(floatX(np.zeros(yTest.shape[1])))\n",
    "g_ho = theano.shared(floatX(np.ones(yTest.shape[1])))\n",
    "bb_ho = theano.shared(floatX(np.zeros(yTest.shape[1])))\n",
    "\n",
    "noise_h, noise_h2, noise_py_x = model(X, w_h, b_h, g_h, bb_h, \n",
    "                                      w_h2, b_h2, g_h2, bb_h2, \n",
    "                                      w_o, b_ho, g_ho, bb_ho)\n",
    "\n",
    "h, h2, py_x = model(X, w_h, b_h, g_h, bb_h, \n",
    "                    w_h2, b_h2, g_h2, bb_h2, \n",
    "                    w_o, b_ho, g_ho, bb_ho)\n",
    "\n",
    "y_x = T.argmax(py_x, axis=1)\n",
    "\n",
    "\n",
    "cost = T.mean(T.nnet.categorical_crossentropy(noise_py_x, Y))\n",
    "params = [w_h, b_h, g_h, bb_h, w_h2, b_h2, g_h2, bb_h2, w_o, b_ho, g_ho, bb_ho]\n",
    "updates = RMSprop(cost, params, lr=0.0001)\n",
    "\n",
    "train = theano.function(inputs=[X, Y], outputs=cost, updates=updates, allow_input_downcast=True)\n",
    "predict = theano.function(inputs=[X], outputs=y_x, allow_input_downcast=True)\n",
    "\n",
    "batch_size = 60\n",
    "\n",
    "for i in range(25):\n",
    "    for start, end in zip(range(0, len(xTrain), batch_size), range(batch_size, len(xTrain), batch_size)):\n",
    "        cost = train(xTrain[start:end], yTrain[start:end])\n",
    "\n",
    "    tr = np.mean(np.argmax(yTest, axis=1) == predict(xTest))\n",
    "    trr =  np.mean(np.argmax(yTrain, axis=1) == predict(xTrain))\n",
    "    print 'Round:', i,\" Test:\", tr, ' Train:', trr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3558 5373 1030]\n",
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "[[ 0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  1.  0.]]\n"
     ]
    }
   ],
   "source": [
    "# random indexing \n",
    "\n",
    "index = np.random.choice(xTrain.shape[0], 3)\n",
    "print index\n",
    "print xTrain[index]\n",
    "print yTrain[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round: 0  Test: 0.479939055358  Train: 0.51532599492\n",
      "Round: 1  Test: 0.515490096496  Train: 0.5583403895\n",
      "Round: 2  Test: 0.558151345861  Train: 0.613547840813\n",
      "Round: 3  Test: 0.583544946673  Train: 0.638272650296\n",
      "Round: 4  Test: 0.605383443372  Train: 0.667908552075\n",
      "Round: 5  Test: 0.633316404266  Train: 0.698221845893\n",
      "Round: 6  Test: 0.660741493144  Train: 0.725486875529\n",
      "Round: 7  Test: 0.671914677501  Train: 0.744453852667\n",
      "Round: 8  Test: 0.682579989843  Train: 0.762574089754\n",
      "Round: 9  Test: 0.684103605891  Train: 0.769348010161\n",
      "Round: 10  Test: 0.692229558151  Train: 0.785944115157\n",
      "Round: 11  Test: 0.701371254444  Train: 0.79966130398\n",
      "Round: 12  Test: 0.70594210259  Train: 0.806943268417\n",
      "Round: 13  Test: 0.71406805485  Train: 0.818120237087\n",
      "Round: 14  Test: 0.719146775013  Train: 0.824386113463\n",
      "Round: 15  Test: 0.717115286948  Train: 0.834716342083\n",
      "Round: 16  Test: 0.724733367191  Train: 0.844538526672\n",
      "Round: 17  Test: 0.724225495175  Train: 0.853852667231\n",
      "Round: 18  Test: 0.721178263078  Train: 0.85300592718\n",
      "Round: 19  Test: 0.726764855256  Train: 0.860287891617\n",
      "Round: 20  Test: 0.731843575419  Train: 0.864182895851\n",
      "Round: 21  Test: 0.724225495175  Train: 0.872311600339\n",
      "Round: 22  Test: 0.724225495175  Train: 0.878069432684\n",
      "Round: 23  Test: 0.729812087354  Train: 0.879762912786\n",
      "Round: 24  Test: 0.733875063484  Train: 0.887214225233\n"
     ]
    }
   ],
   "source": [
    "\"\"\" glorot 4-layer: batch, batch, batch \n",
    "    random indexing                       \"\"\"\n",
    "\n",
    "# batch normalization code adapted from \n",
    "# https://groups.google.com/forum/#!topic/theano-users/dMV6aabL1Ds \n",
    "\n",
    "\n",
    "import theano\n",
    "from theano import tensor as T\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "from theano.tensor.nnet.bn import batch_normalization\n",
    "import numpy as np\n",
    "\n",
    "srng = RandomStreams()\n",
    "\n",
    "def floatX(X):\n",
    "    return np.asarray(X, dtype=theano.config.floatX)\n",
    "\n",
    "def init_weights(shape):\n",
    "    (h, w) = shape\n",
    "    # Glorot normalization - last factor depends on non-linearity\n",
    "    # 0.25 for sigmoid and 0.1 for softmax, 1.0 for tanh or Relu\n",
    "    normalizer = 2.0 * np.sqrt(6) / np.sqrt(h + w) * 1.0\n",
    "    return theano.shared(floatX((np.random.random_sample(shape) - 0.5) * normalizer))\n",
    "\n",
    "def rectify(X, alpha=1.0):\n",
    "#     return T.maximum(X, 0.)\n",
    "    return T.maximum(X, 0.1*X)  #leaky rectifier\n",
    "#     return T.switch(X > 0, X, alpha * (T.exp(X) - 1)) # ELU\n",
    "\n",
    "def softmax(X):\n",
    "    e_x = T.exp(X - X.max(axis=1).dimshuffle(0, 'x'))\n",
    "    return e_x / e_x.sum(axis=1).dimshuffle(0, 'x')\n",
    "\n",
    "def RMSprop(cost, params, lr=0.001, rho=0.99, epsilon=1e-6):\n",
    "    grads = T.grad(cost=cost, wrt=params)\n",
    "    updates = []\n",
    "    for p, g in zip(params, grads):\n",
    "        acc = theano.shared(p.get_value() * 0.)\n",
    "        acc_new = rho * acc + (1 - rho) * g ** 2\n",
    "        gradient_scaling = T.sqrt(acc_new + epsilon)\n",
    "        g = g / gradient_scaling\n",
    "        updates.append((acc, acc_new))\n",
    "        updates.append((p, p - lr * g))\n",
    "    return updates\n",
    "\n",
    "\n",
    "def model(X, w_h, b_h, g_h, bb_h, w_h2, b_h2, g_h2, bb_h2, w_o, b_ho, g_ho, bb_ho):\n",
    "    X = T.dot(X, w_h) + b_h\n",
    "    X = batch_normalization(X, gamma= g_h, beta= bb_h, \n",
    "                            mean= X.mean((0,), keepdims=True),\n",
    "                            std= T.ones_like(X.var((0,), keepdims = True)), \n",
    "                            mode='high_mem')    \n",
    "    h = rectify(X)\n",
    "\n",
    "    h  = T.dot(h, w_h2) + b_h2\n",
    "    h = batch_normalization(h, gamma= g_h2, beta= bb_h2, \n",
    "                            mean= h.mean((0,), keepdims=True),\n",
    "                            std= T.ones_like(h.var((0,), keepdims = True)), \n",
    "                            mode='high_mem')       \n",
    "    h2 = rectify(h)\n",
    "\n",
    "    h2 = T.dot(h2, w_o) + b_ho\n",
    "    h2 = batch_normalization(h2, gamma= g_ho, beta= bb_ho, \n",
    "                            mean= h2.mean((0,), keepdims=True),\n",
    "                            std= T.ones_like(h2.var((0,), keepdims = True)), \n",
    "                            mode='high_mem')   \n",
    "    py_x = softmax(h2)\n",
    "    return h, h2, py_x\n",
    "\n",
    "\n",
    "X = T.fmatrix()\n",
    "Y = T.fmatrix()\n",
    "\n",
    "h1_size = 600\n",
    "h2_size = 550\n",
    "\n",
    "w_h = init_weights((len(vectorizer.vocabulary_), h1_size))\n",
    "b_h = theano.shared(floatX(np.zeros(h1_size)))\n",
    "g_h = theano.shared(floatX(np.ones(h1_size)))\n",
    "bb_h = theano.shared(floatX(np.zeros(h1_size)))\n",
    "\n",
    "w_h2 = init_weights((h1_size, h2_size))\n",
    "b_h2 = theano.shared(floatX(np.zeros(h2_size)))\n",
    "g_h2 = theano.shared(floatX(np.ones(h2_size)))\n",
    "bb_h2 = theano.shared(floatX(np.zeros(h2_size)))\n",
    "\n",
    "w_o = init_weights((h2_size, yTest.shape[1]))\n",
    "b_ho = theano.shared(floatX(np.zeros(yTest.shape[1])))\n",
    "g_ho = theano.shared(floatX(np.ones(yTest.shape[1])))\n",
    "bb_ho = theano.shared(floatX(np.zeros(yTest.shape[1])))\n",
    "\n",
    "noise_h, noise_h2, noise_py_x = model(X, w_h, b_h, g_h, bb_h, \n",
    "                                      w_h2, b_h2, g_h2, bb_h2, \n",
    "                                      w_o, b_ho, g_ho, bb_ho)\n",
    "\n",
    "h, h2, py_x = model(X, w_h, b_h, g_h, bb_h, \n",
    "                    w_h2, b_h2, g_h2, bb_h2, \n",
    "                    w_o, b_ho, g_ho, bb_ho)\n",
    "\n",
    "y_x = T.argmax(py_x, axis=1)\n",
    "\n",
    "\n",
    "cost = T.mean(T.nnet.categorical_crossentropy(noise_py_x, Y))\n",
    "params = [w_h, b_h, g_h, bb_h, w_h2, b_h2, g_h2, bb_h2, w_o, b_ho, g_ho, bb_ho]\n",
    "updates = RMSprop(cost, params, lr=0.0001)\n",
    "\n",
    "train = theano.function(inputs=[X, Y], outputs=cost, updates=updates, allow_input_downcast=True)\n",
    "predict = theano.function(inputs=[X], outputs=y_x, allow_input_downcast=True)\n",
    "\n",
    "batch_size = 60\n",
    "\n",
    "for i in range(26):\n",
    "    for start, end in zip(range(0, len(xTrain), batch_size), range(batch_size, len(xTrain), batch_size)):\n",
    "        index = np.random.choice(xTrain.shape[0], batch_size, replace=False)\n",
    "        cost = train(xTrain[index], yTrain[index])\n",
    "\n",
    "    tr = np.mean(np.argmax(yTest, axis=1) == predict(xTest))\n",
    "    trr =  np.mean(np.argmax(yTrain, axis=1) == predict(xTrain))\n",
    "    print 'Round:', i,\" Test:\", tr, ' Train:', trr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round: 0  Test: 0.470797359066  Train: 0.517696867062\n",
      "Round: 1  Test: 0.511427120366  Train: 0.55766299746\n",
      "Round: 2  Test: 0.544438801422  Train: 0.598306519898\n",
      "Round: 3  Test: 0.585068562722  Train: 0.644877222693\n",
      "Round: 4  Test: 0.61655662773  Train: 0.677561388654\n",
      "Round: 5  Test: 0.626714068055  Train: 0.696697713802\n",
      "Round: 6  Test: 0.64499746064  Train: 0.725148179509\n",
      "Round: 7  Test: 0.672422549518  Train: 0.749872988992\n",
      "Round: 8  Test: 0.670391061453  Train: 0.764436917866\n",
      "Round: 9  Test: 0.687658710005  Train: 0.77866215072\n",
      "Round: 10  Test: 0.691721686135  Train: 0.795258255715\n",
      "Round: 11  Test: 0.69781615033  Train: 0.807620660457\n",
      "Round: 12  Test: 0.703910614525  Train: 0.816934801016\n",
      "Round: 13  Test: 0.709497206704  Train: 0.827265029636\n",
      "Round: 14  Test: 0.712036566785  Train: 0.836240474174\n",
      "Round: 15  Test: 0.716099542915  Train: 0.841320914479\n",
      "Round: 16  Test: 0.715591670899  Train: 0.851989839119\n",
      "Round: 17  Test: 0.71406805485  Train: 0.859779847587\n",
      "Round: 18  Test: 0.721686135094  Train: 0.868077900085\n",
      "Round: 19  Test: 0.72219400711  Train: 0.876545300593\n",
      "Round: 20  Test: 0.726764855256  Train: 0.883319220999\n",
      "Round: 21  Test: 0.720670391061  Train: 0.886028789162\n",
      "Round: 22  Test: 0.727780599289  Train: 0.889246401355\n",
      "Round: 23  Test: 0.725241239208  Train: 0.892633361558\n",
      "Round: 24  Test: 0.73031995937  Train: 0.900254022015\n",
      "Round: 25  Test: 0.731335703403  Train: 0.901608806097\n",
      "Round: 26  Test: 0.733367191468  Train: 0.904657070279\n",
      "Round: 27  Test: 0.73031995937  Train: 0.907197290432\n",
      "Round: 28  Test: 0.730827831386  Train: 0.913971210838\n",
      "Round: 29  Test: 0.728796343321  Train: 0.916680779001\n",
      "Round: 30  Test: 0.73031995937  Train: 0.918712955123\n",
      "Round: 31  Test: 0.731843575419  Train: 0.922607959356\n",
      "Round: 32  Test: 0.729812087354  Train: 0.925148179509\n",
      "Round: 33  Test: 0.730827831386  Train: 0.927519051651\n"
     ]
    }
   ],
   "source": [
    "\"\"\" glorot 4-layer: batch, batch, batch \n",
    "    random indexing, ELU                \"\"\"\n",
    "\n",
    "# batch normalization code adapted from \n",
    "# https://groups.google.com/forum/#!topic/theano-users/dMV6aabL1Ds \n",
    "\n",
    "\n",
    "import theano\n",
    "from theano import tensor as T\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "from theano.tensor.nnet.bn import batch_normalization\n",
    "import numpy as np\n",
    "\n",
    "srng = RandomStreams()\n",
    "\n",
    "def floatX(X):\n",
    "    return np.asarray(X, dtype=theano.config.floatX)\n",
    "\n",
    "def init_weights(shape):\n",
    "    (h, w) = shape\n",
    "    # Glorot normalization - last factor depends on non-linearity\n",
    "    # 0.25 for sigmoid and 0.1 for softmax, 1.0 for tanh or Relu\n",
    "    normalizer = 2.0 * np.sqrt(6) / np.sqrt(h + w) * 1.0\n",
    "    return theano.shared(floatX((np.random.random_sample(shape) - 0.5) * normalizer))\n",
    "\n",
    "def rectify(X, alpha=0.01):\n",
    "#     return T.maximum(X, 0.)\n",
    "#    return T.maximum(X, 0.1*X)  #leaky rectifier\n",
    "     return T.switch(X > 0, X, alpha * (T.exp(X) - 1)) # ELU\n",
    "\n",
    "def softmax(X):\n",
    "    e_x = T.exp(X - X.max(axis=1).dimshuffle(0, 'x'))\n",
    "    return e_x / e_x.sum(axis=1).dimshuffle(0, 'x')\n",
    "\n",
    "def RMSprop(cost, params, lr=0.001, rho=0.99, epsilon=1e-6):\n",
    "    grads = T.grad(cost=cost, wrt=params)\n",
    "    updates = []\n",
    "    for p, g in zip(params, grads):\n",
    "        acc = theano.shared(p.get_value() * 0.)\n",
    "        acc_new = rho * acc + (1 - rho) * g ** 2\n",
    "        gradient_scaling = T.sqrt(acc_new + epsilon)\n",
    "        g = g / gradient_scaling\n",
    "        updates.append((acc, acc_new))\n",
    "        updates.append((p, p - lr * g))\n",
    "    return updates\n",
    "\n",
    "\n",
    "def model(X, w_h, b_h, g_h, bb_h, w_h2, b_h2, g_h2, bb_h2, w_o, b_ho, g_ho, bb_ho):\n",
    "    X = T.dot(X, w_h) + b_h\n",
    "    X = batch_normalization(X, gamma= g_h, beta= bb_h, \n",
    "                            mean= X.mean((0,), keepdims=True),\n",
    "                            std= T.ones_like(X.var((0,), keepdims = True)), \n",
    "                            mode='high_mem')    \n",
    "    h = rectify(X)\n",
    "\n",
    "    h  = T.dot(h, w_h2) + b_h2\n",
    "    h = batch_normalization(h, gamma= g_h2, beta= bb_h2, \n",
    "                            mean= h.mean((0,), keepdims=True),\n",
    "                            std= T.ones_like(h.var((0,), keepdims = True)), \n",
    "                            mode='high_mem')       \n",
    "    h2 = rectify(h)\n",
    "\n",
    "    h2 = T.dot(h2, w_o) + b_ho\n",
    "    h2 = batch_normalization(h2, gamma= g_ho, beta= bb_ho, \n",
    "                            mean= h2.mean((0,), keepdims=True),\n",
    "                            std= T.ones_like(h2.var((0,), keepdims = True)), \n",
    "                            mode='high_mem')   \n",
    "    py_x = softmax(h2)\n",
    "    return h, h2, py_x\n",
    "\n",
    "\n",
    "X = T.fmatrix()\n",
    "Y = T.fmatrix()\n",
    "\n",
    "h1_size = 600\n",
    "h2_size = 550\n",
    "\n",
    "w_h = init_weights((len(vectorizer.vocabulary_), h1_size))\n",
    "b_h = theano.shared(floatX(np.zeros(h1_size)))\n",
    "g_h = theano.shared(floatX(np.ones(h1_size)))\n",
    "bb_h = theano.shared(floatX(np.zeros(h1_size)))\n",
    "\n",
    "w_h2 = init_weights((h1_size, h2_size))\n",
    "b_h2 = theano.shared(floatX(np.zeros(h2_size)))\n",
    "g_h2 = theano.shared(floatX(np.ones(h2_size)))\n",
    "bb_h2 = theano.shared(floatX(np.zeros(h2_size)))\n",
    "\n",
    "w_o = init_weights((h2_size, yTest.shape[1]))\n",
    "b_ho = theano.shared(floatX(np.zeros(yTest.shape[1])))\n",
    "g_ho = theano.shared(floatX(np.ones(yTest.shape[1])))\n",
    "bb_ho = theano.shared(floatX(np.zeros(yTest.shape[1])))\n",
    "\n",
    "noise_h, noise_h2, noise_py_x = model(X, w_h, b_h, g_h, bb_h, \n",
    "                                      w_h2, b_h2, g_h2, bb_h2, \n",
    "                                      w_o, b_ho, g_ho, bb_ho)\n",
    "\n",
    "h, h2, py_x = model(X, w_h, b_h, g_h, bb_h, \n",
    "                    w_h2, b_h2, g_h2, bb_h2, \n",
    "                    w_o, b_ho, g_ho, bb_ho)\n",
    "\n",
    "y_x = T.argmax(py_x, axis=1)\n",
    "\n",
    "\n",
    "cost = T.mean(T.nnet.categorical_crossentropy(noise_py_x, Y))\n",
    "params = [w_h, b_h, g_h, bb_h, w_h2, b_h2, g_h2, bb_h2, w_o, b_ho, g_ho, bb_ho]\n",
    "updates = RMSprop(cost, params, lr=0.0001)\n",
    "\n",
    "train = theano.function(inputs=[X, Y], outputs=cost, updates=updates, allow_input_downcast=True)\n",
    "predict = theano.function(inputs=[X], outputs=y_x, allow_input_downcast=True)\n",
    "\n",
    "batch_size = 60\n",
    "\n",
    "for i in range(34):\n",
    "    for start, end in zip(range(0, len(xTrain), batch_size), range(batch_size, len(xTrain), batch_size)):\n",
    "        index = np.random.choice(xTrain.shape[0], batch_size, replace=False)\n",
    "        cost = train(xTrain[index], yTrain[index])\n",
    "\n",
    "    tr = np.mean(np.argmax(yTest, axis=1) == predict(xTest))\n",
    "    trr =  np.mean(np.argmax(yTrain, axis=1) == predict(xTrain))\n",
    "    print 'Round:', i,\" Test:\", tr, ' Train:', trr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round: 0  Test: 0.488572879634  Train: 0.521422523285\n",
      "Round: 1  Test: 0.533265617064  Train: 0.574089754445\n",
      "Round: 2  Test: 0.563230066023  Train: 0.61049957663\n",
      "Round: 3  Test: 0.591163026917  Train: 0.647586790855\n",
      "Round: 4  Test: 0.615033011681  Train: 0.679593564776\n",
      "Round: 5  Test: 0.637887252412  Train: 0.704149026249\n",
      "Round: 6  Test: 0.660233621127  Train: 0.728365791702\n",
      "Round: 7  Test: 0.672422549518  Train: 0.750211685013\n",
      "Round: 8  Test: 0.684103605891  Train: 0.767146486029\n",
      "Round: 9  Test: 0.693245302184  Train: 0.779508890771\n",
      "Round: 10  Test: 0.696800406298  Train: 0.78899237934\n",
      "Round: 11  Test: 0.704418486541  Train: 0.8\n",
      "Round: 12  Test: 0.71000507872  Train: 0.809314140559\n",
      "Round: 13  Test: 0.714575926866  Train: 0.819305673158\n",
      "Round: 14  Test: 0.72219400711  Train: 0.830313293819\n",
      "Round: 15  Test: 0.723209751143  Train: 0.839458086367\n",
      "Round: 16  Test: 0.728796343321  Train: 0.845046570703\n",
      "Round: 17  Test: 0.73031995937  Train: 0.849957662997\n",
      "Round: 18  Test: 0.729304215338  Train: 0.857070279424\n",
      "Round: 19  Test: 0.733367191468  Train: 0.864352243861\n",
      "Round: 20  Test: 0.737430167598  Train: 0.868585944115\n",
      "Round: 21  Test: 0.738953783647  Train: 0.87400508044\n",
      "Round: 22  Test: 0.736922295582  Train: 0.880270956816\n",
      "Round: 23  Test: 0.739461655663  Train: 0.88382726503\n",
      "Round: 24  Test: 0.738953783647  Train: 0.888230313294\n"
     ]
    }
   ],
   "source": [
    "\"\"\" glorot 4-layer: batch, batch, batch \"\"\"\n",
    "\n",
    "# batch normalization code adapted from \n",
    "# https://groups.google.com/forum/#!topic/theano-users/dMV6aabL1Ds \n",
    "\n",
    "\n",
    "import theano\n",
    "from theano import tensor as T\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "from theano.tensor.nnet.bn import batch_normalization\n",
    "import numpy as np\n",
    "\n",
    "srng = RandomStreams()\n",
    "\n",
    "def floatX(X):\n",
    "    return np.asarray(X, dtype=theano.config.floatX)\n",
    "\n",
    "def init_weights(shape):\n",
    "    (h, w) = shape\n",
    "    # Glorot normalization - last factor depends on non-linearity\n",
    "    # 0.25 for sigmoid and 0.1 for softmax, 1.0 for tanh or Relu\n",
    "    normalizer = 2.0 * np.sqrt(6) / np.sqrt(h + w) * 1.0\n",
    "    return theano.shared(floatX((np.random.random_sample(shape) - 0.5) * normalizer))\n",
    "\n",
    "def rectify(X, alpha=1.0):\n",
    "#     return T.maximum(X, 0.)\n",
    "    return T.maximum(X, 0.1*X)  #leaky rectifier\n",
    "#     return T.switch(X > 0, X, alpha * (T.exp(X) - 1)) # ELU\n",
    "\n",
    "def softmax(X):\n",
    "    e_x = T.exp(X - X.max(axis=1).dimshuffle(0, 'x'))\n",
    "    return e_x / e_x.sum(axis=1).dimshuffle(0, 'x')\n",
    "\n",
    "def RMSprop(cost, params, lr=0.001, rho=0.99, epsilon=1e-6):\n",
    "    grads = T.grad(cost=cost, wrt=params)\n",
    "    updates = []\n",
    "    for p, g in zip(params, grads):\n",
    "        acc = theano.shared(p.get_value() * 0.)\n",
    "        acc_new = rho * acc + (1 - rho) * g ** 2\n",
    "        gradient_scaling = T.sqrt(acc_new + epsilon)\n",
    "        g = g / gradient_scaling\n",
    "        updates.append((acc, acc_new))\n",
    "        updates.append((p, p - lr * g))\n",
    "    return updates\n",
    "\n",
    "\n",
    "def model(X, w_h, b_h, g_h, bb_h, w_h2, b_h2, g_h2, bb_h2, w_o, b_ho, g_ho, bb_ho):\n",
    "    X = T.dot(X, w_h) + b_h\n",
    "    X = batch_normalization(X, gamma= g_h, beta= bb_h, \n",
    "                            mean= X.mean((0,), keepdims=True),\n",
    "                            std= T.ones_like(X.var((0,), keepdims = True)), \n",
    "                            mode='high_mem')    \n",
    "    h = rectify(X)\n",
    "\n",
    "    h  = T.dot(h, w_h2) + b_h2\n",
    "    h = batch_normalization(h, gamma= g_h2, beta= bb_h2, \n",
    "                            mean= h.mean((0,), keepdims=True),\n",
    "                            std= T.ones_like(h.var((0,), keepdims = True)), \n",
    "                            mode='high_mem')       \n",
    "    h2 = rectify(h)\n",
    "\n",
    "    h2 = T.dot(h2, w_o) + b_ho\n",
    "    h2 = batch_normalization(h2, gamma= g_ho, beta= bb_ho, \n",
    "                            mean= h2.mean((0,), keepdims=True),\n",
    "                            std= T.ones_like(h2.var((0,), keepdims = True)), \n",
    "                            mode='high_mem')   \n",
    "    py_x = softmax(h2)\n",
    "    return h, h2, py_x\n",
    "\n",
    "\n",
    "X = T.fmatrix()\n",
    "Y = T.fmatrix()\n",
    "\n",
    "h1_size = 600\n",
    "h2_size = 550\n",
    "\n",
    "w_h = init_weights((len(vectorizer.vocabulary_), h1_size))\n",
    "b_h = theano.shared(floatX(np.zeros(h1_size)))\n",
    "g_h = theano.shared(floatX(np.ones(h1_size)))\n",
    "bb_h = theano.shared(floatX(np.zeros(h1_size)))\n",
    "\n",
    "w_h2 = init_weights((h1_size, h2_size))\n",
    "b_h2 = theano.shared(floatX(np.zeros(h2_size)))\n",
    "g_h2 = theano.shared(floatX(np.ones(h2_size)))\n",
    "bb_h2 = theano.shared(floatX(np.zeros(h2_size)))\n",
    "\n",
    "w_o = init_weights((h2_size, yTest.shape[1]))\n",
    "b_ho = theano.shared(floatX(np.zeros(yTest.shape[1])))\n",
    "g_ho = theano.shared(floatX(np.ones(yTest.shape[1])))\n",
    "bb_ho = theano.shared(floatX(np.zeros(yTest.shape[1])))\n",
    "\n",
    "noise_h, noise_h2, noise_py_x = model(X, w_h, b_h, g_h, bb_h, \n",
    "                                      w_h2, b_h2, g_h2, bb_h2, \n",
    "                                      w_o, b_ho, g_ho, bb_ho)\n",
    "\n",
    "h, h2, py_x = model(X, w_h, b_h, g_h, bb_h, \n",
    "                    w_h2, b_h2, g_h2, bb_h2, \n",
    "                    w_o, b_ho, g_ho, bb_ho)\n",
    "\n",
    "y_x = T.argmax(py_x, axis=1)\n",
    "\n",
    "\n",
    "cost = T.mean(T.nnet.categorical_crossentropy(noise_py_x, Y))\n",
    "params = [w_h, b_h, g_h, bb_h, w_h2, b_h2, g_h2, bb_h2, w_o, b_ho, g_ho, bb_ho]\n",
    "updates = RMSprop(cost, params, lr=0.0001)\n",
    "\n",
    "train = theano.function(inputs=[X, Y], outputs=cost, updates=updates, allow_input_downcast=True)\n",
    "predict = theano.function(inputs=[X], outputs=y_x, allow_input_downcast=True)\n",
    "\n",
    "batch_size = 60\n",
    "\n",
    "for i in range(25):\n",
    "    for start, end in zip(range(0, len(xTrain), batch_size), range(batch_size, len(xTrain), batch_size)):\n",
    "        cost = train(xTrain[start:end], yTrain[start:end])\n",
    "\n",
    "    tr = np.mean(np.argmax(yTest, axis=1) == predict(xTest))\n",
    "    trr =  np.mean(np.argmax(yTrain, axis=1) == predict(xTrain))\n",
    "    print 'Round:', i,\" Test:\", tr, ' Train:', trr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round: 0  Test: 0.483494159472  Train: 0.518204911092\n",
      "Round: 1  Test: 0.541899441341  Train: 0.579847586791\n",
      "Round: 2  Test: 0.583544946673  Train: 0.630313293819\n",
      "Round: 3  Test: 0.613509395632  Train: 0.67315834039\n",
      "Round: 4  Test: 0.629761300152  Train: 0.696528365792\n",
      "Round: 5  Test: 0.648044692737  Train: 0.716342082981\n",
      "Round: 6  Test: 0.662772981209  Train: 0.733107535986\n",
      "Round: 7  Test: 0.666835957339  Train: 0.749364944962\n",
      "Round: 8  Test: 0.679024885729  Train: 0.761896697714\n",
      "Round: 9  Test: 0.690705942103  Train: 0.771549534293\n",
      "Round: 10  Test: 0.694768918233  Train: 0.780016934801\n",
      "Round: 11  Test: 0.699339766379  Train: 0.788653683319\n",
      "Round: 12  Test: 0.702894870493  Train: 0.796613039797\n",
      "Round: 13  Test: 0.707465718639  Train: 0.806265876376\n",
      "Round: 14  Test: 0.709497206704  Train: 0.813039796782\n",
      "Round: 15  Test: 0.711528694769  Train: 0.822015241321\n",
      "Round: 16  Test: 0.714575926866  Train: 0.827434377646\n",
      "Round: 17  Test: 0.724225495175  Train: 0.838272650296\n",
      "Round: 18  Test: 0.727272727273  Train: 0.843691786622\n",
      "Round: 19  Test: 0.727272727273  Train: 0.848941574936\n",
      "Round: 20  Test: 0.729304215338  Train: 0.855884843353\n",
      "Round: 21  Test: 0.729812087354  Train: 0.859610499577\n",
      "Round: 22  Test: 0.728796343321  Train: 0.866045723963\n",
      "Round: 23  Test: 0.728288471305  Train: 0.872142252329\n",
      "Round: 24  Test: 0.727780599289  Train: 0.876545300593\n"
     ]
    }
   ],
   "source": [
    "\"\"\" glorot 4-layer: batch, batch, batch \"\"\"\n",
    "\n",
    "# batch normalization code adapted from \n",
    "# https://groups.google.com/forum/#!topic/theano-users/dMV6aabL1Ds \n",
    "\n",
    "\n",
    "import theano\n",
    "from theano import tensor as T\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "from theano.tensor.nnet.bn import batch_normalization\n",
    "import numpy as np\n",
    "\n",
    "srng = RandomStreams()\n",
    "\n",
    "def floatX(X):\n",
    "    return np.asarray(X, dtype=theano.config.floatX)\n",
    "\n",
    "def init_weights(shape):\n",
    "    (h, w) = shape\n",
    "    # Glorot normalization - last factor depends on non-linearity\n",
    "    # 0.25 for sigmoid and 0.1 for softmax, 1.0 for tanh or Relu\n",
    "    normalizer = 2.0 * np.sqrt(6) / np.sqrt(h + w) * 1.0\n",
    "    return theano.shared(floatX((np.random.random_sample(shape) - 0.5) * normalizer))\n",
    "\n",
    "def rectify(X, alpha=1.0):\n",
    "#     return T.maximum(X, 0.)\n",
    "    return T.maximum(X, 0.1*X)  #leaky rectifier\n",
    "#     return T.switch(X > 0, X, alpha * (T.exp(X) - 1)) # ELU\n",
    "\n",
    "def softmax(X):\n",
    "    e_x = T.exp(X - X.max(axis=1).dimshuffle(0, 'x'))\n",
    "    return e_x / e_x.sum(axis=1).dimshuffle(0, 'x')\n",
    "\n",
    "def RMSprop(cost, params, lr=0.001, rho=0.99, epsilon=1e-6):\n",
    "    grads = T.grad(cost=cost, wrt=params)\n",
    "    updates = []\n",
    "    for p, g in zip(params, grads):\n",
    "        acc = theano.shared(p.get_value() * 0.)\n",
    "        acc_new = rho * acc + (1 - rho) * g ** 2\n",
    "        gradient_scaling = T.sqrt(acc_new + epsilon)\n",
    "        g = g / gradient_scaling\n",
    "        updates.append((acc, acc_new))\n",
    "        updates.append((p, p - lr * g))\n",
    "    return updates\n",
    "\n",
    "\n",
    "def model(X, w_h, b_h, g_h, bb_h, w_h2, b_h2, g_h2, bb_h2, w_o, b_ho, g_ho, bb_ho):\n",
    "    X = T.dot(X, w_h) + b_h\n",
    "    X = batch_normalization(X, gamma= g_h, beta= bb_h,\n",
    "                            #mean= T.zeros_like(X),\n",
    "                            #std= T.ones_like(X), \n",
    "                            mean= T.zeros_like(X.var((0,), keepdims = True)),\n",
    "                            std= T.ones_like(X.var((0,), keepdims = True)), \n",
    "                            mode='high_mem')    \n",
    "    h = rectify(X)\n",
    "\n",
    "    h  = T.dot(h, w_h2) + b_h2\n",
    "    h = batch_normalization(h, gamma= g_h2, beta= bb_h2, \n",
    "                            #mean= T.zeros_like(h),\n",
    "                            #std= T.ones_like(h),\n",
    "                            mean= T.zeros_like(h.var((0,), keepdims = True)),\n",
    "                            std= T.ones_like(h.var((0,), keepdims = True)), \n",
    "                            mode='high_mem')       \n",
    "    h2 = rectify(h)\n",
    "\n",
    "    h2 = T.dot(h2, w_o) + b_ho\n",
    "    h2 = batch_normalization(h2, gamma= g_ho, beta= bb_ho, \n",
    "                            #mean= T.zeros_like(h2),\n",
    "                            #std= T.ones_like(h2),\n",
    "                            mean= T.zeros_like(h2.var((0,), keepdims = True)),\n",
    "                            std= T.ones_like(h2.var((0,), keepdims = True)), \n",
    "                            mode='high_mem')   \n",
    "    py_x = softmax(h2)\n",
    "    return h, h2, py_x\n",
    "\n",
    "\n",
    "X = T.fmatrix()\n",
    "Y = T.fmatrix()\n",
    "\n",
    "h1_size = 600\n",
    "h2_size = 550\n",
    "\n",
    "w_h = init_weights((len(vectorizer.vocabulary_), h1_size))\n",
    "b_h = theano.shared(floatX(np.zeros(h1_size)))\n",
    "g_h = theano.shared(floatX(np.ones(h1_size)))\n",
    "bb_h = theano.shared(floatX(np.zeros(h1_size)))\n",
    "\n",
    "w_h2 = init_weights((h1_size, h2_size))\n",
    "b_h2 = theano.shared(floatX(np.zeros(h2_size)))\n",
    "g_h2 = theano.shared(floatX(np.ones(h2_size)))\n",
    "bb_h2 = theano.shared(floatX(np.zeros(h2_size)))\n",
    "\n",
    "w_o = init_weights((h2_size, yTest.shape[1]))\n",
    "b_ho = theano.shared(floatX(np.zeros(yTest.shape[1])))\n",
    "g_ho = theano.shared(floatX(np.ones(yTest.shape[1])))\n",
    "bb_ho = theano.shared(floatX(np.zeros(yTest.shape[1])))\n",
    "\n",
    "noise_h, noise_h2, noise_py_x = model(X, w_h, b_h, g_h, bb_h, \n",
    "                                      w_h2, b_h2, g_h2, bb_h2, \n",
    "                                      w_o, b_ho, g_ho, bb_ho)\n",
    "\n",
    "h, h2, py_x = model(X, w_h, b_h, g_h, bb_h, \n",
    "                    w_h2, b_h2, g_h2, bb_h2, \n",
    "                    w_o, b_ho, g_ho, bb_ho)\n",
    "\n",
    "y_x = T.argmax(py_x, axis=1)\n",
    "\n",
    "\n",
    "cost = T.mean(T.nnet.categorical_crossentropy(noise_py_x, Y))\n",
    "params = [w_h, b_h, g_h, bb_h, w_h2, b_h2, g_h2, bb_h2, w_o, b_ho, g_ho, bb_ho]\n",
    "updates = RMSprop(cost, params, lr=0.0001)\n",
    "\n",
    "train = theano.function(inputs=[X, Y], outputs=cost, updates=updates, allow_input_downcast=True)\n",
    "predict = theano.function(inputs=[X], outputs=y_x, allow_input_downcast=True)\n",
    "\n",
    "batch_size = 60\n",
    "\n",
    "for i in range(25):\n",
    "    for start, end in zip(range(0, len(xTrain), batch_size), range(batch_size, len(xTrain), batch_size)):\n",
    "        cost = train(xTrain[start:end], yTrain[start:end])\n",
    "\n",
    "    tr = np.mean(np.argmax(yTest, axis=1) == predict(xTest))\n",
    "    trr =  np.mean(np.argmax(yTrain, axis=1) == predict(xTrain))\n",
    "    print 'Round:', i,\" Test:\", tr, ' Train:', trr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Subtensor{int64}.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "var"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.var((0,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
